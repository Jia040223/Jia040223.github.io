

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Serendipity">
  <meta name="keywords" content="">
  
    <meta name="description" content="本学习笔记用于记录我学习Stanford CS236课程的学习笔记，分享记录，也便于自己实时查看。 引入 前面的课程中我们已经学习了许多生成模型的架构，例如VAEs，Score Based Models等。在课程的最后也是总算来到当前最火的生成模型架构：Diffusion Model。其实Diffusion Model与前面模型或多或少都有一定的联系，我们也可以从不同的视角来理解它。">
<meta property="og:type" content="article">
<meta property="og:title" content="Diffusion Model原理">
<meta property="og:url" content="https://jia040223.github.io/2024/09/29/Diffusion%20Model%E5%8E%9F%E7%90%86/index.html">
<meta property="og:site_name" content="Serendipity&#39;s Blog">
<meta property="og:description" content="本学习笔记用于记录我学习Stanford CS236课程的学习笔记，分享记录，也便于自己实时查看。 引入 前面的课程中我们已经学习了许多生成模型的架构，例如VAEs，Score Based Models等。在课程的最后也是总算来到当前最火的生成模型架构：Diffusion Model。其实Diffusion Model与前面模型或多或少都有一定的联系，我们也可以从不同的视角来理解它。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://jia040223.github.io/images/Diffusion%20Model%E5%8E%9F%E7%90%86/1.png">
<meta property="article:published_time" content="2024-09-29T10:34:53.000Z">
<meta property="article:modified_time" content="2024-09-29T17:08:28.988Z">
<meta property="article:author" content="Serendipity">
<meta property="article:tag" content="生成模型">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://jia040223.github.io/images/Diffusion%20Model%E5%8E%9F%E7%90%86/1.png">
  
  
  
  <title>Diffusion Model原理 - Serendipity&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"jia040223.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="Serendipity's Blog" type="application/atom+xml">
</head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Serendipity&#39;s Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Diffusion Model原理"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-09-29 18:34" pubdate>
          2024年9月29日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          6k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          51 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">Diffusion Model原理</h1>
            
            
              <div class="markdown-body">
                
                <p>本学习笔记用于记录我学习Stanford
CS236课程的学习笔记，分享记录，也便于自己实时查看。</p>
<h2 id="引入">引入</h2>
<p>前面的课程中我们已经学习了许多生成模型的架构，例如VAEs，Score Based
Models等。在课程的最后也是总算来到当前最火的生成模型架构：Diffusion
Model。其实Diffusion
Model与前面模型或多或少都有一定的联系，我们也可以从不同的视角来理解它。</p>
<p>笔者本科科研也算是学习研究了一些Diffusion相关的工作，但之前一直没有去梳理生成模型的发展，也没有深究其背后的数学原理。所以借此几乎，正好对一些知识进行整理，并对生成模型进行部分回顾。首先从DDPM和DDIM入手吧，这两篇文章也是之前科研实践学习过很多次了。</p>
<h2 id="ddpm">DDPM</h2>
<p>首先我们知道，DDPM
是个马尔科夫模型（如下图），DDPM包括两个步骤。这两个步骤在原文中定义为前向加噪（forward，下图从右到左）和后向去噪（reverse，下图从左到右）。</p>
<p><img src="/images/Diffusion%20Model原理/1.png" srcset="/img/loading.gif" lazyload></p>
<p>从 <span class="math inline">\(x_0\)</span> 到 <span class="math inline">\(x_T\)</span>
的过程就是前向加噪过程，我们可以看到加噪过程就是对原始图片 <span class="math inline">\(x_0\)</span>
不断添加噪声，使其最后信噪比趋近于0，此时得到的图片也就变成噪声了，而与之相对应的去噪过程就是还原过程，即从噪声不断去噪还原为图片。</p>
<p>我们通过往图片中加入噪声，使得图片变得模糊起来，当加的步骤足够多的时候（也就是T的取值越大的时候，一般取1000），图片已经非常接近一张纯噪声。纯噪声也就意味着多样性，我们的模型在去噪（还原）的过程中能够产生更加多样的图片。</p>
<p>这里的操作实际上就是指在图片加入噪声 <span class="math inline">\(noise\)</span> ，噪声 <span class="math inline">\(noise\)</span>
本身的分布可以是很多样的（btw，保研还被问过这个问题），而论文中采用的是<strong>标准正态分布</strong>，其理由是考虑到其优良的性质，在接下来的公式推理中见到。</p>
<h3 id="推导">推导</h3>
<p>从上面的图可知，DDPM
将前向过程和逆向过程都设计为了马尔可夫链的形式：</p>
<ul>
<li>称从 <span class="math inline">\(x_0\)</span> 到 <span class="math inline">\(x_T\)</span> 的马尔可夫链为<strong>前向过程
(forward process)</strong> 或<strong>扩散过程 (diffusion
process)</strong>；</li>
<li>称从 <span class="math inline">\(x_T\)</span> 到 <span class="math inline">\(x_0\)</span> 的马尔可夫链为<strong>逆向过程
(reverse process)</strong> 或<strong>去噪过程 (denoising
process)</strong>.</li>
</ul>
<p>所以我们的损失函数通过极大似然估计来进行。但这里我们又会遇到和VAE一样的问题，
<span class="math inline">\(log(P(x))\)</span> 中的 <span class="math inline">\(P(x)\)</span> 需要对 <span class="math inline">\(x_{1:T}\)</span>
进行积分，此时我们便可以效仿VAE的做法，即把 <span class="math inline">\(x_{1:T}\)</span>
作为类似VAE中的潜变量，去优化对数似然的下界ELBO（为什么是下界可以参考我都VAEs的文章，简单来说就是用<strong>琴生不等式</strong>即可）：</p>
<p><span class="math display">\[
\begin{align*} ELBO &amp;= \mathbb{E}_{\mathbf{x}_{1:T} \sim
q(\mathbf{x}_{1:T} \vert \mathbf{x}_0)} \left[\log
\frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T} \vert
\mathbf{x}_0)} \right] \\&amp;= \mathbb{E}_{\mathbf{x}_{1:T} \sim
q(\mathbf{x}_{1:T} \vert \mathbf{x}_0)} \left[ \log
\frac{p(\mathbf{x}_T) \prod_{t=1}^T p_\theta(\mathbf{x}_{t-1} \vert
\mathbf{x}_t)}{\prod_{t=1}^T q(\mathbf{x}_t \vert \mathbf{x}_{t-1})}
\right]  \end{align*}
\]</span></p>
<p>至于这里为啥要在给定 <span class="math inline">\(x_0\)</span>
下计算，一方面是单纯的 <span class="math inline">\(q(\mathbf{x}_{1:T})\)</span>
我们没办法计算得出，而 <span class="math inline">\(q(\mathbf{x}_{1:T}|\mathbf{x}_0)\)</span>
我们能求出其闭式解，另一方面在训练时我们的确已经 <span class="math inline">\(x_0\)</span>的信息。</p>
<p>OK，我们继续进行推导</p>
<p><span class="math display">\[
\begin{align} &amp;\ \ \ \ \ \text{ELBO}(\mathbf x_0) \\ &amp;=\mathbb
E_{q(\mathbf x_{1:T}\vert\mathbf x_0)}\left[\log\frac{p(\mathbf
x_T)\prod_{t=1}^{T}p(\mathbf x_{t-1}\vert\mathbf
x_t)}{\prod_{t=1}^{T}q(\mathbf x_t\vert\mathbf x_{t-1})}\right]\\
&amp;=\mathbb E_{q(\mathbf x_{1:T}\vert\mathbf
x_0)}\left[\log\frac{p(\mathbf x_T)\prod_{t=1}^{T}p(\mathbf
x_{t-1}\vert\mathbf x_t)}{q(\mathbf x_1\vert\mathbf
x_0)\prod_{t=2}^{T}q(\mathbf x_t\vert\mathbf x_{t-1},\mathbf
x_0)}\right]\\ &amp;=\mathbb E_{q(\mathbf x_{1:T}\vert\mathbf
x_0)}\left[\log\frac{p(\mathbf x_T)\prod_{t=1}^{T}p(\mathbf
x_{t-1}\vert\mathbf x_t)}{q(\mathbf x_1\vert\mathbf
x_0)\prod_{t=2}^{T}\frac{q(\mathbf x_t\vert\mathbf x_0)q(\mathbf
x_{t-1}\vert\mathbf x_t,\mathbf x_0)}{q(\mathbf x_{t-1}\vert\mathbf
x_0)} }\right]\\ &amp;=\mathbb E_{q(\mathbf x_{1:T}\vert\mathbf
x_0)}\left[\log\frac{p(\mathbf x_T)\prod_{t=1}^{T}p(\mathbf
x_{t-1}\vert\mathbf x_t)}{q(\mathbf x_T\vert\mathbf
x_0)\prod_{t=2}^{T}q(\mathbf x_{t-1}\vert\mathbf x_t,\mathbf
x_0)}\right]\\ &amp;=\mathbb E_{q(\mathbf x_{1:T}\vert\mathbf
x_0)}\left[\log p(\mathbf x_0\vert\mathbf x_1)\right]+\mathbb
E_{q(\mathbf x_{1:T}\vert\mathbf x_0)}\left[\log\frac{p(\mathbf
x_T)}{q(\mathbf x_T\vert\mathbf x_0)}\right]+\sum_{t=2}^T\mathbb
E_{q(\mathbf x_{1:T}\vert\mathbf x_0)}\left[\log\frac{p(\mathbf
x_{t-1}\vert\mathbf x_t)}{q(\mathbf x_{t-1}\vert\mathbf x_t,\mathbf
x_0)}\right]\\ &amp;=\mathbb E_{q(\mathbf x_{1}\vert\mathbf
x_0)}\left[\log p(\mathbf x_0\vert\mathbf x_1)\right]+\mathbb
E_{q(\mathbf x_{T}\vert\mathbf x_0)}\left[\log\frac{p(\mathbf
x_T)}{q(\mathbf x_T\vert\mathbf x_0)}\right]+\sum_{t=2}^T\mathbb
E_{q(\mathbf x_t\vert\mathbf x_0)}\mathbb E_{q(\mathbf
x_{t-1}\vert\mathbf x_t,\mathbf x_0)}\left[\log\frac{p(\mathbf
x_{t-1}\vert\mathbf x_t)}{q(\mathbf x_{t-1}\vert\mathbf x_t,\mathbf
x_0)}\right]\\ &amp;=\underbrace{\mathbb E_{q(\mathbf x_{1}\vert\mathbf
x_0)}\left[\log p(\mathbf x_0\vert\mathbf
x_1)\right]}_\text{reconstruction term}-\underbrace{\text{KL}(q(\mathbf
x_T\vert\mathbf x_0)\Vert p(\mathbf x_T))}_\text{regularization
term}-\sum_{t=2}^T\mathbb E_{q(\mathbf x_t\vert\mathbf
x_0)}\underbrace{\left[\text{KL}(q(\mathbf x_{t-1}\vert\mathbf
x_t,\mathbf x_0)\Vert p(\mathbf x_{t-1}\vert\mathbf
x_t))\right]}_\text{denoising matching terms} \end{align}
\]</span></p>
<p>同样出现了重构项、正则项和匹配项。重构项要求 $x_1 $ 能够重构 <span class="math inline">\(x_0\)</span> ，正则项要求 <span class="math inline">\(x_T\)</span>
的后验分布逼近先验分布，而匹配项则建立起相邻两项 $x_{t−1},x_t $
之间的联系。</p>
<p>现在，我们只需要为式中出现的所有概率分布设计具体的形式，就可以代入计算了。为了让
KL 散度可解，一个自然的想法就是把它们都设计为正态分布的形式。</p>
<h3 id="前向过程">前向过程</h3>
<p>在DDPM的前向过程中，对于 <span class="math inline">\(t \in
[1,T]\)</span> 时刻， <span class="math inline">\(x_t\)</span> 和 <span class="math inline">\(x_{t-1}\)</span> 满足如下关系：</p>
<p><span class="math display">\[x_t = \sqrt{1-\beta_t}x_{t-1} +
\sqrt{\beta_t }\epsilon,  \ \ \ \epsilon\sim N(0,1)\]</span></p>
<p>其中 <span class="math inline">\(β_t∈(0,1)\)</span>
是事先指定的超参数，代表从 $x_{t−1} $ 到 $x_t $ 这一步的方差。</p>
<p>这里的系数设定为开根号的 $ $
，是为了保证马尔科夫链的最后收敛为标准高斯分布。</p>
<p><strong><span class="math inline">\(\sqrt\beta\)</span> 和 <span class="math inline">\(\sqrt{1-\beta}\)</span> 是怎么来的：</strong></p>
<p>我们这里先不管 <span class="math inline">\(\beta\)</span>
，把两个系数分别设为 <span class="math inline">\(a\)</span> 和 <span class="math inline">\(b\)</span> 。</p>
<p>公式变为：</p>
<p><span class="math display">\[x_t = ax_{t-1} + b\epsilon\]</span></p>
<p>我们希望，当 <span class="math inline">\(t\)</span>趋于无穷的时候，
<span class="math inline">\(x_t \sim N(0,1), x_{t-1} \sim
N(0,1)\)</span></p>
<p>我们知道当两个高斯分布相加时，</p>
<p><span class="math display">\[X\sim N(\mu_X,\sigma_X^2),Y\sim
N(\mu_Y,\sigma_Y^2) \]</span></p>
<p><span class="math display">\[Z=aX+bY \]</span></p>
<p>则</p>
<p><span class="math display">\[Z \sim N(a\mu_X+b\mu_Y,
a^2\sigma^2+b^2\sigma^2)\]</span></p>
<p>所以此时</p>
<p><span class="math display">\[x_t~\sim
N(a\mu_{t-1}+b\mu_\epsilon,a^2\sigma_{t-1}^2+b^2\sigma_\epsilon^2)\]</span></p>
<p><span class="math display">\[x_t\sim N(a·0+b·0,
a^2·1+b^2·1)\]</span></p>
<p><span class="math display">\[x_t \sim N(0,a^2+b^2) \]</span></p>
<p>我们想让 <span class="math inline">\(x_{t-1}\)</span> 和 <span class="math inline">\(\epsilon\)</span> 得到的 <span class="math inline">\(x_{t}\)</span> 也服从标准正态分布，即 $ x_{t}
N(0,1)$ ，那么我们就只能让 <span class="math inline">\(a^2+b^2=1\)</span> 。</p>
<p>再令 <span class="math inline">\(\beta=a^2\)</span> ，则 <span class="math inline">\(a=\sqrt{\beta},b=\sqrt{1-\beta}\)</span> 。</p>
<p>或者也可以令 <span class="math inline">\(\alpha=b^2\)</span> ，则 $
a=x_{t-1}+$ 。</p>
<p>说白了，这俩系数就是为了让两个服从标准正态分布的噪声相加得到的东西还是服从正态分布。</p>
<p>OK，在这基础上我们可以继续推导，<strong>让 <span class="math inline">\(x_t\)</span> 用 <span class="math inline">\(x_0\)</span> 来表示</strong>：</p>
<p>令 <span class="math inline">\(\alpha_t=1-\beta_t\)</span>
，则公式变为：</p>
<p><span class="math display">\[x_t=\sqrt{\alpha_t}x_{t-1}+\sqrt{1-\alpha_t}\epsilon\]</span></p>
<p>继续推导：</p>
<p><span class="math display">\[\begin{align*} x_t
&amp;=\sqrt{\alpha_t}x_{t-1}+\sqrt{1-\alpha_t}\epsilon\\
&amp;=\sqrt{\alpha_t}(\sqrt{\alpha_{t-1} }x_{t-2}+\sqrt{1-\alpha_{t-1}
}\epsilon)+\sqrt{1-\alpha_t}\epsilon\\  &amp;=\sqrt{\alpha_t\alpha_{t-1}
}x_{t-2}+\sqrt{\alpha_t(1-\alpha_{t-1})}\epsilon +
\sqrt{1-\alpha_t}\epsilon\\ \end{align*}\]</span></p>
<p>上式最后一行第二项和第三项，可以看做两个正态分布相加。</p>
<p>由于两个正态分布 <span class="math inline">\(X\sim
N(\mu_x,\sigma_x^2), Y\sim N(\mu_y, \sigma_y^2)\)</span> ，相加后有</p>
<p><span class="math inline">\(aX+bY\sim
N(a\mu_x+b\mu_y,a^2\sigma_x^2+b^2\sigma_y^2)\)</span>。所以，合并两个正态分布，得到：</p>
<p><span class="math display">\[x_t=\sqrt{\alpha_t\alpha_{t-1}
}x_{t-2}+\sqrt{1-\alpha_t\alpha_{t-1} }\epsilon\]</span></p>
<p>由数学归纳法，可以推导出：</p>
<p><span class="math display">\[x_t=\sqrt{\alpha_t\alpha_{t-1}...\alpha_1}x_0+\sqrt{1-\alpha_t\alpha_{t-1}...\alpha_1}\epsilon\]</span></p>
<p>再令 <span class="math inline">\(\bar\alpha_t=\alpha_t\alpha_{t-1}...\alpha_1\)</span>
，则公式可以进一步化简为：</p>
<p><span class="math inline">\(x_t=\sqrt
{\bar\alpha_t}x_0+\sqrt{1-\bar\alpha_{t} }\epsilon\)</span> ，由于</p>
<p><span class="math display">\[\lim_{t\to\infty}\sqrt{\bar\alpha_t}=0,\quad\lim_{t\to\infty}\sqrt{1-\bar\alpha_t}=1\]</span></p>
<p>所以我们能够保证马尔科夫链最后能够收敛于标准正态分布</p>
<h3 id="逆向过程">逆向过程</h3>
<p>这里从我们熟知的贝叶斯公式出发：</p>
<p><span class="math display">\[P(A|B)=\frac{P(B|A)P(A)}{P(B)}\]</span></p>
<p>可知</p>
<p><span class="math display">\[P(x_{t-1}|x_t)=\frac{P(x_t|x_{t-1})P(x_{t-1})}{P(x_t)}\]</span></p>
<p>这里我们的 <span class="math inline">\(P(x_{t-1})\)</span> 和 <span class="math inline">\(P(x_t)\)</span> 我们都不知道，但在已知 $ x_0$
的情况下有：</p>
<p><span class="math display">\[P(x_{t-1}|x_t,x_0)=\frac{P(x_t|x_{t-1},x_0)P(x_{t-1}|x_0)}{P(x_t|x_0)}\]</span></p>
<p>把 <span class="math inline">\(x_0=\sqrt{\bar{\alpha_t} }x_0\)</span>
和 <span class="math inline">\(x_t=\sqrt
{\bar\alpha_t}x_0+\sqrt{1-\bar\alpha_{t} }\epsilon\)</span>
带入上式，可得：</p>
<p><span class="math display">\[P(x_{t-1}|x_t,x_0)=\frac{
N(\sqrt{\alpha_t}x_0,1-\bar\alpha_t) N(\sqrt{\bar\alpha_{t-1}
}x_0,1-\bar\alpha_{t-1}) }{ N(\sqrt{\bar\alpha_{t}
}x_0,1-\bar\alpha_{t}) }\]</span></p>
<p>已知高斯分布的概率密度函数为：</p>
<p><span class="math display">\[f(x)=\frac{1}{\sqrt{2\pi\sigma}
}exp(-\frac{(x-\mu)^2}{2\sigma^2})\]</span></p>
<p>所以</p>
<p><span class="math display">\[P(x_{t-1}|x_t,x_0)
\propto   exp-\frac{1}{2}  [   \frac{(x_t-\sqrt{\alpha_t}x_{t-1})^2}{1-\alpha_t}
+\frac{(x_{t-1}-\sqrt{\bar\alpha_{t-1} }x_0)^2}{1-\bar\alpha_{t-1} }
-\frac{(x_{t}-\sqrt{\bar\alpha_{t} }x_0)^2}{1-\bar\alpha_{t} }
]\]</span></p>
<p>此时由于 <span class="math inline">\(x_{t-1}\)</span>
是我们关注的变量，所以整理成关于 <span class="math inline">\(x_{t-1}\)</span> 的形式：</p>
<p><span class="math display">\[P(x_{t-1}|x_t,x_0)
\propto  exp-\frac{1}{2}  [   (\frac{\alpha_t}{1-\alpha_t}+\frac{1}{1-\bar\alpha_{t-1}
})x_{t-1}^2  -(\frac{-2\sqrt{\alpha_t}x_t}{1-\alpha_t}   +    \frac{-2\sqrt{\bar\alpha_{t-1}
}x_0}{1-\bar\alpha_{t-1} })x_{t-1}   +C(x_t,x_0) ]\]</span></p>
<p>其中第三项 <span class="math inline">\(C(x_t,x_0)\)</span> 与 <span class="math inline">\(x_{t-1}\)</span>
无关，作为指数上相加的部分，可以拿到最前面只影响最前面的系数。</p>
<p>所以此时：</p>
<p><span class="math display">\[P(x_{t-1}|x_t,x_0)
\propto  exp-\frac{1}{2}  [   (\frac{\alpha_t}{1-\alpha_t}+\frac{1}{1-\bar\alpha_{t-1}
})x_{t-1}^2  -(\frac{-2\sqrt{\alpha_t}x_t}{1-\alpha_t}   +  \frac{-2\sqrt{\bar\alpha_{t-1}
}x_0}{1-\bar\alpha_{t-1} })x_{t-1}]\]</span></p>
<p>又因为标准正态分布满足 <span class="math inline">\(\propto exp -
\frac{x^2-2\mu x + \mu^2}{2\sigma^2}\)</span> ，所以我们可以得到 <span class="math inline">\(P(x_{t-1}|x_t,x_0)\)</span> 对应的方差</p>
<p><span class="math display">\[
\frac{1}{\sigma^2}=\frac{\alpha_t}{1-\alpha_t}+\frac{1}{1-\bar\alpha_{t-1}
}  =\frac{1-\alpha_t\bar\alpha_{t-1}
}{(1-\alpha_t)(1-\bar\alpha_{t-1})}  =\frac{1-\bar\alpha_{t}
}{(1-\alpha_t)(1-\bar\alpha_{t-1})}\]</span></p>
<p>这里 <span class="math inline">\(\alpha_t\bar\alpha_{t-1}=\bar\alpha_t\)</span>
。所以：</p>
<p><span class="math display">\[\sigma^2=\frac{(1-\alpha_t)(1-\bar\alpha_{t-1})}{1-\bar\alpha_t}\]</span></p>
<p>再看 <span class="math inline">\(x_{t-1}\)</span>
的一次项，得到：</p>
<p><span class="math display">\[\frac{2\mu}{\sigma^2}=
(\frac{-2\sqrt{\alpha_t}x_t}{1-\alpha_t}   +  \frac{-2\sqrt{\bar\alpha_{t-1}
}x_0}{1-\bar\alpha_{t-1} })\]</span></p>
<p>把 <span class="math inline">\(\sigma^2\)</span> 和 <span class="math inline">\(x_0\)</span> 带入上式，化简得到： <span class="math display">\[\mu=\frac{1}{\sqrt{\alpha_t}
}(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t} }\epsilon)\]</span></p>
<p>所以说： <span class="math display">\[P(x_{t-1}|x_t, x_0)\sim
N(\frac{1}{\sqrt{\alpha_t} }(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}
}\epsilon),
\frac{(1-\alpha_t)(1-\bar\alpha_{t-1})}{1-\bar\alpha_t})\]</span></p>
<p>回顾一下我们写的这一大段公式，也就是说，我们已知了先验概率，推导出了后验概率的表达式，得到了在给定
<span class="math inline">\(x_0\)</span> 后的<span class="math inline">\(x_{t-1}\)</span>
的分布的均值和方差。也就是说，上面公式中，我们的</p>
<p><span class="math display">\[q(x_{t-1}\vert x_t,x_0)\sim
N(\frac{1}{\sqrt{\alpha_t} }(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}
}\epsilon),
\frac{(1-\alpha_t)(1-\bar\alpha_{t-1})}{1-\bar\alpha_t})\]</span></p>
<p>接下来， <span class="math inline">\(\epsilon\)</span>
的具体值，我们让模型去拟合就好了。</p>
<h3 id="损失函数">损失函数</h3>
<p>我们之前已经推导了ELBO的具体形式：</p>
<p><span class="math display">\[\text{ELBO}= \underbrace{E_{x_1\sim
q(x_1\vert x_0)}[\log p_\theta(x_0\vert x_1)]}_{ {L_0} }-
\underbrace{KL(q(x_T \vert  x_0)\|p(x_T))}_{ {L_T} }-
\sum_{t=2}^T\underbrace{E_{x_t\sim q(x_t\vert
x_0)}\left[KL(q(x_{t-1}\vert x_t,x_0)\|p_\theta(x_{t-1}\vert
x_t))\right]}_{ {L_{t-1} }}\]</span></p>
<p>这里 <span class="math inline">\(q(x_{t-1}\vert x_t,x_0)\)</span>
我们已经得到了， <span class="math inline">\(q(x_{t}|x_0)\)</span>
也是我们定义的。只需要定义 <span class="math inline">\(p_\theta(x_{t-1}|x_t)\)</span>
即可，为了计算方便，我们也选择与 <span class="math inline">\(q(x_{t-1}\vert x_t,x_0)\)</span> 一样的形式。</p>
<p><span class="math display">\[p_\theta(\textbf{x}_{t-1}|\textbf{x}_t)
= \mathcal{N}(\textbf{x}_{t-1}; \mu_\theta(\textbf{x}_t, t),
\frac{(1-\alpha_t)(1-\bar\alpha_{t-1})}{1-\bar\alpha_t}I)\]</span></p>
<p>其中 <span class="math inline">\(\boldsymbol{\mu}_\theta(\mathbf{x}_t, t) =
\frac{1}{\sqrt{\alpha_t} } \Big( \mathbf{x}_t - \frac{1 -
\alpha_t}{\sqrt{1 - \bar{\alpha}_t} }
\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \Big)\)</span> ，而 <span class="math inline">\({\epsilon}_\theta(\mathbf{x}_t, t)\)</span>
就是我们模型的输出。此时，我们带入可以得到</p>
<p><span class="math display">\[\begin{align}  \mathbf{x}_{t-1} &amp;=
\mathcal{N}(\mathbf{x}_{t-1}; \frac{1}{\sqrt{\alpha_t} } ( \mathbf{x}_t
- \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t} }
{\epsilon}_\theta(\mathbf{x}_t, t) ),
\frac{(1-\alpha_t)(1-\bar\alpha_{t-1})}{1-\bar\alpha_t}I)  \end{align}\]</span></p>
<p>带入上面KL散度的公式，可以得到损失函数 <span class="math inline">\(L_t\)</span> 便为：</p>
<p><span class="math display">\[\begin{aligned} L_t  &amp;=
\mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon} } \Big[\frac{1}{2 \|
\boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t) \|^2_2} \|
\color{blue}{\tilde{\boldsymbol{\mu} }_t(\mathbf{x}_t, \mathbf{x}_0)} -
\color{green}{\boldsymbol{\mu}_\theta(\mathbf{x}_t, t)} \|^2 \Big] \\
&amp;= \mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon} }
\Big[\frac{1}{2  \|\boldsymbol{\Sigma}_\theta \|^2_2} \|
\color{blue}{\frac{1}{\sqrt{\alpha_t} } \Big( \mathbf{x}_t - \frac{1 -
\alpha_t}{\sqrt{1 - \bar{\alpha}_t} } \boldsymbol{\epsilon}_t \Big)} -
\color{green}{\frac{1}{\sqrt{\alpha_t} } \Big( \mathbf{x}_t - \frac{1 -
\alpha_t}{\sqrt{1 - \bar{\alpha}_t} } \boldsymbol{\boldsymbol{\epsilon}
}_\theta(\mathbf{x}_t, t) \Big)} \|^2 \Big] \\ &amp;=
\mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon} } \Big[\frac{ (1 -
\alpha_t)^2 }{2 \alpha_t (1 - \bar{\alpha}_t) \|
\boldsymbol{\Sigma}_\theta \|^2_2} \|\boldsymbol{\epsilon}_t -
\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\|^2 \Big] \\ &amp;=
\mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon} } \Big[\frac{ (1 -
\alpha_t)^2 }{2 \alpha_t (1 - \bar{\alpha}_t) \|
\boldsymbol{\Sigma}_\theta \|^2_2} \|\boldsymbol{\epsilon}_t -
\boldsymbol{\epsilon}_\theta(\sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1
- \bar{\alpha}_t}\boldsymbol{\epsilon}_t, t)\|^2
\Big]  \end{aligned}\]</span></p>
<p>发现可以使用不用权重的简单形式就可以训练得到好的结果，即</p>
<p><span class="math display">\[\begin{aligned} L_\text{simple} &amp;=
\mathbb{E}_{t \sim [1, T], \mathbf{x}_0, \boldsymbol{\epsilon}_t}
\Big[\|\boldsymbol{\epsilon}_t -
\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\|^2 \Big] \\ &amp;=
\mathbb{E}_{t \sim [1, T], \mathbf{x}_0, \boldsymbol{\epsilon}_t}
\Big[\|\boldsymbol{\epsilon}_t -
\boldsymbol{\epsilon}_\theta(\sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1
- \bar{\alpha}_t}\boldsymbol{\epsilon}_t, t)\|^2 \Big]
\end{aligned}\]</span></p>
<p>这样，我们就获得了DDPM的最终目标函数：</p>
<p><span class="math display">\[ L_\text{simple}(\theta)=\mathbb
E_{t,x_0,\epsilon}\left[\Vert\epsilon-\epsilon_\theta(x_t,t)\Vert^2\right]\]</span></p>
<p>具体训练流程和采样流程如下：</p>
<p><img src="/images/Diffusion%20Model原理/2.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="ddim">DDIM</h2>
<p>DDPM虽好，但它只能一步一步老老实实通过 <span class="math inline">\(x_{t}\)</span> 预测 <span class="math inline">\(x_{t-1}\)</span> ，不能跨步运算，如果 <span class="math inline">\(T =1000\)</span>
，那么生成一整图像就需要用网络推理1000次，效率很低。于是为了结局这个问题，DDIM出现了，而且最巧妙的是它不需要重新训练模型。</p>
<p>DDIM始于一个假设，它假设了</p>
<p><span class="math display">\[P(x_{prev}|x_t,x_0)\sim
N(kx_0+mx_t,\sigma_2)\]</span></p>
<p><span class="math display">\[x_{prev}=kx_0+mx_t+\sigma\epsilon,\ \ \
\ \ \epsilon\sim N(0,1)\]</span></p>
<p>又因为加噪过程满足公式 <span class="math inline">\(x_t=\sqrt
{\bar\alpha_t}x_0+\sqrt{1-\bar\alpha_{t} }\epsilon\)</span></p>
<p>把 <span class="math inline">\(x_t\)</span> 带入 <span class="math inline">\(x_{t-1}\)</span> 合并同类项得到：</p>
<p><span class="math display">\[\begin{align*}
x_{prev}&amp;=kx_0+m(\sqrt{\bar\alpha_t}x_0+\sqrt{1-\bar\alpha_t}\epsilon)+\sigma\epsilon\\
&amp;=(k+m\sqrt{\bar\alpha_t})x_0+\epsilon&#39;
\end{align*}\]</span></p>
<p><span class="math display">\[\epsilon&#39;\sim
N(0,m^2(1-\bar\alpha_t)+\sigma^2)\]</span></p>
<p>又因为 <span class="math inline">\(x_{prev}=\sqrt {\bar\alpha_{prev}
}x_0+\sqrt{1-\bar\alpha_{prev} }\epsilon\)</span>
，满足对应系数相同，有：</p>
<p><span class="math display">\[k+m\sqrt{\bar\alpha_t}=\sqrt{\bar{\alpha_{prev}
}}\\ m^2(1-\bar\alpha_t)+\sigma^2=1-\bar\alpha_{prev}\]</span></p>
<p>求得：</p>
<p><span class="math display">\[m=\frac{\sqrt{1-\bar\alpha_{prev}-\sigma^2}
}{\sqrt{1-\bar\alpha_t} }\\  k=\sqrt{\bar\alpha_{prev}
}-\frac{\sqrt{1-\bar\alpha_{prev}-\sigma^2} }{\sqrt{1-\bar\alpha_t}
}\sqrt{\bar\alpha_t}\]</span></p>
<p>带入公式最终化简得：</p>
<p><span class="math display">\[x_{prev}=\sqrt{\bar{\alpha_{prev} }}
(\frac{x_t-\sqrt{1-\bar\alpha_t}\epsilon_t}{\sqrt{\bar\alpha_t}
})  +\sqrt{1-\bar\alpha_{prev}-\sigma^2}\epsilon_t+\sigma^2\epsilon\]</span></p>
<p>其中 <span class="math inline">\(t\)</span> 和 <span class="math inline">\(prev\)</span>
可以相隔多个迭代步数，一般相隔20可以做到采样速度和采样质量比较好地平衡。所以一般DDPM要做1000步，而DDIM是需要50步就可以完成采样。</p>
<p>当这里的 <span class="math inline">\(\sigma\)</span>
选取0的时候，也就意味着变成了一个确定性采样的过程。此时的DDIM就变成了一个Flow
Models，事实上论文里也是这么做的。</p>
<h2 id="从不同角度看扩散模型">从不同角度看扩散模型</h2>
<p>前面我们DDPM的推导过程中，其实可以把扩散模型看成一个给定后验的<strong>多层VAE</strong>。即认为设定了
<span class="math inline">\(p(x_{1:T}|x_0)\)</span>
的形式，然后让模型来从潜变量中采样，最终生成图片。</p>
<p>而DDIM把这个过程变成了一个确定性过程，也就是说把潜变量和数据之间做了一个双射，所以此时也就可以看成<strong>Flow
Models</strong>的一个了</p>
<p>事实上，扩散模型的连续和离散其实对应着随机过程里的概念。一般来说，discrete
time指的是随机过程中的时间 <span class="math inline">\(t\)</span>
只能取离散整数值，而continous-time则指的是时间参数 <span class="math inline">\(t\)</span> 可以取连续值。discrete
time随机过程中的参数在一个离散的时间点只能改变一次；而continuous-time随机过程的参数则可以随时发生变化。</p>
<h3 id="ddpm和sde">DDPM和SDE</h3>
<p>我们在DDPM里的加噪过程。每一个time
step，我们都会按照如下的离散马尔可夫链进行加噪：</p>
<p><span class="math display">\[x_i = \sqrt{1 - \beta_i}x_{i-1} +
\sqrt{\beta_i} \epsilon_{i-1}, i=1,..., N\]</span></p>
<p>为了将上述过程连续化，我们需要引入连续时间随机过程。<strong>而连续时间其实就是让每个离散的时间间隔</strong>
<span class="math inline">\(\Delta t\)</span>
<strong>无限趋近于0，其实也等价于求出</strong> <span class="math inline">\(N \to \infty\)</span>
<strong>​时，上述马尔可夫链的极限</strong></p>
<p>在求极限之前，我们需要先引入一组辅助的noise scale <span class="math inline">\(\{\bar{\beta}_i = N \beta_i\}_{i=1}^N\)</span>
，并将上面的式子改写如下：</p>
<p><span class="math display">\[x_i = \sqrt{1 - \frac{\bar{\beta}_i}{N}
}x_{i-1} + \sqrt{\frac{\bar{\beta}_i}{N} }\epsilon_{i-1}, i = 1,...,
N\]</span></p>
<p>在 <span class="math inline">\(N \to \infty\)</span> ​时，上面的 <span class="math inline">\(\{\bar{\beta}_i\}_{i=1}^{N}\)</span>
就成了一个关于时间 <span class="math inline">\(t\)</span> 的连续函数 $
(t)$ ​，并且 <span class="math inline">\(t \in [0, 1]\)</span>
。随后，我们可以假设 <span class="math inline">\(\Delta t =
\frac{1}{N}\)</span> ​，在每个 <span class="math inline">\(i\Delta
t\)</span> 时刻，连续函数 <span class="math inline">\(\beta(t), x(t),
\epsilon(t)\)</span> 都等于之前的离散值，即：</p>
<p><span class="math display">\[\beta(\frac{i}{N}) = \bar{\beta}_i,
x(\frac{i}{N}) = x_i, \epsilon(\frac{i}{N})=\epsilon_i \]</span></p>
<p>在 <span class="math inline">\(t \in \{0, 1, ...,
\frac{N-1}{N}\}\)</span> ​以及 <span class="math inline">\(\Delta
t=\frac{1}{N}\)</span>
的情况下，我们就可以用连续函数改写之前的式子：</p>
<p><span class="math display">\[\begin{align} x(t+ \Delta t) &amp;=
\sqrt{1-\beta(t+\Delta t)\Delta t}\ x(t) + \sqrt{\beta(t+\Delta t)\Delta
t}\ \epsilon(t) \\ &amp; \approx x(t) - \frac{1}{2}\beta(t+\Delta t)
\Delta t\ x(t) + \sqrt{\beta(t+\Delta t)\Delta t}\ \epsilon(t) \\ &amp;
\approx x(t) - \frac{1}{2}\beta(t)\Delta t\ x(t) + \sqrt{\beta(t)\Delta
t}\ \epsilon(t) \end{align} \]</span></p>
<p>上面的近似只有在 <span class="math inline">\(\Delta t \ll 1\)</span>
时成立。我们将其再移项后就可以得到下式：</p>
<p><span class="math display">\[x(t+\Delta t) - x(t) \approx
-\frac{1}{2} \beta(t)\Delta t\ x(t) + \sqrt{\beta(t)\Delta t}\
\epsilon(t) \\ \mathrm{d} x = -\frac{1}{2}\beta(t)x \mathrm{d}t +
\sqrt{\beta(t)} \mathrm{d}w \]</span></p>
<p>其中， <span class="math inline">\(w\)</span> ​表示的就是Wiener
Process。这里面的第二个式子，就是一SDE方程。</p>
<p>至此，我们证明了DDPM连续化之后，就可以得到一个SDE方程，并且它是一种Variance
Preserving的SDE。<strong>Variance Preserving的含义是当</strong> <span class="math inline">\(t \to \infty\)</span>
<strong>时，它的方差依然有界</strong>。</p>
<p>与此<strong>反向过程也是一个SDE方程，称为reverse SDE</strong>：</p>
<p><span class="math display">\[\text{d}\mathbf{x}=
[\mathbf{f}(\mathbf{x}, t) - g^2(t)\nabla _{\mathbf{x} }\log
p(\mathbf{x})]\text{d}\mathbf{t} + g(t)\text{d}\mathbf{w}\]</span></p>
<p>这个反向过程中的未知量就只有分数函数 $ <em>x p</em>{t}(x)$
​。至此，DDPM和分数模型也产生了联系，实际上二者之间是相互等价的。而DDPM和分数模型本质上都是在学习这个reverse
SDE的解。 我们可以看到，DDPM每一步的去噪其实本质上与<strong>Annealed
Langevin dynamics</strong>是一模一样的。</p>
<p><img src="/images/Diffusion%20Model原理/3.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="ddim与ode">DDIM与ODE</h3>
<p>首先对于一个SDE，</p>
<p><span class="math display">\[\text{d}\mathbf{x}=
\mathbf{f}(\mathbf{x}, t)\text{d}\mathbf{t} +
g(t)\text{d}\mathbf{w}\]</span></p>
<p>我们写出它的<strong>福克-普朗克方程（Fokker-Planck
equation）</strong>：</p>
<p><span class="math display">\[\begin{align*} \nabla _{t}p(\mathbf{x},
t) &amp;= -\nabla _{\mathbf{x} }[\mathbf{f}(\mathbf{x}, t)p(\mathbf{x},
t)] + \frac{1}{2}g^{2}(t)\nabla _{\mathbf{x} }^{2}p(\mathbf{x}, t)\\
&amp;= -\nabla _{\mathbf{x} }[\mathbf{f}(\mathbf{x}, t)p(\mathbf{x}, t)
- \frac{1}{2}(g^{2}(t) - \sigma^{2}(t))\nabla_\mathbf{x}p(\mathbf{x},
t)] + \frac{1}{2}\sigma^{2}(t)\nabla _{\mathbf{x} }^{2}p(\mathbf{x},
t)\\   &amp;= -\nabla _{\mathbf{x} }[(\mathbf{f}(\mathbf{x}, t) -
\frac{1}{2}(g^{2}(t) - \sigma^{2}(t))\nabla_\mathbf{x}\log p(\mathbf{x},
t))p(\mathbf{x})] + \frac{1}{2}\sigma^{2}(t)\nabla _{\mathbf{x}
}^{2}p(\mathbf{x}, t)\\\end{align*}\]</span></p>
<p>现在我们把福克-普朗克方程变成了这样：</p>
<p><span class="math display">\[\nabla_{t}p(\mathbf{x}, t) =
-\nabla_{\mathbf{x} }[(\mathbf{f}(\mathbf{x}, t) - \frac{1}{2}(g^{2}(t)
- \sigma^{2}(t))\nabla_\mathbf{x}\log p(\mathbf{x}, t))p(\mathbf{x})] +
\frac{1}{2}\sigma^{2}(t)\nabla _{\mathbf{x} }^{2}p(\mathbf{x},
t)\]</span></p>
<p>其对应的SDE为：</p>
<p><span class="math display">\[\text{d}\mathbf{x}=
[\mathbf{f}(\mathbf{x}, t) - \frac{1}{2}(g^{2}(t) -
\sigma^{2}(t))\nabla_{\mathbf{x} }\log
p_{t}(\mathbf{x})]\text{d}\mathbf{t} +
\sigma(t)\text{d}\mathbf{w}\]</span></p>
<p>因为前后两个SDE是等价的，他们对应的 <span class="math inline">\(p_{t}(\mathbf{x})\)</span>
是一样的，意味着我们可以改变第二个SDE的方差 <span class="math inline">\(\sigma(t)\)</span> 。当我们取 <span class="math inline">\(\sigma(t)=0\)</span>
，可以得到一个<strong>常微分方程(Ordinary Differential Equation,
ODE)</strong>,</p>
<p><span class="math display">\[\text{d}\mathbf{x}=
[\mathbf{f}(\mathbf{x}, t) - \frac{1}{2}g^{2}(t)\nabla_ {\mathbf{x}
}\log p_{t}(\mathbf{x})]\text{d}\mathbf{t}\]</span></p>
<p><img src="/images/Diffusion%20Model原理/4.png" srcset="/img/loading.gif" lazyload></p>
<p>这个结论有什么作用呢？首先，我们其实更在乎的是边缘概率分布 <span class="math inline">\(q_t(x)\)</span> ，因为我们需要保证它在足够长的时刻
<span class="math inline">\(T\)</span> ， <span class="math inline">\(q_T(x)\)</span> 可以变成一个纯噪声，同时我们还需要
<span class="math inline">\(q_0(x)\)</span>
​符合原始数据分布。上述结论可以保证这一点。同时，扩散模型本质上是在学习一个扩散过程的逆过程，既然前向SDE存在一个对应的ODE，<strong>那么反向过程reverse
SDE其实也有一个对应的ODE，这个反向过程对应的ODE形式也是上面的式子</strong>。</p>
<p>而 DDIM 恰是一种确定性情形，所以我们自然会想到——能不能用 ODE
来描述一个 DDIM 呢？答案是肯定的。DDIM的公式如下：</p>
<p><span class="math display">\[\begin{align}
x_{t-1}&amp;=\sqrt{\bar\alpha_{t-1}
}x_\theta(x_t,t)+\sqrt{1-\bar\alpha_{t-1} }\epsilon_\theta(x_t,t)\\
&amp;=\frac{\sqrt{\bar\alpha_{t-1} }}{\sqrt{\bar\alpha_t}
}\left(x_t-\sqrt{1-\bar\alpha_t}\epsilon_\theta(x_t,t)\right)+\sqrt{1-\bar\alpha_{t-1}
}\epsilon_\theta(x_t,t) \end{align}\]</span></p>
<p>两边均减去 <span class="math inline">\(x_t\)</span> ，得：</p>
<p><span class="math display">\[\begin{align}
x_{t-1}-x_t&amp;=\frac{1}{\sqrt{\bar\alpha_t}
}\left[\left(\sqrt{\bar\alpha_{t-1}
}-\sqrt{\bar\alpha_t}\right)x_t-\left(\sqrt{\bar\alpha_{t-1}(1-\bar\alpha_t)}-\sqrt{\bar\alpha_t(1-\bar\alpha_{t-1})}\right)\epsilon_\theta(\mathbf
x_t,t)\right]\\ &amp;=\frac{1}{\sqrt{\bar\alpha_t}
}\left(\frac{\bar\alpha_{t-1}-\bar\alpha_t}{\sqrt{\bar\alpha_{t-1}
}+\sqrt{\bar\alpha_t}
}x_t-\frac{\bar\alpha_{t-1}-\bar\alpha_t}{\sqrt{\bar\alpha_{t-1}(1-\bar\alpha_t)}+\sqrt{\bar\alpha_t(1-\bar\alpha_{t-1})}
}\epsilon_\theta(x_t,t)\right)\\
&amp;=\frac{\bar\alpha_{t-1}-\bar\alpha_t}{\sqrt{\bar\alpha_t}
}\left(\frac{x_t}{\sqrt{\bar\alpha_{t-1} }+\sqrt{\bar\alpha_t}
}-\frac{\epsilon_\theta(\mathbf
x_t,t)}{\sqrt{\bar\alpha_{t-1}(1-\bar\alpha_t)}+\sqrt{\bar\alpha_t(1-\bar\alpha_{t-1})}
}\right) \end{align}\]</span></p>
<p>记 <span class="math inline">\(x(t)=x_t,\barα(t)=\barα_t\)</span>
，将 <span class="math inline">\(t-1\)</span> 换成 <span class="math inline">\(t−Δt\)</span> 并令 <span class="math inline">\(Δt→0\)</span> ，得：</p>
<p><span class="math display">\[\mathrm dx=\frac{\mathrm
d\bar\alpha(t)}{\sqrt{\bar\alpha(t)}
}\left(\frac{x(t)}{2\sqrt{\bar\alpha(t)}
}-\frac{\epsilon_\theta(x(t),t)}{2\sqrt{\bar\alpha(t)(1-\bar\alpha(t))}
}\right)=\frac{\bar\alpha&#39;(t)}{2\bar\alpha(t)}\left(x(t)-\frac{\epsilon_\theta(x(t),t)}{\sqrt{1-\bar\alpha(t)}
}\right)\mathrm dt \]</span></p>
<p><strong>这就是 DDIM 的 ODE 描述</strong>。</p>
<p>在 DDPM 的设置下，有 $f(x,t)=−β(t)x,g(t)= $ ，代入</p>
<p><span class="math display">\[\text{d}\mathbf{x}=
[\mathbf{f}(\mathbf{x}, t) - \frac{1}{2}g^{2}(t)\nabla_ {\mathbf{x}
}\log p_{t}(\mathbf{x})]\text{d}\mathbf{t}\]</span></p>
<p>得：</p>
<p><span class="math display">\[\mathrm
dx=\left[-\frac{1}{2}\beta(t)x-\frac{1}{2}\beta(t)\nabla_{\mathbf{x}
}\log p_{t}(\mathbf{x})\right]\mathrm
dt=-\frac{1}{2}\beta(t)\left[x+\nabla_{\mathbf{x} }\log
p_{t}(\mathbf{x})\right]\mathrm dt\]</span></p>
<p>与我们上面的式子对应。</p>
<p>既然引入了ODE，那么我们的模型就可以去学习如何解这个ODE，同时也可以引入各种传统的ODE
solver例如：Euler method, Runge–Kutta
method等一些方法。这就是为什么我们可以看到像Stable
Diffusion之类的模型会有那么多sampler的原因，本质上都是一些ODE
solver和SDE solver。但是后面的研究者发现，传统的ODE
solver在采样效果上比不过DDIM，这就非常奇怪了。DPM-Solver的作者在他们的论文中给出了原因：<strong>DDIM充分利用了diffusion
ODE的半线性结构（semi-linear structure），并且它是一个semi-linear
ODE的一阶Solver，而传统的ODE
solver并没有利用好这个半线性结构，因此DDIM的准确度会更高一些，因此采样效果也更好。</strong></p>
<p>这里还需要注意的点是，<strong>diffusion ODE这类模型相比diffusion
SDE存在着诸多好处</strong>，比如：</p>
<ul>
<li>没有随机性，ODE是一个确定性过程，可以以更快的速度收敛，因此可以达到更快的采样速度</li>
<li>由于是确定性过程，可以计算数据似然（likelihood）等。</li>
</ul>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/Stanford-CS236%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/" class="category-chain-item">Stanford CS236深度生成模型</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/" class="print-no-link">#生成模型</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Diffusion Model原理</div>
      <div>https://jia040223.github.io/2024/09/29/Diffusion Model原理/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Serendipity</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2024年9月29日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2024/09/30/%E6%9B%B4%E6%96%B0%E6%97%A5%E5%BF%97/" title="更新说明 2024.9.30">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">更新说明 2024.9.30</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/09/24/Score%20Based%20Models/" title="Score Based Models">
                        <span class="hidden-mobile">Score Based Models</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"Ug8725bpf4JJJkltPotjuquU-MdYXbMMI","appKey":"Po3fbdR9RiF08kxafXGlNgd5","path":"window.location.pathname","placeholder":"留言仅限讨论，严禁广告等行为","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"https://ug8725bp.api.lncldglobal.com","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
