

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Serendipity">
  <meta name="keywords" content="">
  
    <meta name="description" content="本学习笔记用于记录我学习Probabilistic Machine Learning的学习笔记，分享记录，也便于自己实时查看。 一、Probability基础知识 1. Probability space 概率空间是一个三元组$(Ω，F，P)$，其中$Ω$是样本空间，是实验可能结果的集合；$F$是事件空间，它是$Ω$所有可能子集的集合；$P$是概率函数，它是从事件$E \subsetΩ$到$[0,">
<meta property="og:type" content="article">
<meta property="og:title" content="[Probabilistic Machine Learning]: Fundamentals-Probability">
<meta property="og:url" content="https://jia040223.github.io/2024/10/08/Fundamentals-Probability/index.html">
<meta property="og:site_name" content="Serendipity&#39;s Blog">
<meta property="og:description" content="本学习笔记用于记录我学习Probabilistic Machine Learning的学习笔记，分享记录，也便于自己实时查看。 一、Probability基础知识 1. Probability space 概率空间是一个三元组$(Ω，F，P)$，其中$Ω$是样本空间，是实验可能结果的集合；$F$是事件空间，它是$Ω$所有可能子集的集合；$P$是概率函数，它是从事件$E \subsetΩ$到$[0,">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://jia040223.github.io/images/Fundamentals-Probability/0.png">
<meta property="article:published_time" content="2024-10-08T09:02:34.000Z">
<meta property="article:modified_time" content="2024-10-29T03:19:30.709Z">
<meta property="article:author" content="Serendipity">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="概率论与数理统计">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://jia040223.github.io/images/Fundamentals-Probability/0.png">
  
  
  
  <title>[Probabilistic Machine Learning]: Fundamentals-Probability - Serendipity&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"jia040223.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="Serendipity's Blog" type="application/atom+xml">
</head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Serendipity&#39;s Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/" target="_self">
                <i class="iconfont icon-link-fill"></i>
                <span>友链</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="[Probabilistic Machine Learning]: Fundamentals-Probability"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-10-08 17:02" pubdate>
          2024年10月8日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          4.6k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          39 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">[Probabilistic Machine Learning]: Fundamentals-Probability</h1>
            
            
              <div class="markdown-body">
                
                <p>本学习笔记用于记录我学习<strong>Probabilistic Machine Learning</strong>的学习笔记，分享记录，也便于自己实时查看。</p>
<h2 id="一、Probability基础知识">一、Probability基础知识</h2>
<h3 id="1-Probability-space">1. Probability space</h3>
<p>概率空间是一个三元组$(Ω，F，P)$，其中$Ω$是样本空间，是实验可能结果的集合；$F$是事件空间，它是$Ω$所有可能子集的集合；$P$是概率函数，它是从事件$E \subsetΩ$到$[0,1]$中的一个数(即$P: F→[0,1]$)的映射，它满足一定的一致性等要求，具体如下：</p>
<p><img src="/images/Fundamentals-Probability/1.png" srcset="/img/loading.gif" lazyload alt></p>
<h3 id="2-其它">2. 其它</h3>
<p>离散随机变量定义，连续随机变量定义，条件概率，贝叶斯公式…</p>
<p>具体略过</p>
<h2 id="二、常见分布">二、常见分布</h2>
<h3 id="1-Discrete-distributions">1. Discrete distributions</h3>
<h4 id="1-1-Bernoulli-and-binomial-distributions：">1.1 Bernoulli and binomial distributions：</h4>
<p><strong>伯努利分布</strong>和<strong>二项分布</strong>，也很熟悉了</p>
<p><img src="/images/Fundamentals-Probability/2.png" srcset="/img/loading.gif" lazyload alt></p>
<h4 id="1-2-Categorical-and-multinomial-distribution：">1.2 Categorical and multinomial distribution：</h4>
<p><img src="/images/Fundamentals-Probability/3.png" srcset="/img/loading.gif" lazyload alt><br>
其实也就是对伯努利分布和二项分布在<strong>更多的类别</strong>上的分布：</p>
<p><img src="/images/Fundamentals-Probability/4.png" srcset="/img/loading.gif" lazyload alt></p>
<h4 id="1-3-Poisson-distribution">1.3 Poisson distribution</h4>
<p><strong>泊松分布</strong>，本科课程也重点学习过**：**</p>
<p><img src="/images/Fundamentals-Probability/5.png" srcset="/img/loading.gif" lazyload alt></p>
<h4 id="1-4-Negative-binomial-distribution">1.4 Negative binomial distribution</h4>
<p><strong>负二项分布</strong>又称帕斯卡分布（巴斯卡分布），它表示，已知一个事件在伯努利试验中每次的成功的概率是$p$，在一连串伯努利实验中，直到失败$r$次，此时成功次数作为随机变量$x$。</p>
<p>$r=1$时，即为几何分布。</p>
<p><img src="/images/Fundamentals-Probability/6.png" srcset="/img/loading.gif" lazyload alt></p>
<h3 id="2-分布在R上的Continuous-distributions">2. 分布在R上的Continuous distributions</h3>
<h4 id="2-1-Gaussian-（Normal）">2.1 Gaussian （Normal）</h4>
<p><strong>高斯分布</strong>，最经典的分布</p>
<p><img src="/images/Fundamentals-Probability/7.png" srcset="/img/loading.gif" lazyload alt><br>
<img src="/images/Fundamentals-Probability/8.png" srcset="/img/loading.gif" lazyload alt></p>
<h4 id="2-2-Half-normal">2.2 Half-normal</h4>
<p><strong>半正态分布</strong>即一个高斯分布的绝对值（比如很多时候建模需要非负）</p>
<p><img src="/images/Fundamentals-Probability/9.png" srcset="/img/loading.gif" lazyload alt></p>
<h4 id="2-3-Student-t-distribution">2.3 Student t-distribution</h4>
<p><strong>t-分布</strong>也比较熟悉了：</p>
<p><img src="/images/Fundamentals-Probability/10.png" srcset="/img/loading.gif" lazyload alt></p>
<h4 id="2-4-Cauchy-distribution">2.4 Cauchy distribution</h4>
<p><strong>Cauchy distribution</strong>是t-分布的特例：</p>
<p><img src="/images/Fundamentals-Probability/11.png" srcset="/img/loading.gif" lazyload alt></p>
<h4 id="2-5-Laplace-distribution">2.5 Laplace distribution</h4>
<p><strong>拉帕拉斯分布</strong>也是很有名的分布，把高斯分布的平方改成了绝对值：</p>
<p><img src="/images/Fundamentals-Probability/12.png" srcset="/img/loading.gif" lazyload alt></p>
<h4 id="2-6-Sub-Gaussian-and-super-Gaussian-distributions">2.6 Sub-Gaussian and super-Gaussian distributions</h4>
<p>其实就是比较尾部衰减速度。<strong>超高斯分布</strong>式指随机过程$X$的<strong>四阶累计量</strong>恒大于零，并且关于其均值对称分布。而<strong>亚高斯分布</strong>就是恒小于零。</p>
<p>例如拉普拉斯分布就是超高斯分布一种，而<strong>均匀分布</strong>就是亚高斯分布一种。</p>
<p><img src="/images/Fundamentals-Probability/13.png" srcset="/img/loading.gif" lazyload alt><br>
具体对比如图：</p>
<p><img src="/images/Fundamentals-Probability/14.png" srcset="/img/loading.gif" lazyload alt></p>
<h3 id="3-分布在正实数上的Continuous-distributions">3. 分布在正实数上的Continuous distributions</h3>
<h4 id="3-1-Gamma-distribution">3.1 Gamma distribution</h4>
<p><strong>伽马分布</strong>以<strong>伽马函数</strong>为基础，非常灵活。</p>
<p>“指数分布”和“$χ^{2}$分布”都是伽马分布的特例。</p>
<p><img src="/images/Fundamentals-Probability/15.png" srcset="/img/loading.gif" lazyload alt><br>
概率分布的可视化如下：</p>
<p><img src="/images/Fundamentals-Probability/16.png" srcset="/img/loading.gif" lazyload alt></p>
<h4 id="3-2-Exponential-distribution">3.2 Exponential distribution</h4>
<p><strong>指数分布</strong>，伽马分布的特例。本科课程也重点学习过。</p>
<p><img src="/images/Fundamentals-Probability/17.png" srcset="/img/loading.gif" lazyload alt></p>
<h4 id="3-3-Chi-squared-distribution">3.3 Chi-squared distribution</h4>
<p>$χ^{2}$分布，伽马分布的特例。也比较熟悉了。</p>
<p><img src="/images/Fundamentals-Probability/18.png" srcset="/img/loading.gif" lazyload alt><br>
<img src="/images/Fundamentals-Probability/19.png" srcset="/img/loading.gif" lazyload alt></p>
<h4 id="3-4-Inverse-gamma">3.4 Inverse gamma</h4>
<p><strong>倒伽马分布</strong>是伽马分布变量的倒数。倒$χ^{2}$分布是其特例。</p>
<p><img src="/images/Fundamentals-Probability/20.png" srcset="/img/loading.gif" lazyload alt></p>
<h4 id="3-5-Pareto-distribution">3.5 Pareto distribution</h4>
<p><strong>帕累托分布</strong>是以意大利经济学家维弗雷多·帕雷托命名的。 是从大量真实世界的现象中发现的<strong>幂定律</strong>分布。其形式如下：</p>
<p><img src="/images/Fundamentals-Probability/21.png" srcset="/img/loading.gif" lazyload alt><br>
可以注意到，对概率分布取对数，则会得到一个线性函数，所以NLP中大名鼎鼎的<strong>齐夫定律</strong>便服从这个分布：</p>
<p><img src="/images/Fundamentals-Probability/22.png" srcset="/img/loading.gif" lazyload alt><br>
<img src="/images/Fundamentals-Probability/23.png" srcset="/img/loading.gif" lazyload alt><br>
对于尾部比较大的分布的建模，帕累托分布是有用的，现实中许多形式的数据都具有这种特性。如：</p>
<ul>
<li>财富在个人之间的分布（80%的人掌握20%的财富）</li>
<li>人类居住区的大小</li>
<li>对维基百科条目的访问</li>
</ul>
<p>一般认为这是因为数据是由各种潜在因素产生的，当这些潜在因素混合在一起时，自然会导致这种重尾的分布。</p>
<p><img src="/images/Fundamentals-Probability/24.png" srcset="/img/loading.gif" lazyload alt><br>
其概率分布的直观展示如下：</p>
<p><img src="/images/Fundamentals-Probability/25.png" srcset="/img/loading.gif" lazyload alt></p>
<h3 id="4-分布在-0-1-上的Continuous-distributions">4. 分布在[0, 1]上的Continuous distributions</h3>
<h4 id="4-1-Beta-distribution">4.1 Beta distribution</h4>
<p>所谓的以$\alpha, \beta$为参数的 <strong>Beta 分布</strong>$f(x; \alpha, \beta)$，其实描述的就是我们在做抛硬币实验的过程中，我们当前如果已经观测到$\alpha + 1$次正面，$\beta + 1$次反面，那么此时硬币正面朝上的真实概率的可能性分布。</p>
<p>即，Beta 分布是一个作为伯努利分布和二项式分布的<strong>共轭先验分布</strong>的密度函数。</p>
<p><img src="/images/Fundamentals-Probability/26.png" srcset="/img/loading.gif" lazyload alt></p>
<h3 id="5-Multivariate-continuous-distributions">5.Multivariate continuous distributions</h3>
<h4 id="5-1-Multivariate-normal-Gaussian">5.1 Multivariate normal (Gaussian)</h4>
<p>多元高斯函数是最重要最经典的多元分布了，下面会专门详细学习。</p>
<h4 id="5-2-Multivariate-Student-distribution">5.2 Multivariate Student distribution</h4>
<p><strong>多元t分布</strong>的形状与多元高斯比较类似，主要是峰值更低，尾部缩减更慢。</p>
<p><img src="/images/Fundamentals-Probability/27.png" srcset="/img/loading.gif" lazyload alt><br>
当$v$趋近于无穷时，其逐渐逼近多元高斯分布，其均值和协方差矩阵如下：</p>
<p><img src="/images/Fundamentals-Probability/28.png" srcset="/img/loading.gif" lazyload alt></p>
<h4 id="5-3-Circular-normal-von-Mises-Fisher-distribution">5.3 Circular normal (von Mises Fisher) distribution</h4>
<p>现实中，有些数据仅仅分布于一个<strong>单位球</strong>上，而不是欧式空间的任何一点都有概率。此时，<strong>冯·米塞斯分布</strong>就是针对这种情况。</p>
<p>冯·米塞斯分布就是高斯分布在单位球上的拓展。</p>
<p><img src="/images/Fundamentals-Probability/29.png" srcset="/img/loading.gif" lazyload alt></p>
<h4 id="5-4-Matrix-normal-distribution-MN">5.4 Matrix normal distribution (MN)</h4>
<p>Matrix normal distribution是作用于矩阵的正态分布，其定义如下：</p>
<p><img src="/images/Fundamentals-Probability/30.png" srcset="/img/loading.gif" lazyload alt><br>
它可以转化为作用于向量上的多元高斯分布，只要将<strong>矩阵正态分布</strong>进行<strong>向量化</strong>处理便可以得到多元正态分布形式，如上所示。</p>
<p>这两者<strong>完全等价</strong> (证明过程可以参考<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Matrix_normal_distribution">https://en.wikipedia.org/wiki/Matrix_normal_distribution</a>)，公式中的符号$\otimes$表示 <strong>Kronecker积</strong>，$V\otimes U$表示多元正态分布的协方差矩阵；符号$\text{vec}\left(\cdot\right)$表示将给定矩阵<strong>按列</strong>组织成一个向量。</p>
<p>这两者完全等价，但在实践中，考虑到协方差矩阵$V\otimes U\in\mathbb{R}^{(mn)\times (mn)}$，假设我们想生成一个大小为$100\times 200$的随机矩阵$X$，并要求矩阵$X$在概率上服从矩阵正态分布。此时，若利用多元正态分布进行生成，则需要协方差矩阵$V\otimes U$的大小为$(100\times 200)\times (100\times 200)=20000\times 20000$，元素数量为$4\times 10^8$，显然，这个数字很惊人，毕竟存储这么大的矩阵就需要消耗计算机比较多的内存了，所以<strong>矩阵正态分布</strong>有它的优势。</p>
<h4 id="5-5-Wishart-distribution">5.5 Wishart distribution</h4>
<p><strong>Wishart分布</strong>是<strong>伽马分布</strong>的多元形式，也是十分重要的分布。卡方分布也是它的特例。</p>
<p><img src="/images/Fundamentals-Probability/31.png" srcset="/img/loading.gif" lazyload alt><br>
多元高斯分布和Wishart分布有很紧密的联系，设$Y_{1}\ldots Y_{n}\ iid\sim\ N(0,\Sigma)$，其中$Y_{i}(i = 1,\ldots,n)$是$p$维列向量，则随机矩阵$W = \sum_{i = 1}^{n}{Y_{i}Y_{i}^{T}}$的分布就是wishart分布，记作$W\sim Wishart(\Sigma,n)$，可以发现，当协方差矩阵退化为单位1，得到的就是卡方分布。</p>
<h4 id="5-6-Inverse-Wishart-distribution">5.6 Inverse Wishart distribution</h4>
<p>与倒伽马分布和伽马分布的关系类似，服从Wishart分布的随机变量的倒数就服从<strong>倒Wishart分布</strong>。</p>
<p><img src="/images/Fundamentals-Probability/32.png" srcset="/img/loading.gif" lazyload alt><br>
<img src="/images/Fundamentals-Probability/33.png" srcset="/img/loading.gif" lazyload alt></p>
<h4 id="5-7-Dirichlet-distribution">5.7 Dirichlet distribution</h4>
<p><strong>狄利克雷分布</strong>是<strong>Beta分布</strong>的多元形式，自然的其也是<strong>多项分布的共轭先验分布</strong>。</p>
<p><strong>共轭先验</strong>在Beta分布里面已经提到过，目前笔者也只是稍微了解了一点，后面笔者也打算专门去深入了解一下。</p>
<p><img src="/images/Fundamentals-Probability/34.png" srcset="/img/loading.gif" lazyload alt><br>
狄利克雷分布可以用来定义“不确定性”的问题。考虑一个3面骰子。如果我们知道每个结果都是等可能的，我们可以使用“尖峰”对称狄利克雷，如Dir(20,20,20)，即我们确信结果将是不可预测的。相比之下，如果我们不确定结果会是什么样子(例如，它可能是一个有偏的骰子)，那么我们可以使用“平坦”对称狄利克雷，例如Dir(1,1,1)，它可以生成广泛的可能的结果分布。</p>
<p><img src="/images/Fundamentals-Probability/35.png" srcset="/img/loading.gif" lazyload alt><br>
<img src="/images/Fundamentals-Probability/36.png" srcset="/img/loading.gif" lazyload alt><br>
<img src="/images/Fundamentals-Probability/37.png" srcset="/img/loading.gif" lazyload alt></p>
<h2 id="三、高斯联合分布">三、高斯联合分布</h2>
<p>实践中最广泛使用的连续随机变量联合概率分布是多元高斯分布了，也叫多元正态分布(MVN)。这部分是因为其在数学上很方便，而且高斯分布假设在许多情况下是相当合理的。</p>
<h3 id="1-The-multivariate-normal">1. The multivariate normal</h3>
<h4 id="定义">定义</h4>
<p>多元高斯分布的定义应该都很熟悉了：</p>
<p><img src="/images/Fundamentals-Probability/38.png" srcset="/img/loading.gif" lazyload alt><br>
可以对协方差矩阵进行限制：</p>
<p><img src="/images/Fundamentals-Probability/39.png" srcset="/img/loading.gif" lazyload alt></p>
<h4 id="Gaussian-shells">Gaussian shells</h4>
<p>随着维度$D$的增加，样本$x∼N(0,I_D)$中大部分点并不位于原点附近，而是集中在距离原点$r = \sqrt{D}$​ 处的一个薄壳或环形区域。这是因为虽然概率密度随着$\frac{r^2}{2}$指数衰减（距离增大概率密度减小），但球体的体积 随距离增加而增加，导致大多数点集中在距离原点$\sqrt{D}$处的一个<strong>薄环</strong>上。此现象称为“高斯肥皂泡”。</p>
<p>计算点$x$到原点的平方距离$d(x) = \sum_{i=1}^{D} x_i^2$​，其中$x_i \sim N(0, 1)$。</p>
<ul>
<li>期望值：$\mathbb{E}[d^2] = D$。</li>
<li>方差：$\text{Var}(d^2) = D$。</li>
</ul>
<p>所以随着D的增大，<strong>the coefficient of variation</strong>（标准差与期望的比值）会趋近于0</p>
<p><img src="/images/Fundamentals-Probability/40.png" srcset="/img/loading.gif" lazyload alt><br>
<img src="/images/Fundamentals-Probability/41.png" srcset="/img/loading.gif" lazyload alt></p>
<h4 id="Marginals-and-conditionals-of-an-MVN">Marginals and conditionals of an MVN</h4>
<p>对于一个满足多元高斯分布的向量$x$进行分块为$x1，x2$，会发现其<strong>边缘分布</strong>均为高斯分布，<strong>条件分布</strong>也均为高斯分布。</p>
<p>并且$p(x_1|x_2)$的后验均值是$x_2$的线性函数，但协方差于$x_2$无关，这是高斯分布的一个特殊性质。具体如下：</p>
<p><img src="/images/Fundamentals-Probability/42.png" srcset="/img/loading.gif" lazyload alt></p>
<h4 id="其它表达形式">其它表达形式</h4>
<p>高斯分布有其它表达形式，这些形式有对应的优势，例如边缘化公式在<strong>矩形式</strong>下更简单，而条件化公式在<strong>信息形式</strong>下更简单</p>
<p><img src="/images/Fundamentals-Probability/43.png" srcset="/img/loading.gif" lazyload alt></p>
<h3 id="2-Linear-Gaussian-systems">2. Linear Gaussian systems</h3>
<p>线性高斯系统定义如下，一个变量$z$和条件分布$p(y|z)$均为高斯分布：</p>
<p><img src="/images/Fundamentals-Probability/44.png" srcset="/img/loading.gif" lazyload alt><br>
此时<strong>联合分布</strong>$p(z,y)$的形式如下：</p>
<p><img src="/images/Fundamentals-Probability/45.png" srcset="/img/loading.gif" lazyload alt><br>
而后验分布$p(z|y)$也是一个高斯分布：</p>
<p><img src="/images/Fundamentals-Probability/46.png" srcset="/img/loading.gif" lazyload alt></p>
<h2 id="四、The-exponential-family">四、The exponential family</h2>
<p><strong>指数族</strong>包括了众多上面提到的常见分布，比如高斯分布、二项分布、多项式分布、 泊松分布、gamma分布、beta分布等等。</p>
<p>其在机器学习里面起着至关重要的作用，主要因为其独特的优点：</p>
<p><img src="/images/Fundamentals-Probability/47.png" srcset="/img/loading.gif" lazyload alt><br>
<img src="/images/Fundamentals-Probability/48.png" srcset="/img/loading.gif" lazyload alt></p>
<h3 id="1-定义">1. 定义</h3>
<p>指数族分布（Exponential Family Distribution）： 指数族分布是一类可以写成如下形式的分布：<br>
$$p(x|\eta) = h(x) \exp \left( \eta^T T(x) - A(\eta) \right)$$<br>
其中，$\eta$是自然参数，$T(x)$是充分统计量，$A(\eta)$是归一化常数，确保概率分布的积分为1。具体定义如下：</p>
<p><img src="/images/Fundamentals-Probability/49.png" srcset="/img/loading.gif" lazyload alt><br>
在数族分布中，如果自然参数$\eta$之间相互独立，则可以更方便地进行推导和计算。所谓的独立性意味着没有非零的$\eta$满足$\eta^T T(x) = 0$，即自然参数$\eta$不能通过其他参数线性组合来为零。此时我们称一个指数族分布为<strong>最小</strong>，因为这意味着我们不能通过减少自然参数的数量来进一步简化分布的参数化，否则分布会变得冗余。</p>
<p>在多项式分布中，由于参数有一个和为1的约束条件，导致自然参数之间并不完全独立。因此，严格来说，多项式分布并不是最小指数族。但尽管多项式分布中自然参数有依赖性，但可以通过<strong>重新参数化</strong>，将$K$个参数中的一个去掉，使用$K-1$个独立的参数来表示整个分布。这样可以将原来的问题转换为最小指数族的形式，使得参数之间更加独立。</p>
<p><img src="/images/Fundamentals-Probability/50.png" srcset="/img/loading.gif" lazyload alt></p>
<h3 id="2-例子">2. 例子</h3>
<h4 id="2-1-伯努利分布：">2.1 伯努利分布：</h4>
<p>$$\begin{align*} P(x|\mu) &amp;=\mu^x(1-\mu)^{1-x} \&amp;=exp (ln(\mu^x(1-\mu)^{1-x})) \&amp;=exp(xln(\frac{\mu}{1-\mu})+ln(1-\mu)) \end{align*}$$</p>
<p>对比可知有如下关系:</p>
<ul>
<li>[规范参数]$\eta = \phi(\mu)=ln(\frac{\mu}{1-\mu})$</li>
<li>[充分统计量]$T(x)=x$</li>
<li>[累积函数]$A(\eta)=-ln(1-\mu)$</li>
<li>[基础度量值]$h(x)=1$<br>
*$\lambda = logistic(\eta)=\frac{1}{1+e^{-\eta}}$</li>
</ul>
<p><img src="/images/Fundamentals-Probability/51.png" srcset="/img/loading.gif" lazyload alt><br>
上面也提到了多项式分布怎么减少参数量（让参数变为互相独立的）。</p>
<h4 id="2-2-Categorical-distribution">2.2 Categorical distribution</h4>
<p>与伯努利分布类似，由于参数求和为1，所以独立变量只有$K-1$个：</p>
<p><img src="/images/Fundamentals-Probability/52.png" srcset="/img/loading.gif" lazyload alt></p>
<h4 id="2-3-单变量高斯分布">2.3 单变量高斯分布</h4>
<p>高斯分布可做如下变换：</p>
<p>$$\begin{align*} P(x|\mu,\sigma^2)&amp;=\frac{1}{\sqrt{2\pi\sigma^2}}exp(-\frac{1}{2\sigma^2}(x-\mu)^2)  \&amp;=\frac{1}{\sqrt{2\pi}}exp(\frac{\mu}{\sigma^2}x-\frac{1}{2\sigma^2}x^2-\frac{1}{2\sigma^2}\mu^2-ln\sigma) \end{align*}$$</p>
<p>同样对比可知:</p>
<ul>
<li>[规范参数]$\eta = \phi(\lambda)=[\frac{\mu}{\sigma^2},-\frac{1}{2\sigma^2}]$</li>
<li>[充分统计量]$T(x)=[x,x^2]$</li>
<li>[累积函数]$A(\eta)=\frac{1}{2\sigma^2}\mu^2+ln\sigma$</li>
<li>[基础度量值]$h(x)=\frac{1}{\sqrt{2\pi}}$</li>
<li>因为高斯模型有两个参数,所以两个向量长度都为2</li>
</ul>
<p><img src="/images/Fundamentals-Probability/53.png" srcset="/img/loading.gif" lazyload alt><br>
如果限制$\sigma^{2} = 1$，则有如下形式：</p>
<p><img src="/images/Fundamentals-Probability/54.png" srcset="/img/loading.gif" lazyload alt><br>
此时$h(x)$不再是常数。</p>
<h4 id="2-4-多元高斯分布">2.4 多元高斯分布</h4>
<p>与单变量高斯分布推导类似，但比较复杂，如下所示：</p>
<p><img src="/images/Fundamentals-Probability/55.png" srcset="/img/loading.gif" lazyload alt><br>
<img src="/images/Fundamentals-Probability/56.png" srcset="/img/loading.gif" lazyload alt></p>
<h4 id="2-5-不是指数族的例子">2.5 不是指数族的例子</h4>
<p>分布族为指数族的必要条件为它有共同支撑集，也即$S_\theta = {x: p(x) &gt; 0}$与$\theta$无关。</p>
<p>比如说均匀分布$R(0, \theta)$就没有共同支撑集（因为它非零的区域为$[0,\theta]$），所以它不可能是指数族分布。</p>
<p><img src="/images/Fundamentals-Probability/57.png" srcset="/img/loading.gif" lazyload alt></p>
<h3 id="3-重要性质">3. 重要性质</h3>
<h4 id="Log-partition-function">Log partition function</h4>
<p>对数配分函数$A(\eta)$有如下性质：</p>
<p><img src="/images/Fundamentals-Probability/58.png" srcset="/img/loading.gif" lazyload alt><br>
直接从定义证明即可</p>
<h4 id="自然参数和矩参数转换">自然参数和矩参数转换</h4>
<p>对数分区函数$A(\eta)$的梯度等于充分统计量的期望，也就是矩参数（或均值参数）。即：$m = E[T(x)] = \nabla_\eta A(\eta)$这表明我们可以通过计算$A(\eta)$的梯度，从自然参数$\eta$得到对应的矩参数$m$。</p>
<p>如果指数族是最小的，则可以从矩参数$m$转换回自然参数$\eta$。这一过程通过对函数$A(\eta)$的<strong>凸共轭函数</strong>（convex conjugate）$A^<em>(m)$实现，公式为：$\eta = \nabla_m A^</em>(m)$</p>
<p>其中，凸共轭函数$A^<em>(m)$定义为：$A^</em>(m) = \sup_{\eta \in \Omega} \left( m^T \eta - A(\eta) \right)$这意味着通过$A^*$的梯度可以从矩参数$m$转回自然参数$\eta$。</p>
<p><img src="/images/Fundamentals-Probability/59.png" srcset="/img/loading.gif" lazyload alt></p>
<h3 id="4-指数族的极大似然估计">4. 指数族的极大似然估计</h3>
<p>指数族模型的似然函数形式：对于指数族分布模型，其似然函数可以写成以下形式：$p(D|\eta) = \prod_{n=1}^N h(x_n) \exp \left( \eta^T \sum_{n=1}^N T(x_n) - N A(\eta) \right)$</p>
<p>上式可以化简为：</p>
<p>$$p(D|\eta) \propto \exp \left( \eta^T T(D) - N A(\eta) \right)$$</p>
<p>这里$T(D)$是数据集的<strong>充分统计量</strong>之和：<br>
$$T(D) = \left[ \sum_{n=1}^N T_1(x_n), \ldots, \sum_{n=1}^N T_K(x_n) \right]$$<br>
不同的分布对应不同的充分统计量，例如：</p>
<ul>
<li>对于Bernoulli分布，充分统计量$T(D)$为：$T(D) = \left[ \sum_n I(x_n = 1) \right]$</li>
<li>对于一维高斯分布，充分统计量$T(D)$为：$T(D) = \left[ \sum_n x_n, \sum_n x_n^2 \right]$</li>
</ul>
<p><strong>Pitman-Koopman-Darmois定理</strong>说明在某些正则条件下，指数族分布是唯一具有有限充分统计量的分布族。也就是说，在指数族分布中，充分统计量的个数不依赖于数据集的大小。</p>
<p><img src="/images/Fundamentals-Probability/60.png" srcset="/img/loading.gif" lazyload alt><br>
给定数据集$D$，指数族分布的对数似然函数为：<br>
$$\log p(D|\eta) = \eta^T T(D) - N A(\eta) + \text{const}$$</p>
<p>由于$-A(\eta)$是自然参数$\eta$的凸函数，而$\eta^T T(D)$是线性函数，因此可以得出：对数似然函数是凸的，从而存在唯一的全局最大值。</p>
<p>我们对对数似然函数求导，导数如下：<br>
$$\nabla_\eta \log p(D|\eta) = T(D) - N E[T(x)]$$</p>
<p>对于单个数据点$x$，导数为：<br>
$$\nabla_\eta \log p(x|\eta) = T(x) - E[T(x)]$$</p>
<p>至于$E[T(x)]$，我们用数据集进行估计即可：<br>
$$E[T(x)] = \frac{1}{N} \sum_{n=1}^N T(x_n)$$</p>
<p><img src="/images/Fundamentals-Probability/61.png" srcset="/img/loading.gif" lazyload alt></p>
<h2 id="五、随机变量之间的变换">五、随机变量之间的变换</h2>
<h3 id="1-双射">1. 双射</h3>
<p>双射的变换公式很熟悉了，主要就是涉及到<strong>雅可比矩阵</strong>行列式：</p>
<p><img src="/images/Fundamentals-Probability/62.png" srcset="/img/loading.gif" lazyload alt></p>
<h3 id="2-蒙特卡罗近似">2. 蒙特卡罗近似</h3>
<p>也很熟悉了，就是采样估计：</p>
<p><img src="/images/Fundamentals-Probability/63.png" srcset="/img/loading.gif" lazyload alt></p>
<h3 id="3-Probability-integral-transform">3. Probability integral transform</h3>
<p>这个其实就是从均匀分布采样，然后通过逆映射进行计算，这样就相当于从原分布中进行采样了。也比较熟悉了：</p>
<p><img src="/images/Fundamentals-Probability/64.png" srcset="/img/loading.gif" lazyload alt></p>
<h2 id="六、-马尔可夫链">六、 马尔可夫链</h2>
<p>马尔可夫链涉及的知识比较多，书上讲的也都是比较基础的，本科课程也学习过。主要记录一下之前没见过的：</p>
<p>马尔可夫链的最大似然估计：</p>
<p><img src="/images/Fundamentals-Probability/65.png" srcset="/img/loading.gif" lazyload alt><br>
<strong>MAP estimation：<strong>解决数据稀疏的问题，引入了</strong>Dirichlet先验</strong>：</p>
<p><img src="/images/Fundamentals-Probability/66.png" srcset="/img/loading.gif" lazyload alt></p>
<h2 id="七、比较两个分布的相似度">七、比较两个分布的相似度</h2>
<h3 id="1-f-散度">1. f-散度</h3>
<p>f散度是一个函数，这个函数用来衡量两个概率密度p和q的区别，也就是衡量这两个分布多么的相同或者不同。像$KL$散度和$JS$散度都是它的一种特例</p>
<p>f散度定义如下：</p>
<p>$${D_f}(\mathcal P_1|\mathcal P_2)=\int f (\frac{p_2(x)}{p_1(x)})\cdot p_1(x)\mathrm d x=\mathbb E_{x\sim\mathcal P_1}\left[f(\frac{p_2(x)}{p_1(x)})\right] \$$</p>
<p>$f()$就是不同的散度函数，$D_f$就是在f散度函数下，两个分布的差异。规定</p>
<p>*$f$是凸函数(为了用琴生不等式)<br>
*$f ( 1 ) = 0$(如果两个分布一样，刚好公式=0)</p>
<p><img src="/images/Fundamentals-Probability/67.png" srcset="/img/loading.gif" lazyload alt></p>
<p>下面给出一些常见的f-散度例子：</p>
<h4 id="KL-散度">KL 散度</h4>
<p>当$f( r ) = rlog( r )$时，f-散度变为 KL 散度，公式为：</p>
<p>$$D_{KL}(p || q) = \int p(x) \log \frac{p(x)}{q(x)} dx$$</p>
<h4 id="α-散度-Alpha-Divergence">α-散度 (Alpha Divergence)</h4>
<p>当$f(x) = \frac{4}{1 - \alpha^2} (1 - x^{\frac{1+\alpha}{2}})$时，f-散度变为 α-散度，公式为：</p>
<p>$$D^\alpha_A (p || q) = \frac{4}{1 - \alpha^2} \left( 1 - \int p(x)^{\frac{1+\alpha}{2}} q(x)^{\frac{1-\alpha}{2}} dx \right)$$</p>
<p>其中，$\alpha \neq \pm 1$。另一种常用的参数化方式（Minka 方式）为：</p>
<p>$$DD^\alpha_M(p || q) = \frac{1}{\alpha(1-\alpha)} \left( 1 - \int p(x)^\alpha q(x)^{1-\alpha} dx \right)$$</p>
<ul>
<li>当$\alpha \to 0$时，α-散度趋向于$D_{KL}(q||p)$。</li>
<li>当$\alpha \to 1$时，α-散度趋向于$D_{KL}(p||q)$。</li>
<li>当$\alpha = 0.5$时，α-散度等于 Hellinger 距离（见下）。</li>
</ul>
<p><img src="/images/Fundamentals-Probability/70.png" srcset="/img/loading.gif" lazyload alt></p>
<h4 id="Hellinger-距离-Hellinger-Distance">Hellinger 距离 (Hellinger Distance)</h4>
<p>平方的 Hellinger 距离定义为：</p>
<p>$$D_H^2(p || q) = \frac{1}{2} \int \left( \sqrt{p(x)} - \sqrt{q(x)} \right)^2 dx$$</p>
<p>这相当于 f-散度，其中$f( r ) = (\sqrt{r} - 1)^2$。</p>
<p><img src="/images/Fundamentals-Probability/71.png" srcset="/img/loading.gif" lazyload alt></p>
<h4 id="卡方距离-Chi-Squared-Distance">卡方距离 (Chi-Squared Distance)</h4>
<p>卡方距离定义为：</p>
<p>$$\chi^2(p || q) = \frac{1}{2} \int \frac{(q(x) - p(x))^2}{q(x)} dx$$</p>
<p>这对应于 f-散度，其中$f( r ) = ( r - 1 )^2$。</p>
<p><img src="/images/Fundamentals-Probability/72.png" srcset="/img/loading.gif" lazyload alt></p>
<h3 id="2-积分概率度量-Integral-Probability-Metrics-IPM">2. 积分概率度量 (Integral Probability Metrics, IPM)</h3>
<p>IPM 也用于计算两个分布$P$和$Q$之间的差异，其定义为：</p>
<p>$$<br>
D_F(P, Q) = \sup_{f \in F} \left| \mathbb{E}<em>{p(x)}[f(x)] - \mathbb{E}</em>{q(x’)}[f(x’)] \right|<br>
$$</p>
<p>其中，$F$是一类“光滑”的函数。常见的 IPM 度量包括：</p>
<ul>
<li><strong>最大均值差异 (Maximum Mean Discrepancy, MMD)</strong></li>
</ul>
<p>如果$F$是在正定核函数下的 <strong>RKHS</strong>（再生核希尔伯特空间），则对应的 IPM 被称为最大均值差异（MMD）。</p>
<ul>
<li><strong>Wasserstein 距离 (Wasserstein Distance)</strong></li>
</ul>
<p>如果$F$是满足 <strong>Lipschitz</strong> 条件的函数类$F = { ||f||_L \leq 1 }$，即 Lipschitz 常数有界（例如为1）的函数集合，则 IPM 变为 Wasserstein-1 距离：</p>
<p>$$<br>
W_1(P, Q) = \sup_{||f||<em>L \leq 1} \left| \mathbb{E}</em>{p(x)}[f(x)] - \mathbb{E}_{q(x)}[f(x’)] \right|<br>
$$</p>
<p><img src="/images/Fundamentals-Probability/68.png" srcset="/img/loading.gif" lazyload alt><br>
<img src="/images/Fundamentals-Probability/69.png" srcset="/img/loading.gif" lazyload alt></p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/Probabilistic-Machine-Learning/" class="category-chain-item">Probabilistic Machine Learning</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="print-no-link">#机器学习</a>
      
        <a href="/tags/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/" class="print-no-link">#概率论与数理统计</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>[Probabilistic Machine Learning]: Fundamentals-Probability</div>
      <div>https://jia040223.github.io/2024/10/08/Fundamentals-Probability/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Serendipity</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2024年10月8日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2024/10/09/%E9%9D%92%E7%94%98%E5%A4%A7%E7%8E%AF%E7%BA%BF/" title="[旅游日志] :青甘大环线">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">[旅游日志] :青甘大环线</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/09/30/%E6%9B%B4%E6%96%B0%E6%97%A5%E5%BF%97/" title="更新说明 2024.9.30">
                        <span class="hidden-mobile">更新说明 2024.9.30</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"Ug8725bpf4JJJkltPotjuquU-MdYXbMMI","appKey":"Po3fbdR9RiF08kxafXGlNgd5","path":"window.location.pathname","placeholder":"留言仅限讨论，严禁广告等行为","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"https://ug8725bp.api.lncldglobal.com","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
