

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Serendipity">
  <meta name="keywords" content="">
  
    <meta name="description" content="本学习笔记用于记录我学习Probabilistic Machine Learning的学习笔记，分享记录，也便于自己实时查看。 前面Probability部分重点是关注给定参数\(\theta\)后，数据\(D\)的分布，即\(P(D|\theta)\)，而Statistics部分则是关注给定数据分布下，参数\(\theta\)的概率，即\(P(\theta|D)\)。 一、贝叶斯统计 贝">
<meta property="og:type" content="article">
<meta property="og:title" content="[Probabilistic Machine Learning]: Fundamentals-Statistics">
<meta property="og:url" content="https://jia040223.github.io/2024/10/14/Fundamentals-Statistics/index.html">
<meta property="og:site_name" content="Serendipity&#39;s Blog">
<meta property="og:description" content="本学习笔记用于记录我学习Probabilistic Machine Learning的学习笔记，分享记录，也便于自己实时查看。 前面Probability部分重点是关注给定参数\(\theta\)后，数据\(D\)的分布，即\(P(D|\theta)\)，而Statistics部分则是关注给定数据分布下，参数\(\theta\)的概率，即\(P(\theta|D)\)。 一、贝叶斯统计 贝">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://jia040223.github.io/images/Fundamentals-Probability/0.png">
<meta property="article:published_time" content="2024-10-14T06:17:48.000Z">
<meta property="article:modified_time" content="2024-10-27T10:47:21.223Z">
<meta property="article:author" content="Serendipity">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="数学">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://jia040223.github.io/images/Fundamentals-Probability/0.png">
  
  
  
  <title>[Probabilistic Machine Learning]: Fundamentals-Statistics - Serendipity&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"jia040223.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="Serendipity's Blog" type="application/atom+xml">
</head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Serendipity&#39;s Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/" target="_self">
                <i class="iconfont icon-link-fill"></i>
                <span>友链</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="[Probabilistic Machine Learning]: Fundamentals-Statistics"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-10-14 14:17" pubdate>
          2024年10月14日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          8.5k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          71 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">[Probabilistic Machine Learning]: Fundamentals-Statistics</h1>
            
            
              <div class="markdown-body">
                
                <p>本学习笔记用于记录我学习<strong>Probabilistic Machine
Learning</strong>的学习笔记，分享记录，也便于自己实时查看。</p>
<p>前面<strong>Probability</strong>部分重点是关注给定参数<span class="math inline">\(\theta\)</span>后，数据<span class="math inline">\(D\)</span>的分布，即<span class="math inline">\(P(D|\theta)\)</span>，而<strong>Statistics</strong>部分则是关注给定数据分布下，参数<span class="math inline">\(\theta\)</span>的概率，即<span class="math inline">\(P(\theta|D)\)</span>。</p>
<h2 id="一贝叶斯统计">一、贝叶斯统计</h2>
<p>贝叶斯统计也是比较熟悉了，主要就是用贝叶斯公式进行计算后验：</p>
<p><img src="/images/Fundamentals-Statistics/1.png" srcset="/img/loading.gif" lazyload><br>
这里<span class="math inline">\(P(\theta)\)</span>叫做先验，<span class="math inline">\(P(\theta|D)\)</span>是后验，<span class="math inline">\(P(D|\theta)\)</span>叫做似然。</p>
<p>书中以抛硬币实验来讲述了贝叶斯统计的众多概率，这里简单总结一下：</p>
<ul>
<li><strong>Prior</strong>
：均匀分布或者Beta分布，抛硬币我们可以选择Beta分布来指定更强的先验：
<span class="math display">\[p(θ)=Beta(θ∣α,β)∝θ ^{α−1}  (1−θ)
^{β−1}\]</span></li>
<li><strong>Posterior</strong>： 在Beta分布先验条件下可计算得到后验为：
<span class="math display">\[p(θ∣D)∝θ^{N_1}​(1−θ)^{N_0}​⋅θ^{α−1}(1−θ)^{β−1}=Beta(θ∣α+N_1​,β+N_0​)\]</span></li>
<li><strong>MAP 估计</strong>：即让后验最大 <span class="math display">\[\hat{\theta}_{\text{MAP}} = \frac{\alpha + N_1 -
1}{\alpha + N_1 - 1 + \beta + N_0 - 1}\]</span>
用均匀分布先验则和MLE得到的结果一致。</li>
<li><strong>Posterior
Mean</strong>：很多时候会使用后验的均值而非峰值作为参数，: <span class="math display">\[\hat{\theta} = \int \theta \cdot p(\theta|D)
d\theta\]</span></li>
<li><strong>Posterior
Variance</strong>：表达估计的不确定性，对于抛硬币可得到标准差 <span class="math display">\[\sigma = \sqrt{V[\theta|D]} \approx
\sqrt{\frac{\hat{\theta}(1 - \hat{\theta})}{N}}\]</span>
所以随着样本量<span class="math inline">\(N\)</span>的增大，不确定性以<span class="math inline">\(\frac{1}{\sqrt{N}}\)</span>​
的速度下降。不确定性（方差）在<span class="math inline">\(\hat{\theta} =
0.5\)</span>时达到最大，在<span class="math inline">\(\hatθ\)</span>接近0或1时达到最小。这表明，确定一个硬币偏向比确定它是公平的要容易得多。</li>
<li><strong>Credible Intervals</strong>：置信区间，后验分布的
<strong>100(1 - α)%</strong> 置信区间定义 <span class="math display">\[C_\alpha(D) = (l, u) : P(l \leq \theta \leq u|D)
= 1 - \alpha\]</span></li>
<li><strong>Posterior Predictive
Distribution</strong>：假设我们希望预测未来的观测值，贝叶斯最优方法是通过边缘化未知参数来计算后验预测分布：
<span class="math display">\[p(y|D) = \int p(y|\theta) p(\theta|D)
d\theta\]</span>
有时计算该积分可能会很困难，这时可以使用点估计方法，选择一个参数估计值<span class="math inline">\(\hat{\theta} = \delta(D)\)</span>，例如 MLE 或
MAP，从而近似为： <span class="math display">\[p(y|D) \approx
p(y|\hat{\theta})\]</span></li>
<li><strong>Marginal
Likelihood</strong>：对于优化没有影响，主要在于对模型的选择上： <span class="math display">\[p(D|M) = \int p(\theta|M) p(D|\theta, M) d\theta
\quad\]</span></li>
</ul>
<p>还提到了一个定理<strong>de Finetti's
theorem（德·芬尼蒂定理）：</strong>如果数据是可交换的，那么必然存在一个隐藏的随机变量<span class="math inline">\(\theta\)</span>，数据在给定<span class="math inline">\(\theta\)</span>的条件下是独立同分布的。这个定理为贝叶斯方法提供了理论基础。</p>
<p><img src="/images/Fundamentals-Statistics/2.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="二频率学派统计">二、频率学派统计</h2>
<p>与贝叶斯统计不同，频率学派不将参数当作随机变量，而是依赖采样分布来表示不确定性。它通过反复采样来评估数据中的随机性和不确定性，而不是使用先验分布和后验分布。核心思想是重复实验的假设：通过观察如果在不同的数据集上重复实验，估计的量（例如参数）会如何变化，这种变化构成了不确定性的依据。</p>
<p>这个也很熟悉了，简单来说就是频率学派认为参数是一个值，通过不断地实验就能去估计这个值。虽然这个有一定的缺点，但其一些准则在实践中也是被广泛使用的。</p>
<p><img src="/images/Fundamentals-Statistics/3.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="sampling-distributions">1. Sampling distributions</h3>
<p>采样分布是对某个估计器（如最大似然估计，MLE）的结果变化进行的描述。</p>
<p>举例来说，假设从一个真实模型<span class="math inline">\(p(x|\theta^*)\)</span>中采样多个数据集<span class="math inline">\(D^{(s)}\)</span>，然后对每个数据集应用估计器来得到参数估计<span class="math inline">\(\hat{\theta}(D^{(s)})\)</span>。通过让数据集的数量<span class="math inline">\(S\)</span>趋向无穷，我们可以得到估计器的采样分布。这个分布反映了在不同的样本下，参数估计的变化情况。</p>
<p><img src="/images/Fundamentals-Statistics/4.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="bootstrap-自助法">2. Bootstrap 自助法</h3>
<p>当估计器比较复杂或者样本量较小的时候，可以使用<strong>Bootstrap</strong>方法来近似采样分布。自助法的核心是通过从原始数据集中随机采样生成多个伪数据集，然后计算每个伪数据集的参数估计，最终得到估计值的经验分布。主要有两种方法：</p>
<ul>
<li><strong>参数自助法</strong>假设我们知道参数<span class="math inline">\(\theta^*\)</span>，我们可以生成伪数据集并计算估计值。但现实是<span class="math inline">\(\theta^*\)</span>是未知的，所以我们使用从数据中估计出的参数<span class="math inline">\(\hat{\theta}\)</span>，这就称为“参数自助法”。</li>
<li>另一种是<strong>非参数自助法</strong>，它不依赖于特定的生成模型，而是直接从原始数据集中进行<strong>有放回</strong>的采样，这样每个新生成的数据集与原始数据集有相同的大小，但通常会有重复数据点。</li>
</ul>
<p><img src="/images/Fundamentals-Statistics/5.png" srcset="/img/loading.gif" lazyload><br>
<img src="/images/Fundamentals-Statistics/6.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="渐近正态性asymptotic-normality">3. 渐近正态性（Asymptotic
Normality）</h3>
<p>当样本量足够大时，最大似然估计（MLE）的采样分布会趋向于正态分布。这称为MLE的<strong>渐近正态性</strong>。在数学上，它表述为：</p>
<p><span class="math display">\[\sqrt{N}(\hat{\theta} - \theta^*)
\rightarrow N(0, F(\theta^*)^{-1})\]</span></p>
<p>Fisher 信息矩阵其中<span class="math inline">\(F(\theta^*)\)</span>是费舍尔信息矩阵。</p>
<p>费舍尔信息矩阵衡量的是似然函数在真参数处的曲率，表明数据中包含的“信息量”。渐近正态性意味着，当样本量<span class="math inline">\(N\)</span>趋于无穷时，估计值的分布会收敛于一个以真参数<span class="math inline">\(\theta^*\)</span>为中心的高斯分布。</p>
<p><img src="/images/Fundamentals-Statistics/7.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="fisher-信息矩阵">4. Fisher 信息矩阵</h3>
<p><strong>Fisher 信息矩阵（Fisher Information Matrix,
FIM）</strong>与对数似然函数的曲率密切相关。这一矩阵在频率学派统计中有重要作用，主要用于刻画最大似然估计（MLE）的采样分布。此外，Fisher
信息矩阵在贝叶斯统计中也有应用，例如推导 Jeffreys
的无信息先验，以及在优化问题中作为自然梯度下降的一部分。</p>
<p>定义如下：</p>
<p><strong>score function</strong> ： <span class="math display">\[s(\theta)\equiv\nabla_\theta\log
p(x|\theta)\]</span></p>
<p><strong>Fisher 信息矩阵</strong> ： <span class="math display">\[F(\theta) \equiv \mathbb{E}_{x \sim p(x|\theta)}
\left[ \nabla_\theta \log p(x|\theta) \nabla_\theta \log p(x|\theta)^T
\right]\]</span></p>
<p>其第<span class="math inline">\(i,j\)</span>项为：</p>
<p><span class="math display">\[F_{ij} = \mathbb{E}_{x \sim \theta}
\left[ \frac{\partial}{\partial \theta_i} \log p(x|\theta)
\frac{\partial}{\partial \theta_j} \log p(x|\theta) \right]\]</span></p>
<p><img src="/images/Fundamentals-Statistics/8.png" srcset="/img/loading.gif" lazyload><br>
可以看到Fisher 信息矩阵与负对数似然函数（NLL, Negative Log
Likelihood）有关系：</p>
<p><span class="math display">\[\text{NLL}(\theta) = - \log
p(D|\theta)\]</span></p>
<p>我们有如下定理：</p>
<p><strong>定理 4.1</strong> 如果<span class="math inline">\(\log
p(x|\theta)\)</span>是二阶可微的，并且在某些正则条件下，Fisher
信息矩阵等于 NLL 的期望 Hessian 矩阵：</p>
<p><span class="math display">\[
F(\theta)_{ij} = \mathbb{E}_{x \sim \theta} \left[
\frac{\partial}{\partial \theta_i} \log p(x|\theta)
\frac{\partial}{\partial \theta_j} \log p(x|\theta) \right] = -
\mathbb{E}_{x \sim \theta} \left[ \frac{\partial^2}{\partial \theta_i
\partial \theta_j} \log p(x|\theta) \right]
\]</span></p>
<p><img src="/images/Fundamentals-Statistics/9.png" srcset="/img/loading.gif" lazyload><br>
然后书上给了一些常见分布的例子</p>
<ul>
<li>二项分布的 FIM：<span class="math inline">\(F(\theta) =
\mathbb{E}_{x \sim \theta}[-s&#39;(\theta|x)] = \frac{n}{\theta(1 -
\theta)}\)</span></li>
<li>单变量高斯分布的 FIM：<span class="math inline">\(F(\theta) =
\begin{pmatrix} \frac{1}{v} &amp; 0 \\ 0 &amp; \frac{1}{2v^2}
\end{pmatrix}\)</span></li>
<li>逻辑回归的 FIM：<span class="math inline">\(F(w) =
\mathbb{E}_{p(y|X,w,\lambda)}[\nabla^2 L(w)] = X^T \Lambda X + \lambda
I\)</span>，其中<span class="math inline">\(\Lambda_{nn} = \sigma(w^T
x_n)(1 - \sigma(w^T x_n))\)</span>。</li>
<li>指数族分布的 FIM：<span class="math inline">\(F_\eta =
\text{Cov}[T(x)]\)</span></li>
</ul>
<h3 id="频率学派的counterintuitive-properties">5.
频率学派的Counterintuitive properties</h3>
<p>首先是<strong>频率主义的置信区间，</strong>它基于抽样分布来估计参数的不确定性。其定义是，如果重复抽取样本并计算每个样本的置信区间，那么有
95% 的区间会包含真实参数。但对于一个具体的样本，<strong>无法说参数有 95%
的概率落在置信区间内</strong>。换句话说，频率主义认为参数是固定的，数据是随机的，所以无法给出“参数在此区间的概率”。</p>
<p>贝叶斯方法则把数据固定，参数看作是随机的。因此，贝叶斯的可信区间给出了参数在某区间内的概率。这是人们在直觉上通常更关心的问题：已知数据后，参数落在某个范围的概率。</p>
<p>文中提到一个具体的例子：</p>
<p><img src="/images/Fundamentals-Statistics/10.png" srcset="/img/loading.gif" lazyload><br>
然后就是<strong>p 值的误导性，</strong>p
值是指在原假设（H0）成立时，观察到某个统计量或更极端结果的概率。频率主义的假设检验通过计算
p 值来决定是否拒绝原假设，通常认为 p
值很小就意味着原假设不太可能成立。</p>
<p>问题是p
值经常被错误地解释为“原假设为真的概率”，但实际上它只是给出了在原假设成立的情况下，观察到数据的概率。它并没有告诉我们在看到数据后原假设是否成立，或者备择假设（H1）是否更有可能成立。</p>
<p>文中用了一个类比来说明 p
值的误导性。假设“如果一个人是美国人，他大概率不是国会议员”，我们观测到某人是国会议员，但这并不能推导出“这个人很可能不是美国人”。</p>
<p>这是一个典型的错误推理，类似于依赖 p
值来判断假设的真实性。相反，贝叶斯方法会使用贝叶斯定理结合数据推导出假设的后验概率，更符合人们的直觉。</p>
<p><img src="/images/Fundamentals-Statistics/11.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="三共轭先验">三、共轭先验</h2>
<p>如果先验分布<span class="math inline">\(p(\theta)\)</span>属于某个参数化家族<span class="math inline">\(F\)</span>，并且后验分布<span class="math inline">\(p(\theta|D)\)</span>也在该家族中，则称<span class="math inline">\(p(\theta)\)</span>为<span class="math inline">\(p(D|\theta)\)</span>的共轭先验。这意味着贝叶斯更新后，分布保持在同一个家族中，便于计算。</p>
<p>书中详细介绍了常见分布的共轭先验，这里仅做总结：</p>
<h3 id="二项分布">3.1 二项分布</h3>
<ul>
<li><strong>共轭先验：</strong>贝塔分布 <span class="math display">\[p(\theta) = \text{Beta}(\theta | \alpha,
\beta)\]</span></li>
<li><strong>更新公式：</strong>在观察到<span class="math inline">\(k\)</span>次成功和<span class="math inline">\(n\)</span>次试验后，后验分布为： <span class="math display">\[p(\theta | k, n) = \text{Beta}(\theta | \alpha +
k, \beta + n - k)\]</span></li>
</ul>
<h3 id="多类分布">3.2. 多类分布</h3>
<ul>
<li><p><strong>共轭先验：</strong>狄利克雷分布： <span class="math display">\[p(\boldsymbol{\theta}) =
\text{Dirichlet}(\boldsymbol{\theta} |
\boldsymbol{\alpha})\]</span></p></li>
<li><p><strong>更新公式：</strong>如果观察到类别<span class="math inline">\(i\)</span>的次数为<span class="math inline">\(n_i\)</span>​，则后验分布为 <span class="math display">\[p(\boldsymbol{\theta} | \mathbf{n}) =
\text{Dirichlet}(\boldsymbol{\theta} | \boldsymbol{\alpha} +
\mathbf{n})\]</span></p>
<p>其中<span class="math inline">\(\mathbf{n} = (n_1, n_2, \ldots,
n_k)\)</span>表示每个类别的观察次数。</p></li>
</ul>
<h3 id="单变量高斯模型univariate-gaussian-model">3.3
单变量高斯模型（Univariate Gaussian Model）</h3>
<p><strong>3.3.1 给定<span class="math inline">\(\sigma^2\)</span>的后验</strong></p>
<ul>
<li><strong>共轭先验：</strong>另一个高斯分布： <span class="math display">\[N(\mu|m_0, \tau_0^2)\]</span></li>
<li><strong>更新公式：</strong>后验分布也为高斯分布，参数为： <span class="math display">\[
\begin{align}
\hat{\tau}^2 &amp;= \frac{1}{\frac{1}{\sigma^2} + \frac{N}{\tau_0^2}} \\
\hat{m} &amp;= \hat{\tau}^2 \left( \frac{m_0}{\tau_0^2} +
\frac{N\bar{y}}{\sigma^2} \right)
\end{align}
\]</span> 这里，<span class="math inline">\(\bar{y}\)</span>是样本均值。</li>
</ul>
<p><strong>3.3.2 给定</strong><span class="math inline">\(μ\)</span><strong>的后验</strong></p>
<ul>
<li><strong>共轭先验</strong>为逆伽马分布：<br>
<span class="math display">\[\text{IG}(\sigma^2|\alpha_0, \beta_0)
\propto (\sigma^2)^{-\alpha_0 - 1}
\exp\left(-\frac{\beta_0}{\sigma^2}\right)\]</span></li>
<li><strong>更新公式：</strong>后验分布也是逆伽马分布，参数为：<br>
<span class="math display">\[
\begin{align}
&amp;\hat{\alpha} = \alpha_0 + \frac{N}{2} \\
&amp;\hat{\beta} = \beta_0 + \frac{1}{2}\sum_{n=1}^{N}(y_n - \mu)^2
\end{align}
\]</span></li>
</ul>
<p><strong>3.3.3 对于均值</strong><span class="math inline">\(\mu\)</span><strong>和方差</strong><span class="math inline">\(\sigma^2\)</span><strong>的推断</strong></p>
<ul>
<li><p><strong>共轭先验：</strong>正态-逆伽马分布（NIG）： <span class="math display">\[NIG(\mu, \sigma^2 | m, \kappa, a, b) \equiv N(\mu
| m, \frac{\sigma^2}{\kappa}) IG(\sigma^2 | a, b)\]</span>
一般就用正态-逆卡方分布（NIX）： <span class="math display">\[NI\chi^2(\mu, \sigma^2 | m, \kappa, \nu, \tau^2)
\equiv N(\mu | m, \frac{\sigma^2}{\kappa}) \chi^{-2}(\sigma^2 | \nu,
\tau^2)\]</span></p></li>
<li><p><strong>更新公式：</strong>后验参数更新： <span class="math display">\[
\begin{align}
&amp;\hat{m} = \frac{\kappa m + N\hat{x}}{\hat{\kappa}} \\
&amp;\hat{\kappa} = \kappa + N \\
&amp;\hat{\nu} = \nu + N \\  
&amp;\hat{\nu} \hat{\tau}^2 = \nu \tau^2 + \sum_{n=1}^{N}(y_n -
\bar{y})^2 + \frac{N\kappa}{\kappa + N}(m - \bar{y})^2
\end{align}
\]</span></p></li>
<li><p><strong>方差的后验边际分布：</strong><br>
<span class="math display">\[p(\sigma^2 | D) = \chi^{-2}(\sigma^2 |
\hat{\nu}, \hat{\tau}^2)\]</span></p></li>
<li><p><strong>均值的后验边际分布：</strong><br>
<span class="math display">\[p(\mu | D) = T(\mu | \hat{m},
\frac{\hat{\tau}^2}{\hat{\kappa}}, \hat{\nu})\]</span></p></li>
</ul>
<h3 id="对于多变量高斯模型">3.4. 对于多变量高斯模型</h3>
<p><strong>3.4.1 在给定 Σ 的情况下推断 µ</strong></p>
<ul>
<li><strong>共轭先验：</strong>高斯分布: <span class="math display">\[p(\mu) = N(\mu | m, V)\]</span></li>
<li><strong>后验分布：</strong> : <span class="math display">\[p(\mu |
D, \Sigma) = N(\mu | \hat{m}, \hat{V})\]</span> 更新公式： <span class="math display">\[
\begin{align}
&amp;\hat{V}^{-1} = V^{-1} + N\Sigma^{-1} \\
&amp;\hat{m} = \hat{V}(\Sigma^{-1}(Ny) + V^{-1}m)
\end{align}
\]</span></li>
</ul>
<p><strong>3.4.2 在给定 µ 的情况下推断 Σ</strong></p>
<ul>
<li><strong>共轭先验：</strong>逆Wishart分布: <span class="math display">\[p(\Sigma) = IW(\Sigma | \Psi^{-1},
\nu)\]</span></li>
<li><strong>后验分布：</strong> : <span class="math display">\[p(\Sigma
| D, \mu) \propto IW(\Sigma | \hat{\Psi}, \hat{\nu})\]</span>
更新公式：<br>
<span class="math display">\[
\begin{align}
&amp;\hat{\nu} = \nu + N\\
&amp;\hat{\Psi} = \Psi + S_\mu
\end{align}
\]</span></li>
</ul>
<p><strong>3.4.3 同时推断 Σ 和 µ</strong></p>
<ul>
<li><strong>共轭先验：</strong> <span class="math display">\[p(\mu,
\Sigma) = N(\mu | m, V) IW(\Sigma | \Psi^{-1}, \nu)\]</span></li>
<li><strong>后验分布：</strong> <span class="math display">\[p(\mu,
\Sigma | D) \propto |\Sigma|^{-\frac{N + \nu + D + 2}{2}}
\exp\left(-\frac{1}{2} \text{tr}(\Sigma^{-1} M)\right)\]</span>
其中<span class="math inline">\(M\)</span>是更新后的散点矩阵。</li>
</ul>
<h3 id="指数族模型">3.5 指数族模型</h3>
<p>唯一存在共轭先验的分布族是指数族，具体如下：</p>
<ul>
<li><strong>共轭先验：</strong>我们可以将先验分布写成与似然函数相似的形式：
<span class="math display">\[p(\eta|\tilde{\tau}, \tilde{\nu}) =
\frac{1}{Z(\tilde{\tau}, \tilde{\nu})} \exp\left(\tilde{\tau}^T \eta -
\tilde{\nu} A(\eta)\right)\]</span> 其中，<span class="math inline">\(\tilde{\nu}\)</span>是先验的强度，<span class="math inline">\(\frac{\tilde{\tau}}{\tilde{\nu}}\)</span>​
是先验均值，<span class="math inline">\(Z(\tilde{\tau},
\tilde{\nu})\)</span>是归一化因子。</li>
<li><strong>后验分布：</strong> <span class="math display">\[
\begin{align}
p(\eta|D) &amp;= \frac{p(D|\eta) p(\eta)}{p(D)} \\
&amp;= \frac{h(D)}{Z(\tilde{\tau}, \tilde{\nu})p(D)} \exp\left(
(\tilde{\tau} + s(D))^T \eta - (\tilde{\nu} + N) A(\eta)\right) \\
&amp;= \frac{1}{Z(\hat{\tau}, \hat{\nu})} \exp\left(\hat{\tau}^T \eta -
\hat{\nu} A(\eta)\right)
\end{align}
\]</span> 其中： <span class="math display">\[
\begin{align}
&amp;\hat{\tau} = \tilde{\tau} + s(D)\\
&amp;\hat{\nu} = \tilde{\nu} + N \\
&amp;Z(\hat{\tau}, \hat{\nu}) = \frac{Z(\tilde{\tau},
\tilde{\nu})h(D)}{p(D)}
\end{align}
\]</span></li>
</ul>
<p>我们看到，后验分布与先验分布具有相同的形式，只是更新了充分统计量和样本大小。后验均值为先验均值与经验均值（即最大似然估计）之间的组合：</p>
<p><span class="math display">\[\begin{align*} E[\eta|D] &amp;=
\frac{\hat{\tau}}{\hat{\nu}} \\
&amp;= \frac{\tilde{\tau} + s(D)}{\tilde{\nu} + N} \\
&amp;= \frac{\tilde{\nu}}{\tilde{\nu} + N}
\frac{\tilde{\tau}}{\tilde{\nu}} + \frac{N}{\tilde{\nu} + N}
\frac{s(D)}{N}  \\
&amp;= \lambda E[\eta] + (1 - \lambda) \hat{\eta}_{MLE}
\end{align*}\]</span></p>
<p>其中，<span class="math inline">\(\lambda =
\frac{\tilde{\nu}}{\tilde{\nu} + N}\)</span>​。</p>
<ul>
<li><strong>边际似然：</strong> <span class="math display">\[p(D) =
\frac{Z(\hat{\tau}, \hat{\nu}) h(D)}{Z(\tilde{\tau},
\tilde{\nu})}\]</span></li>
<li><strong>后验预测密度：</strong>我们现在推导给定过去数据<span class="math inline">\(D = (x_1, ..., x_N)\)</span>时，未来观测<span class="math inline">\(D&#39; = (x&#39;_1, ...,
x&#39;_{N&#39;})\)</span>的预测密度，如下：</li>
</ul>
<p><span class="math display">\[\begin{align*} p(D&#39;|D) &amp;= \int
p(D&#39;|\eta) p(\eta|D) d\eta \\&amp;=h(D&#39;) \frac{Z(\tilde{\tau} +
s(D) + s(D&#39;), \tilde{\nu} + N + N&#39;)}{Z(\tilde{\tau} + s(D),
\tilde{\nu} + N)} \end{align*}\]</span></p>
<h2 id="四无信息先验">四、无信息先验</h2>
<p>在缺乏领域特定知识时，我们不希望主观定义不合理的先验，于是我们便可以选择无信息先验客观。主要有如下几种：</p>
<h3 id="最大熵先验maximum-entropy-priors">1. 最大熵先验（Maximum entropy
priors）</h3>
<p><strong>最大熵先验</strong>是一种不做过多假设的先验分布，适合在没有充足信息的情况下使用。通过最大化熵来选择先验，这种方法依赖于拉普拉斯提出的“不充分理由原则”，即当我们没有理由偏向某个特定值时，应选择“平坦”的分布。例如，对于伯努利分布的参数
θ（取值范围 [0,1]），最大熵先验是均匀分布。</p>
<p>我们也可以根据已知约束来定义最大熵先验，使其在满足这些约束的同时使得熵最大化。书中举了一个例子：</p>
<p><img src="/images/Fundamentals-Statistics/12.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="杰弗里斯先验jeffreys-priors">2. 杰弗里斯先验（Jeffreys
priors）</h3>
<p><strong>Jeffreys
priors</strong>通过保证对参数化不敏感，即在不同的参数化方式下，后验分布不会改变。杰弗里斯先验的一个关键特性是对参数的变化保持不变，这意味着无论采用何种参数化方式，结果应该是一致的。</p>
<p><img src="/images/Fundamentals-Statistics/13.png" srcset="/img/loading.gif" lazyload><br>
参数<span class="math inline">\(\theta\)</span>的 Jeffreys Prior
为以下形式：</p>
<p><span class="math display">\[p_{J}(\theta) \propto
\sqrt{\mathcal{I}(\theta)}\]</span></p>
<p>其中，<span class="math inline">\(\mathcal{I}(\theta)\)</span>是我们所熟知的Fisher信息量。证明如下：</p>
<p><img src="/images/Fundamentals-Statistics/14.png" srcset="/img/loading.gif" lazyload><br>
例如对于伯努利分布，其杰弗里斯先验是 Beta(1/2, 1/2) 分布。</p>
<h3 id="不变性先验invariant-priors">3. 不变性先验（Invariant
priors）</h3>
<p>不变性先验是指当我们知道某些不变性时，可以将其编码进先验中。例如：</p>
<p><strong>平移不变先验</strong>：对位置参数的推断可以使用平移不变先验，这种先验在任何相同宽度的区间上都分配相同的概率质量。</p>
<p><img src="/images/Fundamentals-Statistics/15.png" srcset="/img/loading.gif" lazyload><br>
<strong>尺度不变先验</strong>：对尺度参数的推断可以使用尺度不变先验，其满足任意比例缩放后保持相同概率质量。</p>
<p><img src="/images/Fundamentals-Statistics/16.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="参考先验reference-priors">4. 参考先验（Reference priors）</h3>
<p>参考先验通过最大化数据集上的后验与先验之间的KL散度来定义。它旨在使先验尽可能远离所有可能的后验分布，从而保持非信息性。参考先验可以看作是对不同数据集的互信息最大化问题。对于一维情况，参考先验等同于Jeffreys
priors，而在高维情况下，计算起来则更复杂。</p>
<h2 id="五层次先验hierarchical-priors">五、层次先验（Hierarchical
priors）</h2>
<p>贝叶斯模型需要为参数<span class="math inline">\(\theta\)</span>指定先验<span class="math inline">\(p(\theta)\)</span>，而先验的参数（超参数<span class="math inline">\(\xi\)</span>）也是未知的。为了处理这种不确定性，我们可以对超参数<span class="math inline">\(\xi\)</span>再定义一个先验，从而构建<strong>层次贝叶斯模型</strong>。这种模型的形式化表达为：<span class="math inline">\(p(\xi, \theta, D) = p(\xi) p(\theta | \xi) p(D |
\theta)\)</span></p>
<p><img src="/images/Fundamentals-Statistics/17.png" srcset="/img/loading.gif" lazyload><br>
这表明数据<span class="math inline">\(D\)</span>通过参数<span class="math inline">\(\theta\)</span>依赖于超参数<span class="math inline">\(\xi\)</span>，从而形成一个层次结构。</p>
<p>在实际问题中，如果我们有多个相关的数据集<span class="math inline">\(D_j\)</span>​，各个数据集有自己的参数<span class="math inline">\(\theta_j\)</span>，那么分别独立估计每个<span class="math inline">\(\theta_j\)</span>​
可能会产生不可靠的结果，特别是当某个数据集较小时。层次模型可以通过共享超参数<span class="math inline">\(\xi\)</span>来借用数据量大的群体的信息，帮助数据量小的群体进行更好的估计.</p>
<h3 id="层次二项模型的例子">1. 层次二项模型的例子</h3>
<p>问题背景：假设我们想估计不同群体中某种疾病的患病率，每个群体的样本量是<span class="math inline">\(N_j\)</span>​，阳性病例数是<span class="math inline">\(y_j\)</span>。我们可以假设<span class="math inline">\(y_j\)</span>服从二项分布<span class="math inline">\(\text{Bin}(N_j, \theta_j)\)</span>，其中<span class="math inline">\(\theta_j\)</span>​ 是该群体的患病率。</p>
<p>如果直接对每个群体单独估计<span class="math inline">\(\theta_j\)</span>，特别是当样本量<span class="math inline">\(N_j\)</span>很小时，可能会导致不可靠的结果。比如，如果<span class="math inline">\(y_j = 0\)</span>，我们可能会估计<span class="math inline">\(\hat{\theta_j} =
0\)</span>，尽管实际的患病率可能更高。</p>
<p>解决方案：为了避免这种问题，可以假设所有的<span class="math inline">\(\theta_j\)</span>不是独立的，而是从一个共同的 Beta
分布中抽取，即<span class="math inline">\(\theta_j \sim \text{Beta}(a,
b)\)</span>。这个假设允许我们通过共享的先验<span class="math inline">\(\xi = (a,
b)\)</span>来提高估计的可靠性。这种模型的联合分布可以写作：<br>
<span class="math display">\[p(D, \theta, \xi) = p(\xi) \prod_{j=1}^{J}
\text{Beta}(\theta_j | \xi) \prod_{j=1}^{J} \text{Bin}(y_j | N_j,
\theta_j)\]</span></p>
<p>后验推断：可以通过<strong>Hamiltonian Monte
Carlo（HMC）</strong>算法来进行后验推断，生成超参数<span class="math inline">\(\xi\)</span>和群体参数<span class="math inline">\(\theta_j\)</span>​
的样本。对于每个群体，后验均值<span class="math inline">\(E[\theta_j |
D]\)</span>会根据数据量的大小进行调整。对于数据较少的群体，估计值会向全体群体的均值（共享信息）靠拢，这种现象被称为<strong>收缩（shrinkage）</strong>。</p>
<p><img src="/images/Fundamentals-Statistics/18.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="层次高斯模型的例子">2. 层次高斯模型的例子</h3>
<p>问题背景：现在考虑实数值数据的情况，假设我们有多个群体的数据，每个群体的数据<span class="math inline">\(y_{ij}\)</span>服从正态分布<span class="math inline">\(N(\theta_j, \sigma^2)\)</span>，其中<span class="math inline">\(\theta_j\)</span>​ 是该群体的均值，<span class="math inline">\(\sigma^2\)</span>是固定的方差。</p>
<p>与二项模型类似，我们可以假设各群体的均值<span class="math inline">\(\theta_j\)</span>​ 来自一个共同的正态分布<span class="math inline">\(\theta_j \sim N(\mu,
\tau^2)\)</span>。这个模型的联合分布为：<br>
<span class="math display">\[p\propto p(\mu)p(\tau^2) \prod_{j=1}^{J}
N(\theta_j | \mu, \tau^2) N(y_j | \theta_j, \sigma_j^2)\]</span></p>
<p>其中<span class="math inline">\(p(\mu)\)</span>和<span class="math inline">\(p(\tau^2)\)</span>是超参数的先验分布，可以假定<span class="math inline">\(\sigma_j^2\)</span>是知道的。</p>
<p><img src="/images/Fundamentals-Statistics/19.png" srcset="/img/loading.gif" lazyload><br>
对于每个群体，后验均值<span class="math inline">\(E[\theta_j |
D]\)</span>会介于单独的最大似然估计值<span class="math inline">\(\hat{\theta_j}\)</span>和全局均值<span class="math inline">\(\mu\)</span>之间。根据公式：<br>
<span class="math display">\[E[\theta_j | D, \mu, \tau^2] = w_j \mu + (1
- w_j) \hat{\theta_j}\]</span></p>
<p>​其中<strong>收缩系数</strong><span class="math inline">\(w_j =
\frac{\sigma_j^2}{\sigma_j^2 +
\tau^2}\)</span>​​。数据量较小或不确定性较高的群体（即<span class="math inline">\(\sigma_j^2\)</span>较大的群体）会有更大的收缩，意味着它们的估计值会更多地依赖于全局均值。</p>
<p>为了解决算法在进行后验推断时的计算效率问题，可以采用<strong>非中心化参数化（non-centered
parameterization）</strong>。这种方法通过重新表达<span class="math inline">\(\theta_j = \mu + \tau \eta_j\)</span>，其中<span class="math inline">\(\eta_j \sim N(0,
1)\)</span>，从而减少参数之间的依赖性，提升推断的计算效率。</p>
<p><img src="/images/Fundamentals-Statistics/20.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="六经验贝叶斯">六、经验贝叶斯</h2>
<p>对于层次贝叶斯模型，在<strong>全贝叶斯推断</strong>中，我们对底层参数和超参数同时进行推断，计算<span class="math inline">\(p(\theta, \xi |
D)\)</span>的联合后验分布。虽然这种方法在统计上是更为严格的，但计算量通常较大。<strong>经验贝叶斯</strong>提供了一种近似方法，首先通过最大化边际似然（如<span class="math inline">\(p(D|\xi)\)</span>）估计超参数<span class="math inline">\(\xi\)</span>，然后在给定这些估计值的条件下推断底层参数的后验分布（如<span class="math inline">\(p(\theta|\hat{\xi},
D)\)</span>）。这种方法通过对超参数做<strong>点估计</strong>，而非推断它们的后验分布，因此简化了计算。</p>
<p>通过边际似然最大化来估计超参数是经验贝叶斯的核心步骤。具体来说，经验贝叶斯在给定数据<span class="math inline">\(D\)</span>的条件下，通过<strong>最大化边际似然</strong><span class="math inline">\(p(D|\xi)\)</span>来找到最优的超参数估计值<span class="math inline">\(\hat{\xi}\)</span>​。这种方法有时也被称为<strong>II类最大似然</strong>，因为它不是直接优化底层参数<span class="math inline">\(\theta\)</span>，而是先优化超参数<span class="math inline">\(\xi\)</span>。</p>
<p><img src="/images/Fundamentals-Statistics/21.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="经验贝叶斯在层次二项模型中的应用">1.
经验贝叶斯在层次二项模型中的应用</h3>
<p>在二项分布的层次模型中，经验贝叶斯的边际似然可以通过积分将底层参数<span class="math inline">\(\theta_j\)</span>​ 消除掉，从而直接用超参数<span class="math inline">\(\xi\)</span>表示边际似然。具体公式是：</p>
<p><span class="math display">\[p(D|\xi) = \prod_j \int
\text{Bin}(y_j|N_j, \theta_j) \text{Beta}(\theta_j | a, b)
d\theta_j\]</span></p>
<p>经验贝叶斯方法通过最大化此边际似然来估计超参数<span class="math inline">\(a\)</span>和<span class="math inline">\(b\)</span>，然后在给定这些估计值后，再计算每个<span class="math inline">\(\theta_j\)</span>的后验分布。</p>
<p><img src="/images/Fundamentals-Statistics/22.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="经验贝叶斯在层次高斯模型中的应用">2.
经验贝叶斯在层次高斯模型中的应用</h3>
<p>在层次高斯模型中，经验贝叶斯可以通过边际化<span class="math inline">\(\theta_j\)</span>​
得到边际似然，并利用最大似然估计超参数<span class="math inline">\(\mu\)</span>和<span class="math inline">\(\tau^2\)</span>。在这个例子中，边际似然公式是：</p>
<p><span class="math display">\[p(D|\mu, \tau^2, \sigma^2) =
\prod_{j=1}^{J} N(y_j | \mu, \tau^2 + \sigma^2)\]</span></p>
<p>然后通过矩匹配方法估计<span class="math inline">\(\tau^2\)</span>和<span class="math inline">\(\mu\)</span>。</p>
<p><img src="/images/Fundamentals-Statistics/23.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="经验贝叶斯在马尔可夫模型中的应用">3.
经验贝叶斯在马尔可夫模型中的应用</h3>
<p>经验贝叶斯还可以用于语言模型中的 <strong>n-gram
平滑问题</strong>。在这个上下文中，经验贝叶斯被用来估计马尔可夫链中状态转移矩阵的先验分布。通过为转移矩阵的每一行设定一个独立的狄利克雷分布作为先验，经验贝叶斯可以通过最大化边际似然来估计先验参数<span class="math inline">\(\alpha\)</span>和<span class="math inline">\(m\)</span>，从而得到自适应的平滑方法。这个方法的优势在于，它能够根据数据自动调整平滑参数<span class="math inline">\(\lambda_j\)</span>，从而提高模型的表现。</p>
<p>在 n-gram
语言模型中，我们希望计算不同词之间的转移概率，比如在二元模型（bigram
model）中，给定词<span class="math inline">\(X_t =
j\)</span>，下一个词<span class="math inline">\(X_{t+1} =
k\)</span>的概率可以由转移矩阵<span class="math inline">\(A_{jk}\)</span>​
来表示。传统的加一平滑方法对每一个可能的词对<span class="math inline">\((j,
k)\)</span>都假设了一个等价的概率，但这种假设往往过于简单。<strong>删除插值（deleted
interpolation）</strong>是一个更复杂的方案，定义了如下的转移矩阵表示：</p>
<p><span class="math display">\[A_{jk} = (1 - \lambda) f_{jk} + \lambda
f_k\]</span></p>
<p>其中，<span class="math inline">\(f_{jk} =
\frac{N_{jk}}{N_j}\)</span>是从词<span class="math inline">\(j\)</span>到词<span class="math inline">\(k\)</span>的 bigram 频率，<span class="math inline">\(f_k = \frac{N_k}{N}\)</span>​​ 是词<span class="math inline">\(k\)</span>的 unigram 频率，而<span class="math inline">\(\lambda\)</span>是一个通过交叉验证选择的平滑参数。</p>
<p><img src="/images/Fundamentals-Statistics/24.png" srcset="/img/loading.gif" lazyload><br>
然而，删除插值方法没有考虑不同的上下文在词的频率中可能有不同的重要性。贝叶斯方法则可以为每个上下文动态地调整平滑参数<span class="math inline">\(\lambda_j\)</span>​。</p>
<p>通过经验贝叶斯的方法对删除插值进行了重新解释。首先，假设转移矩阵的每一行都遵循独立的
Dirichlet 先验分布：</p>
<p><span class="math display">\[A_j \sim Dir(\alpha_0 m_1, \dots,
\alpha_0 m_K) = Dir(\alpha_0 m) = Dir(\alpha)\]</span></p>
<p>其中，<span class="math inline">\(A_j\)</span>是转移矩阵的第<span class="math inline">\(j\)</span>行，<span class="math inline">\(m\)</span>是先验均值向量，满足<span class="math inline">\(\sum_k m_k = 1\)</span>，而<span class="math inline">\(\alpha_0\)</span>​
是先验强度。通过贝叶斯推断，可以得到转移矩阵行<span class="math inline">\(A_j\)</span>的后验分布：</p>
<p><span class="math display">\[A_j \sim Dir(\alpha + N_j)\]</span></p>
<p>其中<span class="math inline">\(N_j = (N_{j1}, \dots,
N_{jK})\)</span>是从状态<span class="math inline">\(j\)</span>转移到其他状态的计数向量。在此基础上，后验预测密度为：</p>
<p><span class="math display">\[p(X_{t+1} = k | X_t = j, D) =
\frac{N_{jk} + \alpha_j m_k}{N_j + \alpha_0}\]</span></p>
<p>这个公式可以改写为删除插值的形式：</p>
<p><span class="math display">\[p(X_{t+1} = k | X_t = j, D) = (1 -
\lambda_j) f_{jk} + \lambda_j m_k\]</span></p>
<p>其中，<span class="math inline">\(\lambda_j = \frac{\alpha_j}{N_j +
\alpha_0}\)</span>是动态调整的平滑参数，表示给定上下文<span class="math inline">\(j\)</span>时，将先验分布与经验数据相结合的权重。</p>
<p><img src="/images/Fundamentals-Statistics/25.png" srcset="/img/loading.gif" lazyload><br>
EB 方法的核心思想是通过数据来估计 <strong>Dirichlet
分布</strong>的超参数<span class="math inline">\(\alpha\)</span>和<span class="math inline">\(m\)</span>。在这个问题中，有一种近似方法来估计先验均值<span class="math inline">\(m\)</span>：</p>
<p><span class="math display">\[m_k \propto |\{ j : N_{jk} &gt; 0
\}|\]</span></p>
<p>这个估计意味着某个词<span class="math inline">\(k\)</span>的先验概率与该词出现在多少种不同的上下文中有关，而不是它的具体出现次数。这种估计方法可以解决某些平滑方法中的不足。举个例子，如果在一个数据集中“you
see”频繁出现，那么虽然 “you” 和 “see” 的 unigram
频率相同，但是它们在新上下文中出现的概率不应该相等。贝叶斯模型通过先验分布的参数<span class="math inline">\(m_k\)</span>​ 可以自适应地处理这种情况。</p>
<p><img src="/images/Fundamentals-Statistics/26.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="七模型选择">七、模型选择</h2>
<p>在统计建模中，选择合适的模型至关重要。所有模型都存在一定的误差，然而某些模型能更好地适应数据，提供有用的预测。选择模型时，我们需要考虑模型的假设和拟合能力，确保它能在现实世界中应用。</p>
<h3 id="贝叶斯模型选择">1. 贝叶斯模型选择</h3>
<p>贝叶斯模型选择的一个自然的想法是利用后验概率来确定最有可能生成数据的模型。公式如下：</p>
<p><span class="math display">\[\hat{m} = \arg\max_{m \in M}
p(m|D)\]</span></p>
<p>这里的<span class="math inline">\(p(m|D)\)</span>表示给定数据<span class="math inline">\(D\)</span>下模型<span class="math inline">\(m\)</span>的后验概率。根据贝叶斯定理，我们可以表示为：</p>
<p><span class="math display">\[p(m|D) = \frac{p(D|m)p(m)}{\sum_{m \in
M} p(D|m)p(m)}\]</span></p>
<p>如果模型的先验是均匀的，即<span class="math inline">\(p(m) =
\frac{1}{|M|}\)</span>，那么最大后验模型是：</p>
<p><span class="math display">\[\hat{m} = \arg\max_{m \in M}
p(D|m)\]</span></p>
<p><img src="/images/Fundamentals-Statistics/27.png" srcset="/img/loading.gif" lazyload><br>
但是不同的模型设计是有好坏之分的。</p>
<p><strong>示例：硬币是否公平？</strong></p>
<p>假设我们想知道某个硬币是否是公平的。我们可以设定两个模型：</p>
<p><span class="math inline">\(M_0\)</span>：假设硬币是公平的，即<span class="math inline">\(\theta = 0.5\)</span>。</p>
<p><span class="math inline">\(M_1\)</span>​：假设硬币是偏向的，即<span class="math inline">\(\theta\)</span>可以是任意值。</p>
<p>通过比较这两个模型的边际似然，我们可以决定哪个模型更有可能解释观察到的数据。例如：</p>
<p>在公平硬币模型下，观察到<span class="math inline">\(N\)</span>次投掷的边际似然是：</p>
<p><span class="math display">\[p(D|M_0) = \left( \frac{1}{2}
\right)^N\]</span></p>
<p>在偏向硬币模型下，边际似然更复杂，需要计算贝塔分布的积分：</p>
<p><span class="math display">\[p(D|M_1) = \int
p(D|\theta)p(\theta|M_1)d\theta\]</span></p>
<p>如果观察到的正面次数较多，模型<span class="math inline">\(M_1\)</span>​ 的可能性会更高。可能说明<span class="math inline">\(M_0\)</span>的先验并不好。</p>
<h3 id="贝叶斯模型平均">2. 贝叶斯模型平均</h3>
<p>如果我们的目标是进行准确的预测，综合所有模型的预测结果通常比只依赖单一模型更好。贝叶斯模型平均可以表示为：</p>
<p><span class="math display">\[p(y|D) = \sum_{m \in M}
p(y|m)p(m|D)\]</span> 这里<span class="math inline">\(p(y|m)\)</span>是模型<span class="math inline">\(m\)</span>对新数据<span class="math inline">\(y\)</span>的预测。通过对所有模型的预测进行加权平均，我们可以得到更稳健的结果。</p>
<p>与机器学习中的集成技术类似，我们取预测器的加权组合。然而，集成的权重不必总和为
1，尤其是在贝叶斯模型平均中，如果有一个最佳模型<span class="math inline">\(m^*\)</span>，在大样本极限下，<span class="math inline">\(p(m|D)\)</span>将成为一个在<span class="math inline">\(m^*\)</span>上的退化分布，其他模型将被忽略。</p>
<p><img src="/images/Fundamentals-Statistics/28.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="边际似然估计">3 边际似然估计</h3>
<p>为了进行贝叶斯模型选择，我们需要计算在给定先验的条件下的边际似然：</p>
<p><span class="math display">\[p(D|m) = \int p(D|\theta,
m)p(\theta|m)d\theta\]</span></p>
<p>这里的积分通常难以直接计算。对于共轭先验模型，边际似然可以解析计算，这类模型因其先验与后验分布形式一致，使得边际似然的计算变得简单明了。</p>
<p>但对于其它的，我们可以使用变分推断或蒙特卡洛方法来估计。文中还给了一个<strong>Harmonic
mean estimator</strong>的方法：</p>
<p><img src="/images/Fundamentals-Statistics/29.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="交叉验证与边际似然的联系">4. 交叉验证与边际似然的联系</h3>
<p><strong>交叉验证</strong>是一种评估模型预测能力的常用方法。它通过将数据划分为训练集和验证集来评估模型的表现。<strong>留一交叉验证（LOO-CV）</strong>是一种特殊的情况，其中每次留出一个样本进行测试，其他样本用于训练。</p>
<p>交叉验证的结果可以用来估计模型的泛化能力。</p>
<p><img src="/images/Fundamentals-Statistics/30.png" srcset="/img/loading.gif" lazyload><br>
它与<strong>对数边际似然（LML）</strong>有很一定的关系：</p>
<p><img src="/images/Fundamentals-Statistics/31.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="条件边际似然">5. 条件边际似然</h3>
<p>边际似然用于回答“从先验生成训练数据的可能性有多大？”。它适用于在不同的固定先验之间进行假设检验，但很多时候我们更关心的是“后验能够生成数据分布中的新样本的概率是多少？”，这与模型的泛化性能相关联。</p>
<p>研究表明，边际似然有时可能与模型的泛化性能负相关。这是因为边际似然可能会出现先验较差但模型快速适应数据的情况</p>
<p>为了解决这个问题，研究者们提出了<strong>条件对数边际似然（CLML）</strong>，公式为：</p>
<p><span class="math display">\[CLML(m) = \sum_{n=K}^{N} \log
p(D_n|D_{1:n-1}, m)\]</span></p>
<p>其中，<span class="math inline">\(K \in \{1, \dots,
N\}\)</span>是算法的一个参数。CLML通过给定前<span class="math inline">\(K\)</span>个数据点的后验分布来评估后续<span class="math inline">\(N -
K\)</span>个数据点的边际似然。这种方法减少了数据点顺序对结果的依赖。特别地，当<span class="math inline">\(K = N -
1\)</span>并对所有数据顺序进行平均时，这种方法相当于<strong>留一法（LOO）估计</strong>。</p>
<p><img src="/images/Fundamentals-Statistics/32.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="贝叶斯留一法估计">6. 贝叶斯留一法估计</h3>
<p>对于监督模型来说，一个我们关注的点是<strong>ELPD（expected
log-pointwise predictive density），</strong>ELPD
是对未来数据的预测性能进行估计的度量：</p>
<p><span class="math display">\[ELPD(m) = \mathbb{E}_{(x^*, y^*)} \left[
\log p(y^*|x^*, D, m) \right]\]</span></p>
<p>由于未来数据未知，因此可以使用 LOO
近似，即将部分数据点从数据集中移除并计算其预测分布：</p>
<p><span class="math display">\[ELPD_{\text{LOO}}(m) = \sum_{n=1}^N \log
p(y_n|x_n, D_{-n}, m)\]</span></p>
<p><img src="/images/Fundamentals-Statistics/33.png" srcset="/img/loading.gif" lazyload></p>
<p>直接计算<span class="math inline">\(ELPD_{\text{LOO}}\)</span>​
需要计算<span class="math inline">\(N\)</span>次不同的后验分布，这比较慢。可以通过只计算一次后验分布<span class="math inline">\(p(\theta|D,
m)\)</span>，然后使用<strong>重要性采样</strong>近似 LOO 积分。</p>
<p>重要性采样的核心思想是定义目标分布<span class="math inline">\(f(\theta) = p(\theta|D_{-n},
m)\)</span>，并使用已知的提议分布<span class="math inline">\(g(\theta) =
p(\theta|D, m)\)</span>，计算重要性权重：</p>
<p><span class="math display">\[w_{s,-n} =
\frac{f(\theta_s)}{g(\theta_s)} \propto
\frac{1}{p(D_n|\theta_s)}\]</span></p>
<p>将这些权重进行归一化后，可以用来近似 <strong>LOO 估计</strong>：</p>
<p><span class="math display">\[ELPD_{\text{IS-LOO}}(m) = \sum_{n=1}^N
\log \sum_{s=1}^S \hat{w}_{s,-n} p(y_n|x_n, \theta_s, m)\]</span></p>
<p><img src="/images/Fundamentals-Statistics/34.png" srcset="/img/loading.gif" lazyload></p>
<p>重要性采样的一个问题是，权重的方差可能非常大，导致一些权重值过大。为了解决这个问题，可以对每个样本的权重拟合一个
<strong>Pareto 分布</strong>，从而对权重进行平滑。这样可以减少异常值对
LOO 估计的影响。</p>
<p><img src="/images/Fundamentals-Statistics/35.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="八模型检测与假设检验">八、模型检测与假设检验</h2>
<h3 id="后验预测检查posterior-predictive-checks">1.
后验预测检查（Posterior Predictive Checks）：</h3>
<p>通过已知数据和模型生成未来的<strong>合成数据</strong>，以评估真实数据与模型生成的数据是否相似。如果模型生成的数据与真实数据差异很大，说明模型无法捕捉数据中的某些特征，模型可能不适合。</p>
<p><img src="/images/Fundamentals-Statistics/36.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="贝叶斯p值">2.贝叶斯p值</h3>
<p>通过计算<strong>贝叶斯p值</strong>来量化模型的合理性。如果观测到的测试统计量位于预测分布的极端部分（即p值接近0或1），说明模型无法合理解释数据</p>
<p><img src="/images/Fundamentals-Statistics/37.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="假设检验">3. 假设检验</h3>
<p>与频率学派的假设检验相对，贝叶斯方法提供了假设检验的两种替代方案：</p>
<ul>
<li><strong>使用贝叶斯因子进行模型比较</strong>：贝叶斯假设检验不再将检验统计量与临界值进行比较，而是评估在两种模型下数据的边际似然比——零假设<span class="math inline">\(M_0\)</span>​ 和替代假设<span class="math inline">\(M_1\)</span>​。这个比值称为贝叶斯因子，表示为：<br>
<span class="math inline">\(B_{1,0} = \frac{p(D | M_1)}{p(D |
M_0)}\)</span>如果<span class="math inline">\(B_{1,0} &gt;
1\)</span>，我们倾向于支持<span class="math inline">\(M_1\)</span>，否则我们更倾向于支持<span class="math inline">\(M_0\)</span>​。</li>
</ul>
<p><img src="/images/Fundamentals-Statistics/38.png" srcset="/img/loading.gif" lazyload><br>
*
<strong>基于参数估计</strong>：即估计在假设条件下附近的概率。例如，要测试硬币是否公平，我们计算正面概率<span class="math inline">\(\theta\)</span>的后验分布，并检查接近 0.5
的区域中有多少概率质量。</p>
<p><img src="/images/Fundamentals-Statistics/39.png" srcset="/img/loading.gif" lazyload></p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/Probabilistic-Machine-Learning/" class="category-chain-item">Probabilistic Machine Learning</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="print-no-link">#机器学习</a>
      
        <a href="/tags/%E6%95%B0%E5%AD%A6/" class="print-no-link">#数学</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>[Probabilistic Machine Learning]: Fundamentals-Statistics</div>
      <div>https://jia040223.github.io/2024/10/14/Fundamentals-Statistics/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Serendipity</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2024年10月14日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2024/10/20/Fundamentals-Graphical%20models/" title="[Probabilistic Machine Learning]: Fundamentals-Graphical models">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">[Probabilistic Machine Learning]: Fundamentals-Graphical models</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/10/09/%E9%9D%92%E7%94%98%E5%A4%A7%E7%8E%AF%E7%BA%BF/" title="[旅游日志] :青甘大环线">
                        <span class="hidden-mobile">[旅游日志] :青甘大环线</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"Ug8725bpf4JJJkltPotjuquU-MdYXbMMI","appKey":"Po3fbdR9RiF08kxafXGlNgd5","path":"window.location.pathname","placeholder":"留言仅限讨论，严禁广告等行为","avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"https://ug8725bp.api.lncldglobal.com","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
