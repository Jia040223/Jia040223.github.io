<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Serendipity&#39;s Blog</title>
  
  
  <link href="https://jia040223.github.io/atom.xml" rel="self"/>
  
  <link href="https://jia040223.github.io/"/>
  <updated>2024-09-18T14:26:25.516Z</updated>
  <id>https://jia040223.github.io/</id>
  
  <author>
    <name>Serendipity</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>GANs</title>
    <link href="https://jia040223.github.io/2024/09/18/GANs/"/>
    <id>https://jia040223.github.io/2024/09/18/GANs/</id>
    <published>2024-09-18T14:10:33.000Z</published>
    <updated>2024-09-18T14:26:25.516Z</updated>
    
    <content type="html"><![CDATA[<p>本学习笔记用于记录我学习StanfordCS236课程的学习笔记，分享记录，也便于自己实时查看。</p><h2 id="引入">引入</h2><p>前面我们学习了VAEs和NormalizingFlows，这两种模型都是基于最小化KL散度（对似然进行评估）来进行优化的。我们也可以看到，为了进行生成，我们往往会定义一个潜变量<span class="math inline">\(z\)</span>，所以对似然进行评估并不容易。VAEs是通过优化似然的下限ELBO来绕过这个问题，而NormalizingFlows是通过限制映射的形式来计算似然。</p><p>直接计算似然来进行评估，要么只能计算其下界，要么需要限制映射的形式。那有没有一种方法能够用间接方法代替这种直接比较，使生成分布变得越来越接近真实分布呢？GANs便是基于一种间接的评估方式进行设计的。</p><h2 id="基本思想">基本思想</h2><p>GANs的间接方法采用这两个分布的下游任务形式。然后，生成网络的训练是相对于该任务进行的，使生成分布变得越来越接近真实分布。GANs的下游任务是区分真实样本和生成样本的任务。或者我们可以说是“非区分”任务，因为我们希望区分尽可能失败。</p><p>因此，在 GANs架构中，我们有一个判别器，它接收真实和生成数据样本，并尽可能地对它们进行分类；还有一个生成器，它被训练成尽可能地欺骗判别器。即GANs由2个重要的部分构成：</p><ul><li>生成器(Generator)：通过机器生成数据，目的是“骗过”判别器。</li><li>判别器(Discriminator)：判断数据是真实的还是生成的，目的是找出生成器做的“假数据”。</li></ul><h2 id="训练过程">训练过程</h2><p>我们知道GANs的思想后，便能很直观的想到用分类问题的交叉熵作为判别器的损失函数。同时生成器的目的则是最大化这个交叉熵损失函数（混淆判别器），所以我们的训练目标是：</p><p><span class="math display">\[\mathop{\text{min}}\limits_{G}\mathop{\text{max}}\limits_{D}\ V(G,D) = \mathbb{E}_{x \sim p_{data}(x)} [\log D(x)] + \mathbb{E}_{z\sim p_z(z)} [\log(1 - D(G(z)))]  \]</span></p><p>其中 <span class="math inline">\(G\)</span> 指的是生成器， <span class="math inline">\(D\)</span> 指的是判别器。</p><p>所以我们的训练目标是一个极大极小的优化问题，在实际中，我们只需要从数据集中进行采样，然后用生成器进行采样，然后对上面的目标函数进行近似计算，最后进行梯度上升或者梯度下降即可</p><p><img src="/images/GANs/1.png"><br>## 与散度的关系<br>那么为什么这样的设计能够间接地去让生成器生成的样本与真实样本的分布相同呢？</p><p>其实本质上，GANs通过引入判别器来间接地计算了 <span class="math inline">\(\frac{P_\theta(x)}{P_{data}(x)}\)</span>，可以证明，对于一个生成器下的最佳判别器对给定 <span class="math inline">\(x\)</span> 的判定为真实样本的概率是<span class="math inline">\(\frac{P_{data}(x)}{P_{data}(x) +P_\theta(x)}\)</span>， 证明如下：</p><p><em>*Proof*</em>: 二分类交叉熵损失函数为：</p><p><span class="math display">\[\begin{align} \mathrm{BCE}(\mathcalP_1,\mathcal P_2)&amp;=-\mathbb E_{x\sim \mathcal P_1}[\logD(x)]-\mathbb E_{x\sim \mathcal P_2}[\log(1-D(x))]\\ &amp;=-\int \logD(x)\cdot p_1(x)\mathrm d x-\int\log(1-D(x))\cdot p_2(x)\mathrm d x\\&amp;=-\int \left[\log D(x)\cdot p_1(x)+\log(1-D(x))\cdotp_2(x)\right]\mathrm d x\\ \end{align} \\\]</span></p><p>易知 <span class="math inline">\(y=a\log x+b\log(1-x)\)</span> 在<span class="math inline">\(x=\frac{a}{a+b}\)</span>处取到唯一极大值（其中 <span class="math inline">\(0\leqa,b\leq1\)</span> ），所以欲使上式最小，只需：</p><p><span class="math inline">\(\forallx,\,D(x)=\frac{p_1(x)}{p_1(x)+p_2(x)} \\\)</span> 这样就证明完成了。</p><p>那么，再看我们的训练目标：</p><p><span class="math display">\[\min_G\max_D  V(G, D) \\  \begin{align}V(G, D)&amp;=\mathbb E_{x\sim\mathcal P_{data}}[\log D(x)]+\mathbbE_{z\sim \mathcal P_z}[\log(1-D(G(z)))]\\ &amp;=\mathbb E_{x\sim\mathcalP_{data}}[\log D(x)]+\mathbb E_{x\sim \mathcal P_{\theta}}[\log(1-D(x))]\end{align} \\\]</span></p><p>而最优判别器为：</p><p><span class="math inline">\(D^\ast(x)=\frac{p_{data}(x)}{p_{data}(x)+p_\theta(x)}\\\)</span><br>将最优判别器代入 <span class="math inline">\(G\)</span> 的优化目标：</p><p><span class="math display">\[\begin{align} V(G, D^\ast)&amp;=\mathbbE_{x\sim \mathcalP_{data}}\left[\log\frac{p_{data}(x)}{p_{data}(x)+p_\theta(x)}\right]+\mathbbE_{x\sim \mathcalP_\theta}\left[\log\frac{p_\theta(x)}{p_{data}(x)+p_\theta(x)}\right]\\&amp;=2\mathrm {JS}(\mathcal P_{data}\|\mathcal P_\theta)-2\log2\end{align} \\ \]</span></p><p>因此，生成器实际上在最小化 $ P_{data}$ 和 <span class="math inline">\(\mathcal P_\theta\)</span> 的 <span class="math inline">\(\mathrm{JS}\)</span> 散度，从而让生成数据的分布<span class="math inline">\(\mathcal P_\theta\)</span> 接近真实分布<span class="math inline">\(\mathcal P_{data}\)</span> 。</p><p><em><strong>注：</strong> <span class="math inline">\(JS\)</span>散度的定义如下：</em></p><p><em><span class="math display">\[\begin{align} \mathrm{JS}(\mathcalP_1\|\mathcal P_2)&amp;=\frac{1}{2}\left[\mathrm{KL}\left(\mathcalP_1\|\mathcal P_A\right)+\mathrm{KL}\left(\mathcal P_2\|\mathcalP_A\right)\right]\\ &amp;=\log 2+\frac{1}{2}\mathbb E_{x\sim\mathcalP_1}\left[\log\frac{p_1(x)}{p_1(x)+p_2(x)}\right]+\frac{1}{2}\mathbbE_{x\sim\mathcal P_2}\left[\log\frac{p_2(x)}{p_1(x)+p_2(x)}\right]\end{align} \\\]</span> 其相比 <span class="math inline">\(KL\)</span>散度最大的特点便是其是对称的</em>。</p><p><img src="/images/GANs/2.png"><br>可以看出GANs是通过判别器来巧妙地规避了计算似然的问题，但正是因为在实践中我们很难得到真正的最佳判别器，所以实际上我们很多时候只是在优化<span class="math inline">\(JS\)</span>散度的一个下界，笔者绝对这是GANs不得不直面的一个问题。</p><h2 id="fgan">fGAN</h2><h3 id="f-散度f-divergence">F-散度(F-divergence)</h3><p>在概率统计中，f散度是一个函数，这个函数用来衡量两个概率密度<span class="math inline">\(p\)</span>和<span class="math inline">\(q\)</span>的区别，也就是衡量这两个分布多么的相同或者不同。像<span class="math inline">\(KL\)</span> 散度和 <span class="math inline">\(JS\)</span> 散度都是它的一种特例</p><p>f散度定义如下：</p><p><span class="math display">\[{D_f}(\mathcal P_1\|\mathcal P_2)=\int f(\frac{p_2(x)}{p_1(x)})\cdot p_1(x)\mathrm d x=\mathbb E_{x\sim\mathcalP_1}\left[f(\frac{p_2(x)}{p_1(x)})\right] \\\]</span> <span class="math inline">\(f()\)</span> 就是不同的散度函数， <span class="math inline">\(D_f\)</span>就是在f散度函数下，两个分布的差异。规定</p><ul><li><span class="math inline">\(f\)</span>是凸函数(为了用琴生不等式)</li><li>$f ( 1 ) = 0 $ (如果两个分布一样，刚好公式=0)</li></ul><p><img src="/images/GANs/3.png"><br>这两个规定保证了 <span class="math inline">\(D_f\)</span>是非负的，而且当两个分布相同时，其值为0，一些常见散度的 <span class="math inline">\(f\)</span> 定义如下：</p><p><img src="/images/GANs/4.png"></p><h3 id="共轭函数fenchel-conjugate">共轭函数(Fenchel Conjugate)</h3><p>一个函数 <span class="math inline">\(f:\;\mathbb{R}^n\mapsto\mathbb{R}\)</span> 的Frenchel 共轭为：</p><p><span class="math display">\[\begin{align} f^*( t)=\sup_{x}\big(\langle t, x\rangle-f( x)) \end{align}\]</span></p><p>Fenchel 共轭有几何上的解释。当 $ x$ 固定时， <span class="math inline">\(\langle t, x\rangle-f( x)\)</span>是一个仿射函数，因此 Fenchel 共轭就是一组仿射函数的上确界。如果 <span class="math inline">\(f\)</span>可微，那么仿射函数取得上确界的位置正好是 <span class="math inline">\(f\)</span> 的切线，此处有 <span class="math inline">\(\nabla f( x)= t\)</span> 。</p><p>我们拿 $f ( x ) = x l o g x $ 来说，当 <span class="math inline">\(x=10,1, 0.1\)</span>时可以看到相应的函数直线，可以看到最大化y的点连起来是个凸函数，很类似$e^{t-1}$</p><p><img src="/images/GANs/5.png"><br>公式图像：</p><p><img src="/images/GANs/6.png"><br>用数学来推一下：</p><p>将 <span class="math inline">\(f ( x ) = x l o g x\)</span> 代入 $y (t ) = x t − f ( x ) $ ，得 $y ( x ) = x t − x l o g x $ ,对于每个给定的<span class="math inline">\(t\)</span>都可以求出最大值，求导为0即可。</p><p>求导后得： <span class="math inline">\(t − l o g x − 1 = 0\)</span>,即 <span class="math inline">\(x=e^{t-1}\)</span> ，代入$f<sup><em>(t)$ ，得 $f^</em>(t)=te</sup>{t-1}-e<sup>{t-1}(t-1)=e</sup>{t-1}$</p><p>读者可以对这个 $ f^*(t)$ 再求一次共轭，可以发现其又变回原函数了。</p><p>事实上，可以证明，对于凸函数来说$ f^{**}(x) = f(x)$</p><p><img src="/images/GANs/7.png"></p><h3 id="应用于gan">应用于GAN</h3><p>那这个跟GAN有啥关系呢？</p><p>假如我们用一个 <span class="math inline">\(D_f\)</span>来评估生成模型，对于 <span class="math inline">\(p(x)\)</span> 和 <span class="math inline">\(q(x)\)</span> 之间的 f-divergence：</p><p><span class="math display">\[ \begin{aligned} D_f(P||Q) &amp;=\int_{x} q(x) f\left(\frac{p(x)}{q(x)}\right) dx \\           &amp;=\int_{x} q(x) \left( \max_{t \in \operatorname{dom}(f^*)}\left\{\frac{p(x)}{q(x)}t - f^*(t)\right\} \right) dx\end{aligned}   \]</span></p><p>记一个函数 D(x)，它输入是 <span class="math inline">\(x\)</span>，输出是 <span class="math inline">\(t\)</span> ，用该函数代替上式中的<span class="math inline">\(t\)</span> ，得到</p><p><span class="math display">\[ \begin{aligned} D_f(P||Q)&amp;\geq\int\limits_{x}q(x)(\frac{p(x)}{q(x)}D(x)-f^{*}(D(x)))dx\\ &amp;= \int\limits_{x}p(x)D(x)dx-\int \limits_{x}q(x)f^{*}(D(x))dx \end{aligned}\]</span></p><p>D(x)其实就是判别器，可以看出，它依然是在解一个求最大值问题，通过这种方法，去逼近f-divergence。</p><p><span class="math display">\[D_f(P||Q)\approx\max \limits_{D}\int\limits_{x}p(x)D(x)dx-\int \limits_{x}q(x)f^{*}(D(x))dx\]</span></p><p>p(x) 和 q(x) 本质上是一个概率，于是有</p><p><span class="math display">\[D_f(P||Q)\approx\max\limits_{D}\{E_{x\sim P}[D(x)]-E_{x\sim Q}[f^*(D(x))]\}\]</span></p><p>用 <span class="math inline">\(P_{data}\)</span> 和 <span class="math inline">\(P_\theta\)</span> 来指代 P 和 Q，有</p><p><span class="math display">\[D_f(P_{data}||P_\theta)\approx\max\limits_{D}\{E_{x\sim P_{data}}[D(x)]-E_{x\simP_\theta}[f^*(D(x))]\}\]</span></p><p>有没有发现这一套下来很熟悉？其实这还是我们之前训练生成器判别器的那一套流程。也就是</p><p><span class="math display">\[ \begin{aligned}G^*&amp;=\mathop{argmin}\limits_{G}D_f(P_{data}||P_\theta)\\&amp;=\mathop{argmin}\limits_{G}\max \limits_{D}\{E_{x\sim P_{data}}[D(x)]-E_{x\simP_\theta}[f^*(D(x))]\}\\&amp;=\mathop{argmin} \limits_{G}\max\limits_{D}V(G, D) \end{aligned} \]</span></p><p>只不过这次的损失函数更加 general 了。换不同的 <span class="math inline">\(f(x)\)</span>，就可以量不同的散度（divergence）。</p><p><img src="/images/GANs/8.png"></p><h2 id="wgan">WGAN</h2><h3 id="js散度-to-wassersteinearth-mover-em距离">JS散度 toWasserstein（Earth-Mover EM）距离</h3><h3 id="js散度的问题">JS散度的问题</h3><p>考虑两个分布"完全不相交"的时候，会发现 <span class="math inline">\(JS\)</span> 散度为常量，梯度为 <span class="math inline">\(0\)</span> 无法优化。</p><p>下面一个例子来说明:</p><p>假设两个二维空间上的概率分布，记为 <span class="math inline">\({P}_d(X_1, Z)\)</span> 和 <span class="math inline">\({P}_g(X_2, Z)\)</span> 。我们刻画 <span class="math inline">\(Z \sim U(0, 1)\)</span> 一个 <span class="math inline">\([0, 1]\)</span> 上的均匀分布，而分别令 $ X_1 = 0$和 <span class="math inline">\(X_2 = \theta\)</span>，因而，它们在二维空间上的概率分布空间就是两条平行线（垂直于 <span class="math inline">\(x\)</span> 的轴，而平行于 <span class="math inline">\(z\)</span> 的轴）。<br>当 <span class="math inline">\(\theta = 0.5\)</span>时，我们考量等价于JS散度的损失函数 <span class="math inline">\(V(G,D^*)\)</span>，由于两个分布概率大于0的空间范围是完全没有重叠的，因此，对于任意 <span class="math inline">\(p_d(x,y) \ne 0\)</span> 必然有 <span class="math inline">\(p_g(x, y) =0\)</span> 成立，反之亦然。</p><p>因而我们就有，对于任意 <span class="math inline">\(x \in\mathbb{R}^2\)</span> ，<br><span class="math display">\[V(G, D^*)= \int_x p_d(x) log\frac{p_{d}(x)}{p_{d}(x) + p_{g}(x)} + p_g(x)log\frac{p_{g}(x)}{p_{d}(x) + p_{g}(x)} dx   \\ = \int_x p_d(x) log (1) +p_g(x)log (1) dx = 0 \\ \]</span>此时，损失函数恒为常量，无法继续指导生成器 <span class="math inline">\(G(x)\)</span>的优化。即此时出现了梯度消失的问题。</p><h3 id="wasserstein距离">Wasserstein距离</h3><p>为了弥补JS散度的局限性，我们需要一种全新的”分布间距离“的度量来进行优化，即使用Wasserstein距离，也被称为“推土机距离”（Earth-Mover），它定义如下：<br><span class="math inline">\(W({P}_d, {P}_g) = inf_{\gamma \in \Pi({P}_d,{P}_g)} {E}[||x - y||] \\\)</span>这样数学形式的刻画可能会让人看得颇为一头雾水，我们逐步来分析解释它。</p><p>其中， <span class="math inline">\(\Pi({P}_d, {P}_g)\)</span>代表一个 <span class="math inline">\({P}_d, {P}_g\)</span>构成的联合分布的集合，且这个集合中的所有联合分布必须满足其边际分布分别为<span class="math inline">\({P}_d, {P}_g\)</span> 。 <span class="math inline">\(||x-y||\)</span> 是两个分布所在空间 <span class="math inline">\(\mathbb{R}^n\)</span> 中两点的欧式距离。</p><p>我们可以将 <span class="math inline">\(\Pi({P}_d, {P}_g)\)</span>中的元素理解为一种“概率的搬运方案”。 而 <span class="math inline">\(\gamma\)</span>是上述集合中的一个联合分布，可以使得任意两点的欧式距离期望最小，即将一个分布搬运为另外一个分布的最小开销。</p><p><img src="/images/GANs/9.png"><br>此时，我们再重新观察上面的场景，当概率分布式为两条平行线上的均匀分布时，显然，最佳方案就是直接与x轴平行地进行概率搬运，对应为：<span class="math inline">\(W(P_0, P_\theta) = |\theta|\)</span>。此时，即使两个分布完全没有重叠部分，我们仍然能通过优化Wasserstein距离来实现两个概率分布之间的距离优化。</p><p>可以给出证明的是，就像JS散度一样，Wasserstein距离收敛于0时，两个分布也完全一致。</p><p>固然，通过Wasserstein距离优化GAN的想法颇为"美好"，不过，找到"最优搬运方案"的优化问题却是难事，在实现层面上，我们难以直接计算Wasserstein距离。不过，基于对偶理论可以将Wasserstein距离变换为积分概率度量IPM框架下的形式，来方便我们进行优化。</p><p>IPM也是用于衡量两个分布之间的距离，它的想法是寻找某种限制下的函数空间<span class="math inline">\(\mathbb{F}\)</span> 中的一个函数 <span class="math inline">\(f(·)\)</span>，使得对任意位置两个分布的差异最大：</p><p><span class="math display">\[d_F(p, q) = sup_{f \in F} \mathbb{E}_{x\sim P}[f(x)] - \mathbb{E}_{x \sim Q}[f(x)] \\\]</span>对于Wasserstein距离而言，则变为：</p><p>$<span class="math inline">\(W(p, q) = sup_{||f||_L \le 1}\mathbb{E}_{x \sim P}[f(x)] - \mathbb{E}_{x \sim Q}[f(x)] \\\)</span>￥因而，在函数 $ f(·)$ 满足Lipschitz约束的函数空间中，即 <span class="math inline">\(||f(x) - f(y)|| \le K||x - y||\)</span>，找到最佳的函数 <span class="math inline">\(f(·)\)</span>，该情况下上式的结果则为Wasserstein距离。</p><p>这个函数 <span class="math inline">\(f(·)\)</span>难以求解，但我们可以用神经网络来拟合它。需要注意的是，从此开始，GAN的<span class="math inline">\(D\)</span>就不再是先前我们认为的“真假判别器”了，它的意义变成了一个距离的度量。此时，GAN的生成器并不改变仍然生产图片，对生成器的训练则是减小与真实分布的Wasserstein距离，判别器<span class="math inline">\(D\)</span>负责给出真实图像和生产图像样本之间的Wasserstein距离，相应的，在固定生成器优化判别器时，化则变为了寻找函数空间<span class="math inline">\(\mathbb{F}\)</span> 中最佳的 <span class="math inline">\(f(·)\)</span> 。</p><p><img src="/images/GANs/10.png"><br>下面的图就可以体现传统GAN的判别器梯度和WGAN的判别器梯度的区别</p><p><img src="/images/GANs/11.png"><br>WGAN便有效解决了某些情况下传统GAN的梯度消失的问题</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;本学习笔记用于记录我学习Stanford
CS236课程的学习笔记，分享记录，也便于自己实时查看。&lt;/p&gt;
&lt;h2 id=&quot;引入&quot;&gt;引入&lt;/h2&gt;
&lt;p&gt;前面我们学习了VAEs和Normalizing
Flows，这两种模型都是基于最小化KL散度（对似然进行评估）来</summary>
      
    
    
    
    <category term="Stanford CS236深度生成模型" scheme="https://jia040223.github.io/categories/Stanford-CS236%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="生成模型" scheme="https://jia040223.github.io/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>Normalizing Flows</title>
    <link href="https://jia040223.github.io/2024/09/17/Normalizing%20Flows/"/>
    <id>https://jia040223.github.io/2024/09/17/Normalizing%20Flows/</id>
    <published>2024-09-17T10:18:14.000Z</published>
    <updated>2024-09-17T10:57:29.014Z</updated>
    
    <content type="html"><![CDATA[<p>本学习笔记用于记录我学习StanfordCS236课程的学习笔记，分享记录，也便于自己实时查看。</p><h2 id="引入">引入</h2><p>生成模型模型的目的是让得到的数据分布 <span class="math inline">\(P_{\theta}\)</span> 与真实的数据分布 <span class="math inline">\(P_{data}\)</span>相同，也就是需要通过给定的样本来建模对应的分布，使得输入经过该模型后可以生成与给定样本类似的新样本。在这种意义下，评估的最佳方式便是使用<strong>极大似然估计</strong>，然而VAEs的做法导致计算似然十分复杂，所以我们只能选择计算似然的下界，也就是ELBO。</p><p>不妨思考一下，VAEs无法计算似然的原因是什么。不难发现，关键在于需要对所有的潜变量<span class="math inline">\(z\)</span>进行积分。所以假如我们有一个<strong>可逆映射</strong>，使得潜变量 <span class="math inline">\(z\)</span> 和数据 <span class="math inline">\(x\)</span>之间的是一一对应的，那我们便可以很轻松计算似然了。</p><p>Normalizing Flows正是这么做的。但可逆映射意味着潜变量 <span class="math inline">\(z\)</span> 的维度需要和数据 <span class="math inline">\(x\)</span>的<strong>维度一致</strong>，所以我们无法利用 <span class="math inline">\(z\)</span> 进行压缩。</p><h2 id="简介">简介</h2><p>正则化流（NormalizingFlow）是一种<strong>可逆生成模型</strong>，用于将一个原始分布通过学习的变换映射到另一个<strong>已知</strong>的概率分布。它可以<strong>将数据从原始分布转换为目标分布</strong>，从而实现数据的生成和采样。</p><p>在正则化流中，我们定义一个变换函数，它将输入样本从原始分布映射到目标分布。这个映射是一个<strong>可逆函数</strong>，确保转换是可逆的，也就是说，在给定目标分布样本的情况下，可以逆向计算出原始分布的样本。这个变换函数通常由一系列的可逆操作组成，每个操作都是可逆的，并且通过组合这些操作可以得到整个变换。常用的可逆操作包括仿射变换、尺度变换、平移变换等。</p><h2 id="原理">原理</h2><h3 id="变量替换">变量替换</h3><p>变量替换的形式如下： $ p_{X}(X)=p_{Z}(f(X))|det~J(f(X))|$</p><ul><li><span class="math inline">\(Z=f(X)\)</span> 是一个可逆的变换</li><li><span class="math inline">\(J(f(X))\)</span> 是 <span class="math inline">\(f(X)\)</span> 的雅可比行列式</li></ul><p>如何理解呢：即给出一个 <span class="math inline">\(X\)</span>，使用一个可逆变换 <span class="math inline">\(f(\cdot)\)</span> 将<span class="math inline">\(X\)</span> 变为 <span class="math inline">\(Z\)</span> ，那么 <span class="math inline">\(p(X)、p(Z)\)</span>这两个分布之间相差的就是这样一个雅可比行列式。</p><p><img src="/images/Normalizing%20Flows/1.png"><br>### 流的组合<br>基本原理：可导的可逆的函数在进行组合后依然是一个可导且可逆的函数</p><p>标准化方向： <span class="math inline">\(f=f_{1}\circf_{2}\circ....f_{N}\)</span></p><p>采样构造概率的方向： <span class="math inline">\(g=g_{N} \circg_{N-1} \circ .... \circ g_{1}\)</span></p><p><img src="/images/Normalizing%20Flows/2.png"></p><p>这种流动的感觉就是标准化流这个名字的由来。</p><p>而由 <span class="math inline">\(p_{X}(X)=p_{Z}(f(X))|det~J(f(X))|\)</span>可知，上面组合出来的 <span class="math inline">\(f\)</span>的雅可比行列式刚好可以表示为每一个 <span class="math inline">\(f_{i}\)</span>的<strong>雅可比行列式</strong>相乘再求行列式。</p><p><span class="math inline">\(det~J(f)=det\prod_{i=1}^{N}J(f_{i})=\prod_{i=1}^{N}det~J(f_{i})\)</span></p><p>因为每一个样本都是独立同分布采样出来的，所以它的loglikelihood就是把他们的每一个loglikelihood加起来。由于做过变量代换，就可以把它变成<strong>我们知道的非常简单的分布</strong>加上剩下的<strong>log雅可比行列式</strong>的和。</p><p><img src="/images/Normalizing%20Flows/3.png"></p><h3 id="计算">计算</h3><p>通过最大似然估计，我们便可以训练模型了。但问题在于，如何构建这种可逆映射和如何让雅可比行列式方便计算。因为对于一般的雅可比行列式的计算复杂度是<span class="math inline">\(O(n^3)\)</span>，但是我们可以构造<strong>半三角的雅可比行矩阵</strong>，这样行列式的计算复杂度只有<span class="math inline">\(O(n)\)</span> 了</p><h2 id="nice-non-linear-independent-components-estimation">NICE:Non-linear Independent Components Estimation</h2><p>NICE的目标是找到一个transformation <span class="math inline">\(z=f(x)\)</span> , 将数据映射到一个新的空间中;这个空间中的 <span class="math inline">\(z\)</span> 的各个分量 <span class="math inline">\(z_d\)</span> 之间都是<strong>独立的</strong>, 即<span class="math inline">\(p_\theta(z)=\prod_dp_{\theta_d}(z_d)\)</span> .在这种"各分量独立"的假设下,模型会自发地学习"most important factors of variation"; 否则, 比如 <span class="math inline">\(h_1\)</span> 和 <span class="math inline">\(h_2\)</span> 之间不独立,那么就浪费了一部分建模能力, 从而无法达到最好的建模效果.</p><p>通过 <span class="math inline">\(z\)</span> 的先验分布和 <span class="math inline">\(x=f^{-1}(z)\)</span> , 可以实现 <span class="math inline">\(x\)</span> 的生成(采样)。一般可以假定 <span class="math inline">\(z\)</span> 的分布满足标准高斯分布。</p><h3 id="映射构造additive-coupling-layer">映射构造(Additive couplinglayer)</h3><p>如何构造构造半三角的雅可比行矩阵呢？NICE给出的方法是：</p><p><span class="math inline">\(z_{1\sim d} = x_{1\sim d}\)</span></p><p><span class="math inline">\(z_{ {d\sim D} } = x_{ {d\sim D} } +u_{\theta}(x_{ {1\sim d} })\)</span></p><p>这个变换的雅克比矩阵为</p><p><span class="math display">\[\frac{\partial z}{\partial x}=\left[       \begin{array}{cc}      I_d&amp;  \bar{0}  \\      [\frac{\partial u_\theta}{\partial x_{1\sim d}}]  &amp;  I_{n-d}  \\   \end{array}    \right]   \]</span></p><p>这个映射的逆变换也很简单，为</p><p><span class="math inline">\(x_{1\sim d} = z_{1\sim d}\)</span></p><p><span class="math inline">\(x_{ {d\sim D} } = z_{ {d\sim D} } -u_{\theta}(z_{ {1\sim d} })\)</span></p><h3 id="combining-coupling-layers">Combining coupling layers</h3><p>事实上, 这个 <span class="math inline">\(f\)</span>是要用很多层叠在一起得到的, 即 <span class="math inline">\(f=f_L \circ... \circ f_2 \circ f_1\)</span> 。 在堆叠coupling layer的时候,注意到每个变换有一部分输入是不变的。这样才能让所有部分都能得到变换. 即,第一层 <span class="math inline">\(z_1=x_1\)</span> , 变 <span class="math inline">\(x_2\)</span> , 那么第二层就 <span class="math inline">\(z_2=x_2\)</span> , 变 <span class="math inline">\(z_1\)</span> .</p><p>另外, 堆叠后的雅克比行列式为</p><p><span class="math display">\[\left|\det \frac{\partial z}{\partial x} \right| = \left|\det\frac{\partial f_L(x)}{\partial f_{L-1}(x)}\right| \cdot \left|\det\frac{\partial f_{L-1}(x)}{\partial f_{L-2}(x)}\right| \cdot \ldots\cdot \left|\det \frac{\partial f_2(x)}{\partial f_1(x)}\right|\]</span></p><p>这些行列式的绝对值为1。</p><h3 id="allowing-scaling">Allowing scaling</h3><p>因为每个行列式的绝对值都是1, 因此 <span class="math inline">\(f\)</span> 是volume preserving（体积不变的）的.为了消除这个限制, 在 <span class="math inline">\(f_L\)</span>后又乘了一个diagonal scaling matrix <span class="math inline">\(S\)</span> , 即 <span class="math inline">\(z=S\cdot f_{1, ...,L}(x)\)</span> .这样既可以让一些重要特征又更大的变化范围,又可以让一些不重要的特征减小变化范围(降维). 所以最后目标函数为</p><p><span class="math inline">\(\log p_X(x)=\sum_{i=1}^D [\logp_{H_i}(f_i(x)) + \log |S_{ii}|]\)</span></p><h2 id="density-estimation-using-real-nvp">Density Estimation Using RealNVP</h2><p><strong>RealNVP</strong>将<strong>NICE</strong>中的每一层的映射改为如下:</p><p><span class="math inline">\(\begin{aligned} z_{1:d}&amp;=x_{1:d}\\z_{d+1:D} &amp;=x_{d+1:D} \odot exp(s(x_{1:d})) +t(x_{1:d})\end{aligned}\)</span></p><p>逆变换为</p><p><span class="math inline">\(\begin{aligned} x_{1:d}&amp;=z_{1:d}\\x_{d+1:D} &amp;=(z_{d+1:D}- t(x_{1:d})) \odot exp(-s(x_{1:d}))\end{aligned}\)</span></p><p>这个变换的雅克比矩阵为</p><p><span class="math display">\[  \frac{\partial z}{\partial x}=\left[       \begin{array}{cc}      I_d&amp;  \bar{0}  \\      \frac{\partial z_{d+1:D} }{\partial x_{1:d}}  &amp;  diag(exp(s(x_{1:d})))  \\   \end{array}    \right]   \]</span></p><p>其中 <span class="math inline">\(diag(exp(s(x_{1:d})))\)</span> 是将$ exp(s(x_{1:d}))$ 这个向量展开为对角矩阵.这个雅克比矩阵的log-determinant为 <span class="math display">\[\prod_{i=1}^d \log \exp(s(x_{1:d}))=\sum_{i=1}^ds(x_{1:d})\]</span> 其中没有任何 <span class="math inline">\(s\)</span>和 <span class="math inline">\(t\)</span> 行列式的计算,因此二者可以任意复杂且hidden layer采用不同于输入的维度.</p><p>这样我们便完成了一个更加复杂的构造，同时它的表现也自然比NICE更好。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;本学习笔记用于记录我学习Stanford
CS236课程的学习笔记，分享记录，也便于自己实时查看。&lt;/p&gt;
&lt;h2 id=&quot;引入&quot;&gt;引入&lt;/h2&gt;
&lt;p&gt;生成模型模型的目的是让得到的数据分布 &lt;span class=&quot;math inline&quot;&gt;&#92;(P_{&#92;theta}</summary>
      
    
    
    
    <category term="Stanford CS236深度生成模型" scheme="https://jia040223.github.io/categories/Stanford-CS236%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="生成模型" scheme="https://jia040223.github.io/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>VAEs</title>
    <link href="https://jia040223.github.io/2024/09/13/VAEs/"/>
    <id>https://jia040223.github.io/2024/09/13/VAEs/</id>
    <published>2024-09-13T11:31:24.000Z</published>
    <updated>2024-09-17T10:13:51.034Z</updated>
    
    <content type="html"><![CDATA[<p>本学习笔记用于记录我学习StanfordCS236课程的学习笔记，分享记录，也便于自己实时查看。</p><h2 id="潜变量">潜变量</h2><p>对于生成模型，我们可以试图寻找一组潜变量z，这个潜变量可以有具体含义，例如对于人脸生成模型的眼睛，鼻子，嘴巴等。通过修改这些潜变量我们可以得到不同风格的生成对象。但是对于图片或者自然语言而言，人为指定这种潜变量极为困难。</p><p>所以我们可以并不人为指定潜变量的含义，例如无监督学习的GMM（高斯混合聚类）就并没有指定每个类别具体的含义。但高斯混合聚类人为指定了类别的数量，这对于生成模型也是很难实现定义的。</p><p>对于GMM来说，事实上是定义了一组离散的潜变量，在每个潜变量下的数据分布服从高斯分布，它可以给我们一些启示，虽然每个类别的概率只是定义为正态分布，但它组合之后可以形成非常复杂的概率分布。</p><p><img src="/images/VAEs/1.png"><br>所以不妨我们可以设定有无穷多个高斯聚类的组合，即设定潜变量 z是一个连续的随机变量，而每个潜变量 z的值对应于一个高斯分布，事实上这也正是VAEs做的</p><h2 id="核心思想">核心思想</h2><p>VAE 的目标是学习一个生成器，将随机向量 <span class="math inline">\(z\in R^d\)</span> 映射到 <span class="math inline">\(x \in R^D\)</span> ,使得 <span class="math inline">\(x\)</span>的分布尽可能接近真实数据的分布。</p><p>这里的 <span class="math inline">\(z\)</span>其实就是上面提到的潜变量，他是一个连续的随机变量，实践中一般定义为服从高斯分布。而对于每个z的值，我们假设x的分布是满足均值为<span class="math inline">\(\mu(z)\)</span> ，协方差矩阵为 <span class="math inline">\(\Sigma(z)\)</span>（可以通过神经网络进行学习）的高斯分布。理论上这样的组合可以逼近任意的概率分布。</p><p><img src="/images/VAEs/2.png"><br>当然PPT中的 <span class="math inline">\(z\sim N(0, I)\)</span>只是一个例子，也可以有更复杂的定义，但在实践中<strong>一般使用标准正态分布</strong>。</p><h2 id="生成和训练">生成和训练</h2><h3 id="损失函数">损失函数</h3><p>对于一个生成模型来说，<strong>生成和评估</strong>的难易很大程度上决定了它的实用性和价值。对于上面VAE的假设来说，生成是很简单。即假设我们已经知道了<span class="math inline">\(p(x|z)\)</span> ，我们只需要先<strong>采样<span class="math inline">\(z\)</span> ，再采样 <span class="math inline">\(x\)</span></strong> 就能得到数据。</p><p>但是评估并不容易，这意味着模型的训练可能是一个棘手的问题。对于评估，既然是衡量两个分布的相似度，我们能否直接用各种散度（如KL 散度）作为损失函数呢？当然可以。在蒙特卡洛抽样（Monte CarloSampling）下，最小化KL散度就是<strong>最大似然估计</strong>。</p><p>那么我们的目标是 <span class="math inline">\(θ _∗=argmax\sum_{i=1}^{n}{  logp_  θ​  (x  _i})\)</span> ，注意到</p><p><span class="math inline">\(\sum logP_\theta(x) =\sum  log(\sumq(z)P_\theta(x|z))\)</span></p><p>对于等式右边的计算是非常复杂的，因为 <span class="math inline">\(z\)</span> 的取值理论上具有无穷多个</p><p><img src="/images/VAEs/3.png"><br>所以我们需要对这个公式进行简化，注意到</p><p><span class="math display">\[\begin{align}P_\theta(x) &amp;= \sum (q(z) \frac{p_\theta(z, x)}{q(z)}) \nonumber \\            &amp;= E_{z \simq(z)}\left(\frac{p_\theta(z,x)}{q(z)}\right)\end{align}\]</span></p><p>通过<strong>蒙特卡洛抽样</strong>（Monte Carlo Sampling），我们可以从<span class="math inline">\(q(z)\)</span>中采样若干数据点，然后进行平均即可估计 <span class="math inline">\(P_\theta(x)\)</span>的值。但很可惜，我们无法通过蒙特卡洛抽样来估计 <span class="math inline">\(log(P_\theta(x))\)</span> , 因为</p><p><span class="math inline">\(log(E_{z \simq(z)}(\frac{p_\theta(z,x)}{q(z)})) \ne E_{z \simq(z)}(log(\frac{p_\theta(z,x)}{q(z)}))\)</span></p><p><img src="/images/VAEs/4.png"><br>但幸运的是，对于对数函数是一个严格的<strong>凹函数</strong>，所以对于凹函数来说有</p><p><span class="math inline">\(log(px + (1-p)x^{&#39;}) \geq plogx+(1-p)logx^{&#39;}\)</span> ，进一步扩展便就是著名的琴生不等式：</p><p><img src="/images/VAEs/5.png"></p><p>琴生不等式</p><p>因此 <span class="math inline">\(log(E_{z \simq(z)}(\frac{p_\theta(z,x)}{q(z)})) \geq E_{z \simq(z)}(log(\frac{p_\theta(z,x)}{q(z)}))\)</span></p><p>所以我们可以通过这种方法来<strong>估计似然的下限</strong>，即上面不等号的右边，叫做<strong>ELBO（EvidenceLower Bound）</strong></p><p><img src="/images/VAEs/6.png"><br>至于这个界限有多紧，我们对 <span class="math inline">\(logP(x)\)</span>进行一下推导，就能得到它们之间相差的便是 <span class="math inline">\(D_{KL}(q(z)||p(z|x;\theta)\)</span> ，也就是说当<span class="math inline">\(q(z)\)</span>与我们的后验分布越接近，这个界限越紧。</p><p><img src="/images/VAEs/7.png"><br>其实这里的推导就是<strong>EM算法里面的推导</strong>，最大化ELBO的过程就是对应于EM算法里面的M步（后续有机会可能也会写一写）。非常可惜的是，EM算法无法直接应用于此，因为 E-step 要求我们能够表达出后验分布 <span class="math inline">\(p_\theta(z|x)\)</span>，但没关系，如果我们能够最大化ELBO，也能保证似然的下限被最大化。</p><p>问题似乎解决了，但值得注意的是，ELBO 是关于函数 <span class="math inline">\(q\)</span> 的<strong>泛函</strong>，也就是说 <span class="math inline">\(q\)</span>可以取任意函数，这并不好直接优化。为了解决这个问题，我们可以将 <span class="math inline">\(q(z)\)</span> 限制为以 <span class="math inline">\(\phi\)</span> 为参数的某<strong>可解分布族 <span class="math inline">\(q_\phi(z|x)\)</span></strong>，这样优化变量就从函数 <span class="math inline">\(q\)</span> 变成了参数<span class="math inline">\(\phi\)</span> 。不过，由于我们限制了 <span class="math inline">\(q\)</span> 的形式，所以即便能求出最优的参数 <span class="math inline">\(\phi\)</span> ，也大概率不是 <span class="math inline">\(q\)</span>的最优解。显然，为了尽可能逼近最优解，我们应该让选取的分布族越复杂越好。</p><p><img src="/images/VAEs/8.png"><br><strong>那么这里有一个小问题</strong>——为什么 <span class="math inline">\(q(z)\)</span> 参数化后写作 <span class="math inline">\(q_\phi(z|x)\)</span> 而不是 <span class="math inline">\(q_\phi(z)\)</span> ?</p><p>首先， <span class="math inline">\(q\)</span>本来就是我们人为引入的，它是否以 <span class="math inline">\(x\)</span>为条件完全是我们的设计，且并不与之前的推导冲突；其次，ELBO与似然当 <span class="math inline">\(q(z)=p_θ(z|x)\)</span>时是完全等价的，可见对于不同的 <span class="math inline">\(x\)</span>，其 <span class="math inline">\(q(z)\)</span>的最佳形式是不同的，所以这么设定有利于<strong>减少ELBO与似然的距离</strong>。</p><p><img src="/images/VAEs/9.png"><br>在VAE中 的 <span class="math inline">\(p_θ(x|z)\)</span> 和 <span class="math inline">\(q_\phi(z|x)\)</span>都由神经网络表示，因此我们用<strong>梯度下降</strong>来最大化 ELBO即可。即对ELBO取负数就是最终的损失函数。</p><p><img src="/images/VAEs/10.png"><br>注意到这样的形式中并没有 <span class="math inline">\(p_θ(x|z)\)</span>一项，我们只需要稍微变化一下：</p><p><span class="math display">\[\begin{align}L(x;\theta, \phi) &amp;= \sum q_{\phi}(z|x)\left[\log(p_{\theta}(z,x;\theta)) - \log(q_{\phi}(z|x))\right] \\                  &amp;= \sum q_{\phi}(z|x)\left[\log(p_{\theta}(z,x;\theta)) - \log(p(z)) + \log(p(z)) -\log(q_{\phi}(z|x))\right] \\                  &amp;= \sum q_{\phi} (z|x)\left[\log(p_{\theta}(x|z))- \log\left(\frac{q_{\phi}(z|x)}{p(z)}\right)\right] \\                  &amp;= E_{z \simq_{\phi}(z|x)}\left[\log(p_{\theta}(x|z))\right] - D_{KL}(q_{\phi}(z|x)|| p(z))\end{align}\]</span></p><p>这里就把我们的目标分成了两项：</p><ol type="1"><li>第一项是<strong>重构项</strong>，要求我们尽可能重构数据本身</li><li>第二项是<strong>正则项</strong>，要求我们的后验与先验接近</li></ol><p>所以可以看到，它与自动编码器最大的区别在于有第二项，这保证了隐藏变量<span class="math inline">\(z\)</span>的分布，从而我们<strong>可以从先验中对 <span class="math inline">\(z\)</span>取样</strong>从而进行生成。换句话来说，VAEs是对潜变量进行了正则化的自动编码器，因为我们知道了潜变量<span class="math inline">\(z\)</span>的分布形式，所以它能够用于生成。</p><p>按照<strong>蒙特卡洛抽样</strong>（Monte CarloSampling），理论上求这个期望需要对每个样本多次采样进行计算，最后平均。但在具体实践中，往往采样一次进行计算就行。</p><h3 id="梯度计算细节重参数化技巧">梯度计算细节：重参数化技巧</h3><p>有一个细节是现在 <span class="math inline">\(z\)</span> 是从</p><p><span class="math inline">\(q_\phi(z|x)∼N(μ_ϕ(x)，diag(\sigma^{2}_ϕ(x)))\)</span></p><p>中采样的，但梯度无法经过采样传播到参数 <span class="math inline">\(\phi\)</span>。但其实解决方法很简单，对于高斯函数，只需要先从 <span class="math inline">\(N(0,I)\)</span> 中采样 <span class="math inline">\(\epsilon\)</span> 再计算 <span class="math inline">\(z=μ_ϕ(x)+\epsilon⋅σ_ϕ(x)\)</span> 即可。</p><p>这种技巧也叫做<strong>重参数化技巧</strong>，其最开始应该是在强化学习中出现的，后面有时间也可以写一写。</p><p><img src="/images/VAEs/11.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;本学习笔记用于记录我学习Stanford
CS236课程的学习笔记，分享记录，也便于自己实时查看。&lt;/p&gt;
&lt;h2 id=&quot;潜变量&quot;&gt;潜变量&lt;/h2&gt;
&lt;p&gt;对于生成模型，我们可以试图寻找一组潜变量z，这个潜变量可以有具体含义，例如对于人脸生成模型的眼睛，鼻子，嘴巴等</summary>
      
    
    
    
    <category term="Stanford CS236深度生成模型" scheme="https://jia040223.github.io/categories/Stanford-CS236%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"/>
    
    
    <category term="生成模型" scheme="https://jia040223.github.io/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
</feed>
