{"meta":{"title":"Serendipity's Blog","subtitle":"","description":"","author":"Serendipity","url":"https://jia040223.github.io","root":"/"},"pages":[{"title":"关于我","date":"2024-09-17T10:50:16.194Z","updated":"2024-09-17T10:50:16.194Z","comments":false,"path":"about/index.html","permalink":"https://jia040223.github.io/about/index.html","excerpt":"","text":"欢迎来到我的博客！这个博客是我个人的学习和探索之旅的记录，我希望通过它分享我的想法和见解。无论是技术、理论还是生活中的点滴，我都会在这里进行更新。如果你对我的内容有任何疑问或建议，欢迎通过 GitHub 与我联系。期待与你的交流与互动！ 个人简介 教育经历 高中：石门县第一中学 本科：中国科学院大学（计算机科学与技术专业） 目前感兴趣的方向 大模型 深度学习 计算机视觉 关于博客 学习笔记 我会持续更新我目前正在学习的内容和笔记，包括但不限于深度学习与大模型，计算机视觉，微积分与线性代数等。 项目地址：repository 科研日志 我会记录一些科研上的心得体会，分享一些我在科研中的经验和教训。 生活记录 我会记录一些好玩的生活片段，同样包括但不限于旅游日志、生活琐事和所悟所想，分享是一种很好的生活调味剂，希望你会喜欢。 联系方式 Email：jiachenghao21@mails.ucas.ac.cn QQ：1142747996"}],"posts":[{"title":"更新说明 2024.9.30","slug":"更新日志","date":"2024-09-29T16:34:30.000Z","updated":"2024-09-29T17:01:04.065Z","comments":true,"path":"2024/09/30/更新日志/","permalink":"https://jia040223.github.io/2024/09/30/%E6%9B%B4%E6%96%B0%E6%97%A5%E5%BF%97/","excerpt":"","text":"国庆假期到了，这学期课程压力比较小，所以也是能为以后的科研学习一下相关知识。但国庆假期还是给自己放了一个大长假，这段时间估计是不太会更新了。 Stanford CS236 最近Stanford CS236课程也算是看完了，后面可能还会有一些内容打算写一写吧。主要还是围绕diffusion，包括 ldm diffusion的condition控制 如何把diffusion用于离散的数据 前面的文章可能也会补一补。感觉diffusion涉及到的数学知识还是挺多的，后面有机会可以来补一补数学基础。 Probabilistic Machine Learning 打算学习一下Probabilistic Machine Learning这本书，后面应该也会边学边记录一下，也强烈给读者推荐这本书，特别对于像我这样致力于在AI领域进行研究但基础比较薄弱的同学。 这本书应该也是将来一段时间我的学习重点了，内容还是很多的。 数学 Stanford CS236课程还是涉及到挺多的数学知识，后面有机会可以来补一补数学基础。之前保研复习了一下微积分，线代，微分方程这些，后面可能会多看一看优化相关（比如什么拉格朗日对偶问题，每次遇到都是混过去了）的知识，同时对diffusion涉及的一些知识也多了解了解，可能包括： SDE和ODE的解法 傅里叶变换 优化理论 科研 国庆之后也可能会具体进行一些导师的项目，后面在科研上的学习有机会也可以记录一下。 碎碎念 感觉还是太菜了，什么都不会。感觉大学四年在课堂上学的东西真的太基础了。 以前本科的实习也就是看了几篇论文就开始做，然后也就用的别人的模型，在上面小修小补，以至于做了一学期的生成模型，现在看了Stanford CS236，感觉以前真的啥都不知道。 虽然可能跟着别人脚步走也能发论文吧，但还是希望能夯实一下理论基础，希望以后科研的日子能过得轻松一点。","categories":[{"name":"更新日志","slug":"更新日志","permalink":"https://jia040223.github.io/categories/%E6%9B%B4%E6%96%B0%E6%97%A5%E5%BF%97/"}],"tags":[{"name":"更新日志","slug":"更新日志","permalink":"https://jia040223.github.io/tags/%E6%9B%B4%E6%96%B0%E6%97%A5%E5%BF%97/"}]},{"title":"Diffusion Model原理","slug":"Diffusion Model原理","date":"2024-09-29T10:34:53.000Z","updated":"2024-09-29T17:08:28.988Z","comments":true,"path":"2024/09/29/Diffusion Model原理/","permalink":"https://jia040223.github.io/2024/09/29/Diffusion%20Model%E5%8E%9F%E7%90%86/","excerpt":"","text":"本学习笔记用于记录我学习Stanford CS236课程的学习笔记，分享记录，也便于自己实时查看。 引入 前面的课程中我们已经学习了许多生成模型的架构，例如VAEs，Score Based Models等。在课程的最后也是总算来到当前最火的生成模型架构：Diffusion Model。其实Diffusion Model与前面模型或多或少都有一定的联系，我们也可以从不同的视角来理解它。 笔者本科科研也算是学习研究了一些Diffusion相关的工作，但之前一直没有去梳理生成模型的发展，也没有深究其背后的数学原理。所以借此几乎，正好对一些知识进行整理，并对生成模型进行部分回顾。首先从DDPM和DDIM入手吧，这两篇文章也是之前科研实践学习过很多次了。 DDPM 首先我们知道，DDPM 是个马尔科夫模型（如下图），DDPM包括两个步骤。这两个步骤在原文中定义为前向加噪（forward，下图从右到左）和后向去噪（reverse，下图从左到右）。 从 \\(x_0\\) 到 \\(x_T\\) 的过程就是前向加噪过程，我们可以看到加噪过程就是对原始图片 \\(x_0\\) 不断添加噪声，使其最后信噪比趋近于0，此时得到的图片也就变成噪声了，而与之相对应的去噪过程就是还原过程，即从噪声不断去噪还原为图片。 我们通过往图片中加入噪声，使得图片变得模糊起来，当加的步骤足够多的时候（也就是T的取值越大的时候，一般取1000），图片已经非常接近一张纯噪声。纯噪声也就意味着多样性，我们的模型在去噪（还原）的过程中能够产生更加多样的图片。 这里的操作实际上就是指在图片加入噪声 \\(noise\\) ，噪声 \\(noise\\) 本身的分布可以是很多样的（btw，保研还被问过这个问题），而论文中采用的是标准正态分布，其理由是考虑到其优良的性质，在接下来的公式推理中见到。 推导 从上面的图可知，DDPM 将前向过程和逆向过程都设计为了马尔可夫链的形式： 称从 \\(x_0\\) 到 \\(x_T\\) 的马尔可夫链为前向过程 (forward process) 或扩散过程 (diffusion process)； 称从 \\(x_T\\) 到 \\(x_0\\) 的马尔可夫链为逆向过程 (reverse process) 或去噪过程 (denoising process). 所以我们的损失函数通过极大似然估计来进行。但这里我们又会遇到和VAE一样的问题， \\(log(P(x))\\) 中的 \\(P(x)\\) 需要对 \\(x_{1:T}\\) 进行积分，此时我们便可以效仿VAE的做法，即把 \\(x_{1:T}\\) 作为类似VAE中的潜变量，去优化对数似然的下界ELBO（为什么是下界可以参考我都VAEs的文章，简单来说就是用琴生不等式即可）： \\[ \\begin{align*} ELBO &amp;= \\mathbb{E}_{\\mathbf{x}_{1:T} \\sim q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_0)} \\left[\\log \\frac{p_\\theta(\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_0)} \\right] \\\\&amp;= \\mathbb{E}_{\\mathbf{x}_{1:T} \\sim q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_0)} \\left[ \\log \\frac{p(\\mathbf{x}_T) \\prod_{t=1}^T p_\\theta(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t)}{\\prod_{t=1}^T q(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1})} \\right] \\end{align*} \\] 至于这里为啥要在给定 \\(x_0\\) 下计算，一方面是单纯的 \\(q(\\mathbf{x}_{1:T})\\) 我们没办法计算得出，而 \\(q(\\mathbf{x}_{1:T}|\\mathbf{x}_0)\\) 我们能求出其闭式解，另一方面在训练时我们的确已经 \\(x_0\\)的信息。 OK，我们继续进行推导 \\[ \\begin{align} &amp;\\ \\ \\ \\ \\ \\text{ELBO}(\\mathbf x_0) \\\\ &amp;=\\mathbb E_{q(\\mathbf x_{1:T}\\vert\\mathbf x_0)}\\left[\\log\\frac{p(\\mathbf x_T)\\prod_{t=1}^{T}p(\\mathbf x_{t-1}\\vert\\mathbf x_t)}{\\prod_{t=1}^{T}q(\\mathbf x_t\\vert\\mathbf x_{t-1})}\\right]\\\\ &amp;=\\mathbb E_{q(\\mathbf x_{1:T}\\vert\\mathbf x_0)}\\left[\\log\\frac{p(\\mathbf x_T)\\prod_{t=1}^{T}p(\\mathbf x_{t-1}\\vert\\mathbf x_t)}{q(\\mathbf x_1\\vert\\mathbf x_0)\\prod_{t=2}^{T}q(\\mathbf x_t\\vert\\mathbf x_{t-1},\\mathbf x_0)}\\right]\\\\ &amp;=\\mathbb E_{q(\\mathbf x_{1:T}\\vert\\mathbf x_0)}\\left[\\log\\frac{p(\\mathbf x_T)\\prod_{t=1}^{T}p(\\mathbf x_{t-1}\\vert\\mathbf x_t)}{q(\\mathbf x_1\\vert\\mathbf x_0)\\prod_{t=2}^{T}\\frac{q(\\mathbf x_t\\vert\\mathbf x_0)q(\\mathbf x_{t-1}\\vert\\mathbf x_t,\\mathbf x_0)}{q(\\mathbf x_{t-1}\\vert\\mathbf x_0)} }\\right]\\\\ &amp;=\\mathbb E_{q(\\mathbf x_{1:T}\\vert\\mathbf x_0)}\\left[\\log\\frac{p(\\mathbf x_T)\\prod_{t=1}^{T}p(\\mathbf x_{t-1}\\vert\\mathbf x_t)}{q(\\mathbf x_T\\vert\\mathbf x_0)\\prod_{t=2}^{T}q(\\mathbf x_{t-1}\\vert\\mathbf x_t,\\mathbf x_0)}\\right]\\\\ &amp;=\\mathbb E_{q(\\mathbf x_{1:T}\\vert\\mathbf x_0)}\\left[\\log p(\\mathbf x_0\\vert\\mathbf x_1)\\right]+\\mathbb E_{q(\\mathbf x_{1:T}\\vert\\mathbf x_0)}\\left[\\log\\frac{p(\\mathbf x_T)}{q(\\mathbf x_T\\vert\\mathbf x_0)}\\right]+\\sum_{t=2}^T\\mathbb E_{q(\\mathbf x_{1:T}\\vert\\mathbf x_0)}\\left[\\log\\frac{p(\\mathbf x_{t-1}\\vert\\mathbf x_t)}{q(\\mathbf x_{t-1}\\vert\\mathbf x_t,\\mathbf x_0)}\\right]\\\\ &amp;=\\mathbb E_{q(\\mathbf x_{1}\\vert\\mathbf x_0)}\\left[\\log p(\\mathbf x_0\\vert\\mathbf x_1)\\right]+\\mathbb E_{q(\\mathbf x_{T}\\vert\\mathbf x_0)}\\left[\\log\\frac{p(\\mathbf x_T)}{q(\\mathbf x_T\\vert\\mathbf x_0)}\\right]+\\sum_{t=2}^T\\mathbb E_{q(\\mathbf x_t\\vert\\mathbf x_0)}\\mathbb E_{q(\\mathbf x_{t-1}\\vert\\mathbf x_t,\\mathbf x_0)}\\left[\\log\\frac{p(\\mathbf x_{t-1}\\vert\\mathbf x_t)}{q(\\mathbf x_{t-1}\\vert\\mathbf x_t,\\mathbf x_0)}\\right]\\\\ &amp;=\\underbrace{\\mathbb E_{q(\\mathbf x_{1}\\vert\\mathbf x_0)}\\left[\\log p(\\mathbf x_0\\vert\\mathbf x_1)\\right]}_\\text{reconstruction term}-\\underbrace{\\text{KL}(q(\\mathbf x_T\\vert\\mathbf x_0)\\Vert p(\\mathbf x_T))}_\\text{regularization term}-\\sum_{t=2}^T\\mathbb E_{q(\\mathbf x_t\\vert\\mathbf x_0)}\\underbrace{\\left[\\text{KL}(q(\\mathbf x_{t-1}\\vert\\mathbf x_t,\\mathbf x_0)\\Vert p(\\mathbf x_{t-1}\\vert\\mathbf x_t))\\right]}_\\text{denoising matching terms} \\end{align} \\] 同样出现了重构项、正则项和匹配项。重构项要求 $x_1 $ 能够重构 \\(x_0\\) ，正则项要求 \\(x_T\\) 的后验分布逼近先验分布，而匹配项则建立起相邻两项 $x_{t−1},x_t $ 之间的联系。 现在，我们只需要为式中出现的所有概率分布设计具体的形式，就可以代入计算了。为了让 KL 散度可解，一个自然的想法就是把它们都设计为正态分布的形式。 前向过程 在DDPM的前向过程中，对于 \\(t \\in [1,T]\\) 时刻， \\(x_t\\) 和 \\(x_{t-1}\\) 满足如下关系： \\[x_t = \\sqrt{1-\\beta_t}x_{t-1} + \\sqrt{\\beta_t }\\epsilon, \\ \\ \\ \\epsilon\\sim N(0,1)\\] 其中 \\(β_t∈(0,1)\\) 是事先指定的超参数，代表从 $x_{t−1} $ 到 $x_t $ 这一步的方差。 这里的系数设定为开根号的 $ $ ，是为了保证马尔科夫链的最后收敛为标准高斯分布。 \\(\\sqrt\\beta\\) 和 \\(\\sqrt{1-\\beta}\\) 是怎么来的： 我们这里先不管 \\(\\beta\\) ，把两个系数分别设为 \\(a\\) 和 \\(b\\) 。 公式变为： \\[x_t = ax_{t-1} + b\\epsilon\\] 我们希望，当 \\(t\\)趋于无穷的时候， \\(x_t \\sim N(0,1), x_{t-1} \\sim N(0,1)\\) 我们知道当两个高斯分布相加时， \\[X\\sim N(\\mu_X,\\sigma_X^2),Y\\sim N(\\mu_Y,\\sigma_Y^2) \\] \\[Z=aX+bY \\] 则 \\[Z \\sim N(a\\mu_X+b\\mu_Y, a^2\\sigma^2+b^2\\sigma^2)\\] 所以此时 \\[x_t~\\sim N(a\\mu_{t-1}+b\\mu_\\epsilon,a^2\\sigma_{t-1}^2+b^2\\sigma_\\epsilon^2)\\] \\[x_t\\sim N(a·0+b·0, a^2·1+b^2·1)\\] \\[x_t \\sim N(0,a^2+b^2) \\] 我们想让 \\(x_{t-1}\\) 和 \\(\\epsilon\\) 得到的 \\(x_{t}\\) 也服从标准正态分布，即 $ x_{t} N(0,1)$ ，那么我们就只能让 \\(a^2+b^2=1\\) 。 再令 \\(\\beta=a^2\\) ，则 \\(a=\\sqrt{\\beta},b=\\sqrt{1-\\beta}\\) 。 或者也可以令 \\(\\alpha=b^2\\) ，则 $ a=x_{t-1}+$ 。 说白了，这俩系数就是为了让两个服从标准正态分布的噪声相加得到的东西还是服从正态分布。 OK，在这基础上我们可以继续推导，让 \\(x_t\\) 用 \\(x_0\\) 来表示： 令 \\(\\alpha_t=1-\\beta_t\\) ，则公式变为： \\[x_t=\\sqrt{\\alpha_t}x_{t-1}+\\sqrt{1-\\alpha_t}\\epsilon\\] 继续推导： \\[\\begin{align*} x_t &amp;=\\sqrt{\\alpha_t}x_{t-1}+\\sqrt{1-\\alpha_t}\\epsilon\\\\ &amp;=\\sqrt{\\alpha_t}(\\sqrt{\\alpha_{t-1} }x_{t-2}+\\sqrt{1-\\alpha_{t-1} }\\epsilon)+\\sqrt{1-\\alpha_t}\\epsilon\\\\ &amp;=\\sqrt{\\alpha_t\\alpha_{t-1} }x_{t-2}+\\sqrt{\\alpha_t(1-\\alpha_{t-1})}\\epsilon + \\sqrt{1-\\alpha_t}\\epsilon\\\\ \\end{align*}\\] 上式最后一行第二项和第三项，可以看做两个正态分布相加。 由于两个正态分布 \\(X\\sim N(\\mu_x,\\sigma_x^2), Y\\sim N(\\mu_y, \\sigma_y^2)\\) ，相加后有 \\(aX+bY\\sim N(a\\mu_x+b\\mu_y,a^2\\sigma_x^2+b^2\\sigma_y^2)\\)。所以，合并两个正态分布，得到： \\[x_t=\\sqrt{\\alpha_t\\alpha_{t-1} }x_{t-2}+\\sqrt{1-\\alpha_t\\alpha_{t-1} }\\epsilon\\] 由数学归纳法，可以推导出： \\[x_t=\\sqrt{\\alpha_t\\alpha_{t-1}...\\alpha_1}x_0+\\sqrt{1-\\alpha_t\\alpha_{t-1}...\\alpha_1}\\epsilon\\] 再令 \\(\\bar\\alpha_t=\\alpha_t\\alpha_{t-1}...\\alpha_1\\) ，则公式可以进一步化简为： \\(x_t=\\sqrt {\\bar\\alpha_t}x_0+\\sqrt{1-\\bar\\alpha_{t} }\\epsilon\\) ，由于 \\[\\lim_{t\\to\\infty}\\sqrt{\\bar\\alpha_t}=0,\\quad\\lim_{t\\to\\infty}\\sqrt{1-\\bar\\alpha_t}=1\\] 所以我们能够保证马尔科夫链最后能够收敛于标准正态分布 逆向过程 这里从我们熟知的贝叶斯公式出发： \\[P(A|B)=\\frac{P(B|A)P(A)}{P(B)}\\] 可知 \\[P(x_{t-1}|x_t)=\\frac{P(x_t|x_{t-1})P(x_{t-1})}{P(x_t)}\\] 这里我们的 \\(P(x_{t-1})\\) 和 \\(P(x_t)\\) 我们都不知道，但在已知 $ x_0$ 的情况下有： \\[P(x_{t-1}|x_t,x_0)=\\frac{P(x_t|x_{t-1},x_0)P(x_{t-1}|x_0)}{P(x_t|x_0)}\\] 把 \\(x_0=\\sqrt{\\bar{\\alpha_t} }x_0\\) 和 \\(x_t=\\sqrt {\\bar\\alpha_t}x_0+\\sqrt{1-\\bar\\alpha_{t} }\\epsilon\\) 带入上式，可得： \\[P(x_{t-1}|x_t,x_0)=\\frac{ N(\\sqrt{\\alpha_t}x_0,1-\\bar\\alpha_t) N(\\sqrt{\\bar\\alpha_{t-1} }x_0,1-\\bar\\alpha_{t-1}) }{ N(\\sqrt{\\bar\\alpha_{t} }x_0,1-\\bar\\alpha_{t}) }\\] 已知高斯分布的概率密度函数为： \\[f(x)=\\frac{1}{\\sqrt{2\\pi\\sigma} }exp(-\\frac{(x-\\mu)^2}{2\\sigma^2})\\] 所以 \\[P(x_{t-1}|x_t,x_0) \\propto exp-\\frac{1}{2} [ \\frac{(x_t-\\sqrt{\\alpha_t}x_{t-1})^2}{1-\\alpha_t} +\\frac{(x_{t-1}-\\sqrt{\\bar\\alpha_{t-1} }x_0)^2}{1-\\bar\\alpha_{t-1} } -\\frac{(x_{t}-\\sqrt{\\bar\\alpha_{t} }x_0)^2}{1-\\bar\\alpha_{t} } ]\\] 此时由于 \\(x_{t-1}\\) 是我们关注的变量，所以整理成关于 \\(x_{t-1}\\) 的形式： \\[P(x_{t-1}|x_t,x_0) \\propto exp-\\frac{1}{2} [ (\\frac{\\alpha_t}{1-\\alpha_t}+\\frac{1}{1-\\bar\\alpha_{t-1} })x_{t-1}^2 -(\\frac{-2\\sqrt{\\alpha_t}x_t}{1-\\alpha_t} + \\frac{-2\\sqrt{\\bar\\alpha_{t-1} }x_0}{1-\\bar\\alpha_{t-1} })x_{t-1} +C(x_t,x_0) ]\\] 其中第三项 \\(C(x_t,x_0)\\) 与 \\(x_{t-1}\\) 无关，作为指数上相加的部分，可以拿到最前面只影响最前面的系数。 所以此时： \\[P(x_{t-1}|x_t,x_0) \\propto exp-\\frac{1}{2} [ (\\frac{\\alpha_t}{1-\\alpha_t}+\\frac{1}{1-\\bar\\alpha_{t-1} })x_{t-1}^2 -(\\frac{-2\\sqrt{\\alpha_t}x_t}{1-\\alpha_t} + \\frac{-2\\sqrt{\\bar\\alpha_{t-1} }x_0}{1-\\bar\\alpha_{t-1} })x_{t-1}]\\] 又因为标准正态分布满足 \\(\\propto exp - \\frac{x^2-2\\mu x + \\mu^2}{2\\sigma^2}\\) ，所以我们可以得到 \\(P(x_{t-1}|x_t,x_0)\\) 对应的方差 \\[ \\frac{1}{\\sigma^2}=\\frac{\\alpha_t}{1-\\alpha_t}+\\frac{1}{1-\\bar\\alpha_{t-1} } =\\frac{1-\\alpha_t\\bar\\alpha_{t-1} }{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})} =\\frac{1-\\bar\\alpha_{t} }{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}\\] 这里 \\(\\alpha_t\\bar\\alpha_{t-1}=\\bar\\alpha_t\\) 。所以： \\[\\sigma^2=\\frac{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}\\] 再看 \\(x_{t-1}\\) 的一次项，得到： \\[\\frac{2\\mu}{\\sigma^2}= (\\frac{-2\\sqrt{\\alpha_t}x_t}{1-\\alpha_t} + \\frac{-2\\sqrt{\\bar\\alpha_{t-1} }x_0}{1-\\bar\\alpha_{t-1} })\\] 把 \\(\\sigma^2\\) 和 \\(x_0\\) 带入上式，化简得到： \\[\\mu=\\frac{1}{\\sqrt{\\alpha_t} }(x_t-\\frac{1-\\alpha_t}{\\sqrt{1-\\bar\\alpha_t} }\\epsilon)\\] 所以说： \\[P(x_{t-1}|x_t, x_0)\\sim N(\\frac{1}{\\sqrt{\\alpha_t} }(x_t-\\frac{1-\\alpha_t}{\\sqrt{1-\\bar\\alpha_t} }\\epsilon), \\frac{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t})\\] 回顾一下我们写的这一大段公式，也就是说，我们已知了先验概率，推导出了后验概率的表达式，得到了在给定 \\(x_0\\) 后的\\(x_{t-1}\\) 的分布的均值和方差。也就是说，上面公式中，我们的 \\[q(x_{t-1}\\vert x_t,x_0)\\sim N(\\frac{1}{\\sqrt{\\alpha_t} }(x_t-\\frac{1-\\alpha_t}{\\sqrt{1-\\bar\\alpha_t} }\\epsilon), \\frac{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t})\\] 接下来， \\(\\epsilon\\) 的具体值，我们让模型去拟合就好了。 损失函数 我们之前已经推导了ELBO的具体形式： \\[\\text{ELBO}= \\underbrace{E_{x_1\\sim q(x_1\\vert x_0)}[\\log p_\\theta(x_0\\vert x_1)]}_{ {L_0} }- \\underbrace{KL(q(x_T \\vert x_0)\\|p(x_T))}_{ {L_T} }- \\sum_{t=2}^T\\underbrace{E_{x_t\\sim q(x_t\\vert x_0)}\\left[KL(q(x_{t-1}\\vert x_t,x_0)\\|p_\\theta(x_{t-1}\\vert x_t))\\right]}_{ {L_{t-1} }}\\] 这里 \\(q(x_{t-1}\\vert x_t,x_0)\\) 我们已经得到了， \\(q(x_{t}|x_0)\\) 也是我们定义的。只需要定义 \\(p_\\theta(x_{t-1}|x_t)\\) 即可，为了计算方便，我们也选择与 \\(q(x_{t-1}\\vert x_t,x_0)\\) 一样的形式。 \\[p_\\theta(\\textbf{x}_{t-1}|\\textbf{x}_t) = \\mathcal{N}(\\textbf{x}_{t-1}; \\mu_\\theta(\\textbf{x}_t, t), \\frac{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}I)\\] 其中 \\(\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t) = \\frac{1}{\\sqrt{\\alpha_t} } \\Big( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t} } \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t) \\Big)\\) ，而 \\({\\epsilon}_\\theta(\\mathbf{x}_t, t)\\) 就是我们模型的输出。此时，我们带入可以得到 \\[\\begin{align} \\mathbf{x}_{t-1} &amp;= \\mathcal{N}(\\mathbf{x}_{t-1}; \\frac{1}{\\sqrt{\\alpha_t} } ( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t} } {\\epsilon}_\\theta(\\mathbf{x}_t, t) ), \\frac{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}I) \\end{align}\\] 带入上面KL散度的公式，可以得到损失函数 \\(L_t\\) 便为： \\[\\begin{aligned} L_t &amp;= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon} } \\Big[\\frac{1}{2 \\| \\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t) \\|^2_2} \\| \\color{blue}{\\tilde{\\boldsymbol{\\mu} }_t(\\mathbf{x}_t, \\mathbf{x}_0)} - \\color{green}{\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t)} \\|^2 \\Big] \\\\ &amp;= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon} } \\Big[\\frac{1}{2 \\|\\boldsymbol{\\Sigma}_\\theta \\|^2_2} \\| \\color{blue}{\\frac{1}{\\sqrt{\\alpha_t} } \\Big( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t} } \\boldsymbol{\\epsilon}_t \\Big)} - \\color{green}{\\frac{1}{\\sqrt{\\alpha_t} } \\Big( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t} } \\boldsymbol{\\boldsymbol{\\epsilon} }_\\theta(\\mathbf{x}_t, t) \\Big)} \\|^2 \\Big] \\\\ &amp;= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon} } \\Big[\\frac{ (1 - \\alpha_t)^2 }{2 \\alpha_t (1 - \\bar{\\alpha}_t) \\| \\boldsymbol{\\Sigma}_\\theta \\|^2_2} \\|\\boldsymbol{\\epsilon}_t - \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)\\|^2 \\Big] \\\\ &amp;= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon} } \\Big[\\frac{ (1 - \\alpha_t)^2 }{2 \\alpha_t (1 - \\bar{\\alpha}_t) \\| \\boldsymbol{\\Sigma}_\\theta \\|^2_2} \\|\\boldsymbol{\\epsilon}_t - \\boldsymbol{\\epsilon}_\\theta(\\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon}_t, t)\\|^2 \\Big] \\end{aligned}\\] 发现可以使用不用权重的简单形式就可以训练得到好的结果，即 \\[\\begin{aligned} L_\\text{simple} &amp;= \\mathbb{E}_{t \\sim [1, T], \\mathbf{x}_0, \\boldsymbol{\\epsilon}_t} \\Big[\\|\\boldsymbol{\\epsilon}_t - \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)\\|^2 \\Big] \\\\ &amp;= \\mathbb{E}_{t \\sim [1, T], \\mathbf{x}_0, \\boldsymbol{\\epsilon}_t} \\Big[\\|\\boldsymbol{\\epsilon}_t - \\boldsymbol{\\epsilon}_\\theta(\\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon}_t, t)\\|^2 \\Big] \\end{aligned}\\] 这样，我们就获得了DDPM的最终目标函数： \\[ L_\\text{simple}(\\theta)=\\mathbb E_{t,x_0,\\epsilon}\\left[\\Vert\\epsilon-\\epsilon_\\theta(x_t,t)\\Vert^2\\right]\\] 具体训练流程和采样流程如下： DDIM DDPM虽好，但它只能一步一步老老实实通过 \\(x_{t}\\) 预测 \\(x_{t-1}\\) ，不能跨步运算，如果 \\(T =1000\\) ，那么生成一整图像就需要用网络推理1000次，效率很低。于是为了结局这个问题，DDIM出现了，而且最巧妙的是它不需要重新训练模型。 DDIM始于一个假设，它假设了 \\[P(x_{prev}|x_t,x_0)\\sim N(kx_0+mx_t,\\sigma_2)\\] \\[x_{prev}=kx_0+mx_t+\\sigma\\epsilon,\\ \\ \\ \\ \\ \\epsilon\\sim N(0,1)\\] 又因为加噪过程满足公式 \\(x_t=\\sqrt {\\bar\\alpha_t}x_0+\\sqrt{1-\\bar\\alpha_{t} }\\epsilon\\) 把 \\(x_t\\) 带入 \\(x_{t-1}\\) 合并同类项得到： \\[\\begin{align*} x_{prev}&amp;=kx_0+m(\\sqrt{\\bar\\alpha_t}x_0+\\sqrt{1-\\bar\\alpha_t}\\epsilon)+\\sigma\\epsilon\\\\ &amp;=(k+m\\sqrt{\\bar\\alpha_t})x_0+\\epsilon&#39; \\end{align*}\\] \\[\\epsilon&#39;\\sim N(0,m^2(1-\\bar\\alpha_t)+\\sigma^2)\\] 又因为 \\(x_{prev}=\\sqrt {\\bar\\alpha_{prev} }x_0+\\sqrt{1-\\bar\\alpha_{prev} }\\epsilon\\) ，满足对应系数相同，有： \\[k+m\\sqrt{\\bar\\alpha_t}=\\sqrt{\\bar{\\alpha_{prev} }}\\\\ m^2(1-\\bar\\alpha_t)+\\sigma^2=1-\\bar\\alpha_{prev}\\] 求得： \\[m=\\frac{\\sqrt{1-\\bar\\alpha_{prev}-\\sigma^2} }{\\sqrt{1-\\bar\\alpha_t} }\\\\ k=\\sqrt{\\bar\\alpha_{prev} }-\\frac{\\sqrt{1-\\bar\\alpha_{prev}-\\sigma^2} }{\\sqrt{1-\\bar\\alpha_t} }\\sqrt{\\bar\\alpha_t}\\] 带入公式最终化简得： \\[x_{prev}=\\sqrt{\\bar{\\alpha_{prev} }} (\\frac{x_t-\\sqrt{1-\\bar\\alpha_t}\\epsilon_t}{\\sqrt{\\bar\\alpha_t} }) +\\sqrt{1-\\bar\\alpha_{prev}-\\sigma^2}\\epsilon_t+\\sigma^2\\epsilon\\] 其中 \\(t\\) 和 \\(prev\\) 可以相隔多个迭代步数，一般相隔20可以做到采样速度和采样质量比较好地平衡。所以一般DDPM要做1000步，而DDIM是需要50步就可以完成采样。 当这里的 \\(\\sigma\\) 选取0的时候，也就意味着变成了一个确定性采样的过程。此时的DDIM就变成了一个Flow Models，事实上论文里也是这么做的。 从不同角度看扩散模型 前面我们DDPM的推导过程中，其实可以把扩散模型看成一个给定后验的多层VAE。即认为设定了 \\(p(x_{1:T}|x_0)\\) 的形式，然后让模型来从潜变量中采样，最终生成图片。 而DDIM把这个过程变成了一个确定性过程，也就是说把潜变量和数据之间做了一个双射，所以此时也就可以看成Flow Models的一个了 事实上，扩散模型的连续和离散其实对应着随机过程里的概念。一般来说，discrete time指的是随机过程中的时间 \\(t\\) 只能取离散整数值，而continous-time则指的是时间参数 \\(t\\) 可以取连续值。discrete time随机过程中的参数在一个离散的时间点只能改变一次；而continuous-time随机过程的参数则可以随时发生变化。 DDPM和SDE 我们在DDPM里的加噪过程。每一个time step，我们都会按照如下的离散马尔可夫链进行加噪： \\[x_i = \\sqrt{1 - \\beta_i}x_{i-1} + \\sqrt{\\beta_i} \\epsilon_{i-1}, i=1,..., N\\] 为了将上述过程连续化，我们需要引入连续时间随机过程。而连续时间其实就是让每个离散的时间间隔 \\(\\Delta t\\) 无限趋近于0，其实也等价于求出 \\(N \\to \\infty\\) ​时，上述马尔可夫链的极限 在求极限之前，我们需要先引入一组辅助的noise scale \\(\\{\\bar{\\beta}_i = N \\beta_i\\}_{i=1}^N\\) ，并将上面的式子改写如下： \\[x_i = \\sqrt{1 - \\frac{\\bar{\\beta}_i}{N} }x_{i-1} + \\sqrt{\\frac{\\bar{\\beta}_i}{N} }\\epsilon_{i-1}, i = 1,..., N\\] 在 \\(N \\to \\infty\\) ​时，上面的 \\(\\{\\bar{\\beta}_i\\}_{i=1}^{N}\\) 就成了一个关于时间 \\(t\\) 的连续函数 $ (t)$ ​，并且 \\(t \\in [0, 1]\\) 。随后，我们可以假设 \\(\\Delta t = \\frac{1}{N}\\) ​，在每个 \\(i\\Delta t\\) 时刻，连续函数 \\(\\beta(t), x(t), \\epsilon(t)\\) 都等于之前的离散值，即： \\[\\beta(\\frac{i}{N}) = \\bar{\\beta}_i, x(\\frac{i}{N}) = x_i, \\epsilon(\\frac{i}{N})=\\epsilon_i \\] 在 \\(t \\in \\{0, 1, ..., \\frac{N-1}{N}\\}\\) ​以及 \\(\\Delta t=\\frac{1}{N}\\) 的情况下，我们就可以用连续函数改写之前的式子： \\[\\begin{align} x(t+ \\Delta t) &amp;= \\sqrt{1-\\beta(t+\\Delta t)\\Delta t}\\ x(t) + \\sqrt{\\beta(t+\\Delta t)\\Delta t}\\ \\epsilon(t) \\\\ &amp; \\approx x(t) - \\frac{1}{2}\\beta(t+\\Delta t) \\Delta t\\ x(t) + \\sqrt{\\beta(t+\\Delta t)\\Delta t}\\ \\epsilon(t) \\\\ &amp; \\approx x(t) - \\frac{1}{2}\\beta(t)\\Delta t\\ x(t) + \\sqrt{\\beta(t)\\Delta t}\\ \\epsilon(t) \\end{align} \\] 上面的近似只有在 \\(\\Delta t \\ll 1\\) 时成立。我们将其再移项后就可以得到下式： \\[x(t+\\Delta t) - x(t) \\approx -\\frac{1}{2} \\beta(t)\\Delta t\\ x(t) + \\sqrt{\\beta(t)\\Delta t}\\ \\epsilon(t) \\\\ \\mathrm{d} x = -\\frac{1}{2}\\beta(t)x \\mathrm{d}t + \\sqrt{\\beta(t)} \\mathrm{d}w \\] 其中， \\(w\\) ​表示的就是Wiener Process。这里面的第二个式子，就是一SDE方程。 至此，我们证明了DDPM连续化之后，就可以得到一个SDE方程，并且它是一种Variance Preserving的SDE。Variance Preserving的含义是当 \\(t \\to \\infty\\) 时，它的方差依然有界。 与此反向过程也是一个SDE方程，称为reverse SDE： \\[\\text{d}\\mathbf{x}= [\\mathbf{f}(\\mathbf{x}, t) - g^2(t)\\nabla _{\\mathbf{x} }\\log p(\\mathbf{x})]\\text{d}\\mathbf{t} + g(t)\\text{d}\\mathbf{w}\\] 这个反向过程中的未知量就只有分数函数 $ x p{t}(x)$ ​。至此，DDPM和分数模型也产生了联系，实际上二者之间是相互等价的。而DDPM和分数模型本质上都是在学习这个reverse SDE的解。 我们可以看到，DDPM每一步的去噪其实本质上与Annealed Langevin dynamics是一模一样的。 DDIM与ODE 首先对于一个SDE， \\[\\text{d}\\mathbf{x}= \\mathbf{f}(\\mathbf{x}, t)\\text{d}\\mathbf{t} + g(t)\\text{d}\\mathbf{w}\\] 我们写出它的福克-普朗克方程（Fokker-Planck equation）： \\[\\begin{align*} \\nabla _{t}p(\\mathbf{x}, t) &amp;= -\\nabla _{\\mathbf{x} }[\\mathbf{f}(\\mathbf{x}, t)p(\\mathbf{x}, t)] + \\frac{1}{2}g^{2}(t)\\nabla _{\\mathbf{x} }^{2}p(\\mathbf{x}, t)\\\\ &amp;= -\\nabla _{\\mathbf{x} }[\\mathbf{f}(\\mathbf{x}, t)p(\\mathbf{x}, t) - \\frac{1}{2}(g^{2}(t) - \\sigma^{2}(t))\\nabla_\\mathbf{x}p(\\mathbf{x}, t)] + \\frac{1}{2}\\sigma^{2}(t)\\nabla _{\\mathbf{x} }^{2}p(\\mathbf{x}, t)\\\\ &amp;= -\\nabla _{\\mathbf{x} }[(\\mathbf{f}(\\mathbf{x}, t) - \\frac{1}{2}(g^{2}(t) - \\sigma^{2}(t))\\nabla_\\mathbf{x}\\log p(\\mathbf{x}, t))p(\\mathbf{x})] + \\frac{1}{2}\\sigma^{2}(t)\\nabla _{\\mathbf{x} }^{2}p(\\mathbf{x}, t)\\\\\\end{align*}\\] 现在我们把福克-普朗克方程变成了这样： \\[\\nabla_{t}p(\\mathbf{x}, t) = -\\nabla_{\\mathbf{x} }[(\\mathbf{f}(\\mathbf{x}, t) - \\frac{1}{2}(g^{2}(t) - \\sigma^{2}(t))\\nabla_\\mathbf{x}\\log p(\\mathbf{x}, t))p(\\mathbf{x})] + \\frac{1}{2}\\sigma^{2}(t)\\nabla _{\\mathbf{x} }^{2}p(\\mathbf{x}, t)\\] 其对应的SDE为： \\[\\text{d}\\mathbf{x}= [\\mathbf{f}(\\mathbf{x}, t) - \\frac{1}{2}(g^{2}(t) - \\sigma^{2}(t))\\nabla_{\\mathbf{x} }\\log p_{t}(\\mathbf{x})]\\text{d}\\mathbf{t} + \\sigma(t)\\text{d}\\mathbf{w}\\] 因为前后两个SDE是等价的，他们对应的 \\(p_{t}(\\mathbf{x})\\) 是一样的，意味着我们可以改变第二个SDE的方差 \\(\\sigma(t)\\) 。当我们取 \\(\\sigma(t)=0\\) ，可以得到一个常微分方程(Ordinary Differential Equation, ODE), \\[\\text{d}\\mathbf{x}= [\\mathbf{f}(\\mathbf{x}, t) - \\frac{1}{2}g^{2}(t)\\nabla_ {\\mathbf{x} }\\log p_{t}(\\mathbf{x})]\\text{d}\\mathbf{t}\\] 这个结论有什么作用呢？首先，我们其实更在乎的是边缘概率分布 \\(q_t(x)\\) ，因为我们需要保证它在足够长的时刻 \\(T\\) ， \\(q_T(x)\\) 可以变成一个纯噪声，同时我们还需要 \\(q_0(x)\\) ​符合原始数据分布。上述结论可以保证这一点。同时，扩散模型本质上是在学习一个扩散过程的逆过程，既然前向SDE存在一个对应的ODE，那么反向过程reverse SDE其实也有一个对应的ODE，这个反向过程对应的ODE形式也是上面的式子。 而 DDIM 恰是一种确定性情形，所以我们自然会想到——能不能用 ODE 来描述一个 DDIM 呢？答案是肯定的。DDIM的公式如下： \\[\\begin{align} x_{t-1}&amp;=\\sqrt{\\bar\\alpha_{t-1} }x_\\theta(x_t,t)+\\sqrt{1-\\bar\\alpha_{t-1} }\\epsilon_\\theta(x_t,t)\\\\ &amp;=\\frac{\\sqrt{\\bar\\alpha_{t-1} }}{\\sqrt{\\bar\\alpha_t} }\\left(x_t-\\sqrt{1-\\bar\\alpha_t}\\epsilon_\\theta(x_t,t)\\right)+\\sqrt{1-\\bar\\alpha_{t-1} }\\epsilon_\\theta(x_t,t) \\end{align}\\] 两边均减去 \\(x_t\\) ，得： \\[\\begin{align} x_{t-1}-x_t&amp;=\\frac{1}{\\sqrt{\\bar\\alpha_t} }\\left[\\left(\\sqrt{\\bar\\alpha_{t-1} }-\\sqrt{\\bar\\alpha_t}\\right)x_t-\\left(\\sqrt{\\bar\\alpha_{t-1}(1-\\bar\\alpha_t)}-\\sqrt{\\bar\\alpha_t(1-\\bar\\alpha_{t-1})}\\right)\\epsilon_\\theta(\\mathbf x_t,t)\\right]\\\\ &amp;=\\frac{1}{\\sqrt{\\bar\\alpha_t} }\\left(\\frac{\\bar\\alpha_{t-1}-\\bar\\alpha_t}{\\sqrt{\\bar\\alpha_{t-1} }+\\sqrt{\\bar\\alpha_t} }x_t-\\frac{\\bar\\alpha_{t-1}-\\bar\\alpha_t}{\\sqrt{\\bar\\alpha_{t-1}(1-\\bar\\alpha_t)}+\\sqrt{\\bar\\alpha_t(1-\\bar\\alpha_{t-1})} }\\epsilon_\\theta(x_t,t)\\right)\\\\ &amp;=\\frac{\\bar\\alpha_{t-1}-\\bar\\alpha_t}{\\sqrt{\\bar\\alpha_t} }\\left(\\frac{x_t}{\\sqrt{\\bar\\alpha_{t-1} }+\\sqrt{\\bar\\alpha_t} }-\\frac{\\epsilon_\\theta(\\mathbf x_t,t)}{\\sqrt{\\bar\\alpha_{t-1}(1-\\bar\\alpha_t)}+\\sqrt{\\bar\\alpha_t(1-\\bar\\alpha_{t-1})} }\\right) \\end{align}\\] 记 \\(x(t)=x_t,\\barα(t)=\\barα_t\\) ，将 \\(t-1\\) 换成 \\(t−Δt\\) 并令 \\(Δt→0\\) ，得： \\[\\mathrm dx=\\frac{\\mathrm d\\bar\\alpha(t)}{\\sqrt{\\bar\\alpha(t)} }\\left(\\frac{x(t)}{2\\sqrt{\\bar\\alpha(t)} }-\\frac{\\epsilon_\\theta(x(t),t)}{2\\sqrt{\\bar\\alpha(t)(1-\\bar\\alpha(t))} }\\right)=\\frac{\\bar\\alpha&#39;(t)}{2\\bar\\alpha(t)}\\left(x(t)-\\frac{\\epsilon_\\theta(x(t),t)}{\\sqrt{1-\\bar\\alpha(t)} }\\right)\\mathrm dt \\] 这就是 DDIM 的 ODE 描述。 在 DDPM 的设置下，有 $f(x,t)=−β(t)x,g(t)= $ ，代入 \\[\\text{d}\\mathbf{x}= [\\mathbf{f}(\\mathbf{x}, t) - \\frac{1}{2}g^{2}(t)\\nabla_ {\\mathbf{x} }\\log p_{t}(\\mathbf{x})]\\text{d}\\mathbf{t}\\] 得： \\[\\mathrm dx=\\left[-\\frac{1}{2}\\beta(t)x-\\frac{1}{2}\\beta(t)\\nabla_{\\mathbf{x} }\\log p_{t}(\\mathbf{x})\\right]\\mathrm dt=-\\frac{1}{2}\\beta(t)\\left[x+\\nabla_{\\mathbf{x} }\\log p_{t}(\\mathbf{x})\\right]\\mathrm dt\\] 与我们上面的式子对应。 既然引入了ODE，那么我们的模型就可以去学习如何解这个ODE，同时也可以引入各种传统的ODE solver例如：Euler method, Runge–Kutta method等一些方法。这就是为什么我们可以看到像Stable Diffusion之类的模型会有那么多sampler的原因，本质上都是一些ODE solver和SDE solver。但是后面的研究者发现，传统的ODE solver在采样效果上比不过DDIM，这就非常奇怪了。DPM-Solver的作者在他们的论文中给出了原因：DDIM充分利用了diffusion ODE的半线性结构（semi-linear structure），并且它是一个semi-linear ODE的一阶Solver，而传统的ODE solver并没有利用好这个半线性结构，因此DDIM的准确度会更高一些，因此采样效果也更好。 这里还需要注意的点是，diffusion ODE这类模型相比diffusion SDE存在着诸多好处，比如： 没有随机性，ODE是一个确定性过程，可以以更快的速度收敛，因此可以达到更快的采样速度 由于是确定性过程，可以计算数据似然（likelihood）等。","categories":[{"name":"Stanford CS236深度生成模型","slug":"Stanford-CS236深度生成模型","permalink":"https://jia040223.github.io/categories/Stanford-CS236%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}],"tags":[{"name":"生成模型","slug":"生成模型","permalink":"https://jia040223.github.io/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}]},{"title":"Score Based Models","slug":"Score Based Models","date":"2024-09-24T12:40:58.000Z","updated":"2024-09-25T10:50:29.370Z","comments":true,"path":"2024/09/24/Score Based Models/","permalink":"https://jia040223.github.io/2024/09/24/Score%20Based%20Models/","excerpt":"","text":"本学习笔记用于记录我学习Stanford CS236课程的学习笔记，分享记录，也便于自己实时查看。 引入 Score function 上一次我们学习了Energy Based Model。其核心做法是对一个数据集 \\({x_{1}, x_{2}, ..., x_{N}}\\) ，我们把数据的概率分布 \\(p(x)\\) 建模为： \\[p_{\\theta}(\\mathbf{x}) = \\frac{e^{-f_{\\theta}(\\mathbf{x})}}{Z_{\\theta}}\\] 这里 \\(f_{\\theta}(\\mathbf{x})\\in \\mathbb{R}\\) 。 \\(Z_{\\theta}\\) 是归一化项保证 \\(p_{\\theta}(\\mathbf{x})\\) 是概率。 \\(\\theta\\) 是他们的参数。 我们一般可以通过最大似然估计的方式来训练参数 \\(\\theta\\) ， \\[\\max_{\\theta}\\sum\\limits_{i=1}^{N}\\log_{\\theta}(\\mathbf{x}_{i})\\] 但是因为 \\[\\log p_{\\theta}(\\mathbf{x}) = -f_{\\theta}(\\mathbf{x}) - \\log Z_{\\theta}\\] \\(Z_{\\theta}\\) 是intractable的，我们无法求出 \\(\\log p_{\\theta}(\\mathbf{x})\\) ，自然也就无法优化参数 \\(\\theta\\) 。 为了解决归一化项无法计算的问题，我们引入score function。 score function的定义为 \\(\\nabla _{\\mathbf{x}}\\log p(\\mathbf{x})\\) 所以我们可以发现，score function是与 \\(Z _{\\theta}\\) 无关的： \\[\\mathbf{s}_{\\theta}(\\mathbf{x}) = \\nabla_{\\mathbf{x}}\\log(\\mathbf{x}_{\\theta}) = -\\nabla_{\\mathbf{x}}f_{\\theta}(\\mathbf{x}) - \\nabla_{\\mathbf{x}}\\log Z_{\\theta} = -\\nabla_{\\mathbf{x}}f _{\\theta}(\\mathbf{x})\\] Score Based Model Score matching 现在我们想要训练一个网络来估计出真实的score function。自然地，我们可以最小化真实的score function和网络输出的MSE： \\[\\mathcal{L} =\\frac{1}{2} \\mathbb{E}_{p(\\mathbf{x})}[||\\nabla_{\\mathbf{x}}\\log p(\\mathbf{x}) - \\mathbf{s} _{\\theta}(\\mathbf{x})||^{2}]\\] 但是这样的一个loss我们是算不出来的，因为我们并不知道真实的 \\(p(\\mathbf{x})\\) 是什么。而score matching方法就可以让我们在不知道真实的 \\(p(\\mathbf{x})\\) 的情况下最小化这个loss。Score matching的推导如下： 我们把上面loss的期望写开，二次项打开，可以得到 \\[\\begin{align*}\\mathcal{L} =&amp; \\frac{1}{2}\\mathbb{E}_{p(\\mathbf{x})}[||\\nabla _{\\mathbf{x}}\\log p(\\mathbf{x}) - \\mathbf{s} _{\\theta}(\\mathbf{x})||^{2}]\\\\=&amp; \\frac{1}{2}\\int p(\\mathbf{x}) [||\\nabla _{\\mathbf{x}}\\log p(\\mathbf{x})||^{2} + ||\\mathbf{s} _{\\theta}(\\mathbf{x})||^{2} - 2(\\nabla _{\\mathbf{x}}\\log p(\\mathbf{x}))^{T}\\mathbf{s} _{\\theta}(\\mathbf{x})] d \\mathbf{x}\\end{align*}\\] 第一项对于 \\(\\theta\\) 来说是常数可以忽略。 第二项为 \\[\\int p(\\mathbf{x}) ||\\mathbf{s} _{\\theta}(\\mathbf{x})||^{2} d \\mathbf{x}\\] 对于第三项，若 \\(\\mathbf{x}\\) 的维度为 \\(N\\) ： \\[ \\begin{align*}&amp; -2\\int p(\\mathbf{x}) (\\nabla _{\\mathbf{x}}\\log p(\\mathbf{x}))^{T}\\mathbf{s} _{\\theta}(\\mathbf{x}) d \\mathbf{x}\\\\ =&amp; -2 \\int p(\\mathbf{x}) \\sum\\limits_{i=1}^{N}\\frac{\\partial \\log p(\\mathbf{x})}{\\partial \\mathbf{x}_{i}}\\mathbf{s}_{\\theta i}(\\mathbf{x}) d \\mathbf{x}\\\\ =&amp; -2 \\sum\\limits_{i=1}^{N} \\int p(\\mathbf{x}) \\frac{1}{p(\\mathbf{x})} \\frac{\\partial p(\\mathbf{x})}{\\partial \\mathbf{x}_{i}}\\mathbf{s}_{\\theta i}(\\mathbf{x}) d \\mathbf{x}\\\\ =&amp; -2 \\sum\\limits_{i=1}^{N} \\int \\frac{\\partial p(\\mathbf{x})}{\\partial \\mathbf{x}_{i}}\\mathbf{s}_{\\theta i}(\\mathbf{x}) d \\mathbf{x}\\\\ =&amp; 2 \\sum\\limits_{i=1}^{N} - \\int \\frac{\\partial p(\\mathbf{x})\\mathbf{s}_{\\theta i}(\\mathbf{x})}{\\partial \\mathbf{x}_{i}} d \\mathbf{x} + \\int p(\\mathbf{x}) \\frac{\\partial \\mathbf{s}_{\\theta i}(\\mathbf{x})}{\\partial \\mathbf{x}_{i}} d \\mathbf{x}\\\\ =&amp; 2 \\sum\\limits_{i=1}^{N} - \\int p(\\mathbf{x})\\mathbf{s}_{\\theta i}(\\mathbf{x})\\bigg\\rvert^{\\infty}_{-\\infty} d \\mathbf{x_{/i}} + \\int p(\\mathbf{x}) \\frac{\\partial \\mathbf{s}_{\\theta i}(\\mathbf{x})}{\\partial \\mathbf{x}_{i}} d \\mathbf{x}\\\\ =&amp; 2 \\sum\\limits_{i=1}^{N} \\int p(\\mathbf{x}) \\frac{\\partial \\mathbf{s}_{\\theta i}(\\mathbf{x})}{\\partial \\mathbf{x}_{i}} d \\mathbf{x}\\\\ =&amp; 2\\int p(\\mathbf{x}) \\sum\\limits_{i=1}^{N} \\frac{\\partial \\mathbf{s}_{\\theta i}(\\mathbf{x})}{\\partial \\mathbf{x}_{i}} d \\mathbf{x}\\\\ =&amp; 2\\int p(\\mathbf{x}) \\text{tr}(\\nabla _{\\mathbf{x}}\\mathbf{s}_{\\theta}(\\mathbf{x})) d \\mathbf{x}\\end{align*} \\] 所以最后的loss是第二和第三项的和： \\[ \\begin{align*} \\mathcal{L} &amp;=\\frac{1}{2} \\int p(\\mathbf{x}) ||\\mathbf{s} _{\\theta}(\\mathbf{x})||^{2} d \\mathbf{x} + \\int p(\\mathbf{x}) \\text{tr}(\\nabla _{\\mathbf{x}}\\mathbf{s}_{\\theta}(\\mathbf{x})) d \\mathbf{x}\\\\\\\\ &amp;= \\mathbb{E}_{p(\\mathbf{x})}[\\frac{1}{2}||\\mathbf{s} _{\\theta}(\\mathbf{x})||^{2} + \\text{tr}(\\nabla _{\\mathbf{x}}\\mathbf{s}_{\\theta}(\\mathbf{x}))]\\end{align*} \\] 当然，这个推导虽然是从能量模型引入的，但并不局限于能量模型，事实上，他是一个更大的模型家族。 Score Matching Langevin Dynamics (SMLD) 现在我们已经通过神经网络学习到了数据分布的score function，那么如何用score function从这个数据分布中得到样本呢？答案就是朗之万动力学采样(Langevin Dynamics): \\[ \\mathbf{x}_{i+1} = \\mathbf{x}_{i} + \\epsilon \\nabla_{\\mathbf{x}}\\log p(\\mathbf{x}) + \\sqrt{2 \\epsilon}\\mathbf{z}_{i}, \\quad \\mathbf{z} _{i} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}), \\quad i=0,1,\\cdots K\\ \\] 这里的采样是一个迭代的过程。 \\(\\epsilon\\) 是一个很小的量。 \\(\\mathbf{x}_{0}\\) 随机初始，通过上面的迭代式更新。当迭代次数 \\(K\\) 足够大的时候， \\(\\mathbf{x}\\) 就收敛于该分布的一个样本。 上图的具体解释我就不再赘述了。 这样我们其实就得到了一个生成模型。我们可以先训练一个网络用来估计score function，然后用Langevin Dynamics和网络估计的score function采样，就可以得到原分布的样本。因为整个方法由score matching和Langevin Dynamics两部分组成，所以叫SMLD。 训练 说完了损失函数和采样过程，那么对这个模型我们怎么训练呢？相信敏锐的读者已经注意到了，我们损失函数： \\[ \\begin{align*} \\mathcal{L} &amp;= \\mathbb{E}_{p(\\mathbf{x})}[\\frac{1}{2}||\\mathbf{s} _{\\theta}(\\mathbf{x})||^{2} + \\text{tr}(\\nabla _{\\mathbf{x}}\\mathbf{s}_{\\theta}(\\mathbf{x}))]\\end{align*} \\] 这个第二项并不是很好计算。对于维度为 \\(N\\) 的数据，我们计算雅可比矩阵的迹需要进行 \\(N\\) 次反向传播，这对于高维度的数据的训练是不能接受的。 对于这个问题，主要有两种解决方法。 Denoising score matching Denoising score matching的做法就是在 score matching 的基础上，对输入数据加噪。需要注意的是，此时的 score 是对加噪后的数据进行求导，而非原输入数据。score 的方向是(对数)概率密度增长最快的方向，也就是最接近真实数据的方向。 Denoising score matching 的玩法是：在给定输入 \\(x\\) 的情况下，将条件分布$ q(|x)$建模为高斯分布，其中 \\(\\tilde{x}\\) 代表加噪后的数据，并且边缘化这个条件分布，以 \\(p(\\tilde{x}) \\equiv \\int q(\\tilde{x}|x)p(x) dx\\) 来近似原数据分布，因此噪声强度不太大时，我们可以认为加噪后数据的概率分布与原数据的概率分布大致相同。 此时，score \\(\\frac{\\partial log(p(\\tilde{x}))}{\\partial \\tilde{x}}\\) 中由于 $ p(x)$ 项在求导时与 \\(\\tilde{x}\\) 无关，可以略去了，具体推导如下： \\[ \\begin{align*} \\frac{1}{2} \\mathbb{E}_{\\tilde{x} \\sim q_{\\sigma}} \\left[ \\| \\nabla_{\\tilde{x}} \\log q_{\\sigma}(\\tilde{x}) - s_{\\theta}(\\tilde{x}) \\|_2^2 \\right] &amp;= \\frac{1}{2} \\int q_{\\sigma}(\\tilde{x}) \\| \\nabla_{\\tilde{x}} \\log q_{\\sigma}(\\tilde{x}) - s_{\\theta}(\\tilde{x}) \\|_2^2 d\\tilde{x} \\\\ &amp;= \\frac{1}{2} \\int q_{\\sigma}(\\tilde{x}) \\| \\nabla_{\\tilde{x}} \\log q_{\\sigma}(\\tilde{x}) \\|_2^2 d\\tilde{x} + \\frac{1}{2} \\int q_{\\sigma}(\\tilde{x}) \\| s_{\\theta}(\\tilde{x}) \\|_2^2 d\\tilde{x}- \\int q_{\\sigma}(\\tilde{x}) \\nabla_{\\tilde{x}} \\log q_{\\sigma}(\\tilde{x})^T s_{\\theta}(\\tilde{x}) d\\tilde{x} \\end{align*} \\] 这里一样的，第一项是常数，第二项只涉及 \\(s_{\\theta}(\\tilde{x})\\) ，我们可以处理，第三项比较棘手。但我们可以类似地用分布积分法进行处理： \\[ \\begin{align*} &amp;- \\int q_{\\sigma}(\\tilde{x}) \\nabla_{\\tilde{x}} \\log q_{\\sigma}(\\tilde{x})^T s_{\\theta}(\\tilde{x}) d\\tilde{x} \\\\ &amp;= - \\int q_{\\sigma}(\\tilde{x}) \\frac{1}{q_{\\sigma}(\\tilde{x})} \\nabla_{\\tilde{x}} q_{\\sigma}(\\tilde{x})^T s_{\\theta}(\\tilde{x}) d\\tilde{x} \\\\ &amp;= - \\int \\nabla_{\\tilde{x}} q_{\\sigma}(\\tilde{x})^T s_{\\theta}(\\tilde{x}) d\\tilde{x} \\\\ &amp;= - \\int \\nabla_{\\tilde{x}} \\left( \\int p_{\\text{data}}(x) q_{\\sigma}(\\tilde{x} | x) dx \\right)^T s_{\\theta}(\\tilde{x}) d\\tilde{x} \\\\ &amp;= - \\int \\left( \\int p_{\\text{data}}(x) \\nabla_{\\tilde{x}} q_{\\sigma}(\\tilde{x} | x) dx \\right)^T s_{\\theta}(\\tilde{x}) d\\tilde{x} \\\\ &amp;= - \\int \\left( \\int p_{\\text{data}}(x) q_{\\sigma}(\\tilde{x} | x) \\nabla_{\\tilde{x}} \\log q_{\\sigma}(\\tilde{x} | x) dx \\right)^T s_{\\theta}(\\tilde{x}) d\\tilde{x} \\\\ &amp;= - \\int \\int p_{\\text{data}}(x) q_{\\sigma}(\\tilde{x} | x) \\nabla_{\\tilde{x}} \\log q_{\\sigma}(\\tilde{x} | x)^Ts_{\\theta}(\\tilde{x}) dx \\ d\\tilde{x} \\end{align*} \\] 这里我们 \\(q(\\tilde{x}|x)\\) 是已知的，也就可以计算了。 OK，让我们代入原式之中： \\[ \\begin{align*} &amp;\\frac{1}{2} \\mathbb{E}_{\\tilde{\\mathbf{x}} \\sim q_{\\sigma}} \\left[ \\|\\nabla_{\\tilde{\\mathbf{x}}} \\log q_{\\sigma} (\\tilde{\\mathbf{x}}) - s_{\\theta} (\\tilde{\\mathbf{x}}) \\|_2^2 \\right] \\\\ &amp;= \\text{const.} + \\frac{1}{2} \\mathbb{E}_{\\mathbf{x} \\sim q_{\\sigma}} \\left[ \\| s_{\\theta} (\\mathbf{x}) \\|_2^2 \\right] - \\int q_{\\sigma} (\\tilde{\\mathbf{x}}) \\nabla_{\\tilde{\\mathbf{x}}} \\log q_{\\sigma} (\\tilde{\\mathbf{x}})^{\\top} s_{\\theta} (\\tilde{\\mathbf{x}}) d\\tilde{\\mathbf{x}} \\\\ &amp;= \\text{const.} + \\frac{1}{2} \\mathbb{E}_{\\mathbf{x} \\sim q_{\\sigma}} \\left[ \\| s_{\\theta} (\\tilde{\\mathbf{x}}) \\|_2^2 \\right] - \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}(\\mathbf{x}), \\tilde{\\mathbf{x}} \\sim q_{\\sigma}(\\tilde{\\mathbf{x}}|\\mathbf{x})} \\left[ \\nabla_{\\tilde{\\mathbf{x}}} \\log q_{\\sigma} (\\tilde{\\mathbf{x}}|\\mathbf{x})^{\\top} s_{\\theta} (\\tilde{\\mathbf{x}}) \\right] \\\\ &amp;= \\text{const.} + \\frac{1}{2} \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}(\\mathbf{x}), \\tilde{\\mathbf{x}} \\sim q_{\\sigma}(\\tilde{\\mathbf{x}}|\\mathbf{x})} \\left[ \\| s_{\\theta} (\\tilde{\\mathbf{x}}) - \\nabla_{\\tilde{\\mathbf{x}}} \\log q_{\\sigma} (\\tilde{\\mathbf{x}}|\\mathbf{x}) \\|_2^2 \\right] - \\frac{1}{2} \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}(\\mathbf{x}), \\tilde{\\mathbf{x}} \\sim q_{\\sigma}(\\tilde{\\mathbf{x}})} \\left[ \\| \\nabla_{\\tilde{\\mathbf{x}}} \\log q_{\\sigma} (\\tilde{\\mathbf{x}}) \\|_2^2 \\right] \\\\ &amp;= \\text{const.} + \\frac{1}{2} \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}(\\mathbf{x}), \\tilde{\\mathbf{x}} \\sim q_{\\sigma}(\\tilde{\\mathbf{x}}|\\mathbf{x})} \\left[ \\| s_{\\theta} (\\tilde{\\mathbf{x}}) - \\nabla_{\\tilde{\\mathbf{x}}} \\log q_{\\sigma} (\\tilde{\\mathbf{x}}|\\mathbf{x}) \\|_2^2 \\right] + \\text{const.} \\end{align*} \\] 看到没有！这也就是说，score 的方向与所加噪声的方向是相反的。 于是，在 denoising score matching 的体制下，朝着 score 的方向走，其实就是在去噪，在做 denoising。 在实践中，我们可以选择将 \\(q(\\tilde{x}|x)\\) 建模为 \\(N(\\tilde{x};x;\\sigma^2)\\) ，即均值为原数据 \\(x\\) ，方差为预设的 \\(\\sigma^2\\) 的高斯分布。于是，根据高斯分布的性质，有： \\[\\tilde{x}=x + \\sigma \\epsilon, \\epsilon\\sim N(0,I)\\] 其中， \\(\\epsilon\\) 是从标准高斯分布中采样出来的噪声。 接着，在以上化简出的 score 中代入高斯分布的概率密度函数，可以得到 score 为： \\[\\frac{\\partial log (q(\\tilde{x}|x))}{\\partial \\tilde{x}} = -(\\frac{\\tilde{x}-x}{\\sigma^2})=-\\frac{\\epsilon}{\\sigma}\\] 虽然我们对计算进行了大幅度简化，但这也导致了我们估计的是加噪数据的梯度。具体训练流程如下： Sliced score matching Sliced score matching的思想是，如果模型预测的梯度与真实梯度相同等价于他们在不同方向下的投影均相同，所以我们引入一个投影向量用于训练。这样我们的目标和最终化简（用分部积分即可）的格式如下： goal： \\[ \\frac{1}{2} \\mathbb{E}_{\\mathbf{v} \\sim p_v} \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}} \\left[ \\left( \\mathbf{v}^{\\top} \\nabla_{\\mathbf{x}} \\log p_{\\text{data}} (\\mathbf{x}) - \\mathbf{v}^{\\top} s_{\\theta} (\\mathbf{x}) \\right)^2 \\right] \\] loss： \\[\\mathbb{E}_{\\mathbf{v} \\sim p_v} \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}} \\left[ \\mathbf{v}^{\\top} \\nabla_{\\mathbf{x}} s_{\\theta} (\\mathbf{x}) \\mathbf{v} + \\frac{1}{2} (\\mathbf{v}^{\\top} s_{\\theta} (\\mathbf{x}))^2 \\right] \\] 这样我们便只需要进行一次反向传播了，大大减少了训练需要的计算量，计算图如下： 具体训练过程如下： 虽然这种方法的训练计算量会比Denoising score matching大，但它是对真实数据梯度进行的估计 问题 现在我们得到了SMLD生成模型，但实际上这个模型由很大的问题。首先看一下其在实践中的效果： 可以看到效果并不好。我们不妨从损失函数来分析一下原因： \\[ \\mathcal{L} = \\mathbb{E}_{p(\\mathbf{x})}[||\\nabla_{\\mathbf{x}}\\log p(\\mathbf{x}) - \\mathbf{s}_{\\theta}(\\mathbf{x})||^{2}] = \\int p(\\mathbf{x})||\\nabla_{\\mathbf{x}}\\log p(\\mathbf{x}) - \\mathbf{s} _{\\theta}(\\mathbf{x})||^{2} d \\mathbf{x}\\ \\] 观察我们用来训练神经网络的损失函数，我们可以发现这个L2项其实是被 \\(p(\\mathbf{x})\\) 加权了。所以对于低概率的区域，估计出来的score function就很不准确： 对于上面这张图来说，只有在高概率的红色区域，loss才高，score function可以被准确地估计出来。但如果我们采样的初始点在低概率区域的话，因为估计出的score function不准确，很有可能生成不出真实分布的样本。 此外，在现实中，比如对于图片来说，其往往是分布在一个低维度流型上，也就是大部分空间的概率密度几乎为0，此时我们的梯度定义已经失去了意义： 同时，我们通过Langevin Dynamics进行采样并不能很好还原聚点的样本比： SMLD的改进 那怎么样才能解决上面的问题呢？Denoising score matching给我们给了一定的启发。 其实可以通过给数据增加噪声扰动的方式扩大高概率区域的面积。给原始分布加上高斯噪声，原始分布的方差会变大。这样相当于高概率区域的面积就增大了，更多区域的score function可以被准确地估计出来。 但是噪声扰动的强度如何控制是个问题： 强度太小起不到效果，高概率区域的面积还是太小 强度太大会破坏数据的原始分布，估计出来的score function就和原分布关系不大了 所以噪声强度越高，高概率区域面积越大，训练得到的梯度越准，但与原始数据的梯度差距也就越大。所以我们不妨加不同程度的噪声，让网络可以学到加了不同噪声的原始分布的score function。这样既保证了原始低概率密度地区能学习到有效的梯度，同时原始高概率密度区的梯度估计是准确的。 说起来很拗口，其实很好理解。我们定义序列 \\({\\sigma_{1 \\sim L}} , \\quad \\sigma {1} \\lt \\sigma {2} \\lt \\cdots \\lt \\sigma _{L}\\) ，代表从小到大的噪声强度。这样我们可以定义经过噪声扰动之后的数据样本，服从一个经过噪声扰动之后的分布， \\[ \\mathbf{x} + \\sigma_{i}\\mathbf{z} = \\int p(\\mathbf{y}) \\mathcal{N}(\\mathbf{x}|\\mathbf{y}, \\sigma {i}^{2}\\mathbf{I})d \\mathbf{y}\\ \\] 我们用神经网络来估计经过噪声扰动过的分布的score function，并把噪声强度 \\(\\sigma_i\\) 作为一个输入： \\[ \\mathcal{L} = \\frac{1}{L}\\sum_\\limits {i=1}^{L} \\lambda (i) \\mathbb{E}_{p _{\\sigma {i}}(\\mathbf{x})}[||\\nabla_{\\mathbf{x}}\\log p_{\\sigma _ {i}}(\\mathbf{x}) - \\mathbf{s} _{\\theta}(\\mathbf{x, \\sigma_i})||^{2}] \\] 其中 \\(\\lambda(i)\\) 是权重，在实践中可以取 \\(\\sigma_{i}^{2}\\) 采样方式也要做出相应的变化，我们对于不同的噪声强度 \\(L, L-1, \\cdots, 1\\) 做Langevin采样，上一个scale的结果作为这一次的初始化。这样我们每一次的初始化都能在梯度估计的有效区域。 这种采样方式也叫做Annealed Langevin dynamics，具体训练流程如下： 从离散到连续 当我们做Langevin dynamics迭代次数足够多时，我们可以用随机微分方程(Stochastic Differential Equation, SDE)来建模这个采样过程。 \\[\\mathbf{x}_{i+1} = \\mathbf{x}_{i} + \\epsilon \\nabla_{\\mathbf{x}}\\log p(\\mathbf{x}_i) + \\sqrt{2 \\epsilon}\\mathbf{z}_{i}, \\quad i=0,1,\\cdots K\\] 当 \\(K\\to\\infty\\) 时，我们定义 \\(\\Delta t = \\epsilon,\\; \\Delta t \\to 0\\) \\[\\mathbf{x}_{t+\\Delta t} - \\mathbf{x}_{t}= \\nabla_{\\mathbf{x}}\\log p(\\mathbf{x}_i)\\Delta t + \\sqrt{2 \\Delta t}\\mathbf{z}_{i}\\] 我们将 \\(\\nabla _{\\mathbf{x}}\\log p(\\mathbf{x}_i)\\) 和 \\(\\sqrt{2}\\) 一般化为 \\(\\mathbf{f}(\\mathbf{x}, t)\\) 和 \\(g(t)\\) ，这样上面就变成了 \\[\\mathbf{x} _{t+\\Delta t} - \\mathbf{x}_{t}= \\mathbf{f}(\\mathbf{x}, t)\\Delta t + g(t) \\sqrt{\\Delta t}\\mathbf{z} _{i}\\] 其中 \\[\\sqrt{\\Delta t}\\mathbf{z} _{i} \\sim \\mathcal{N}(\\mathbf{0}, \\Delta t\\mathbf{I})\\] 这里可以引入布朗运动，如果我们定义 \\(\\mathbf{w}\\) 是一个布朗运动，那么 \\[ \\begin{gather*}\\mathbf{w}_{t+\\Delta t} = \\mathbf{w}_{t} + \\mathcal{N}(\\mathbf{0}, \\Delta t\\mathbf{I}),\\\\ \\sqrt{\\Delta t}\\mathbf{z} _{i} = \\mathbf{w}_{t+\\Delta t} - \\mathbf{w}_{t}.\\end{gather*} \\] 讲布朗运动带入到上面，得到 \\[\\mathbf{x}_{t+\\Delta t} - \\mathbf{x}_{t}= \\mathbf{f}(\\mathbf{x}, t)\\Delta t + g(t)(\\mathbf{w}_{t+\\Delta t} - \\mathbf{w}_{t})\\] 当 \\(\\Delta t \\to 0\\) , \\[\\text{d}\\mathbf{x}= \\mathbf{f}(\\mathbf{x}, t)\\text{d}\\mathbf{t} + g(t)\\text{d}\\mathbf{w}\\] 这里 \\(\\mathbf{f}(\\mathbf{x}, t)\\) 叫做drift coefficient, \\(g(t)\\) 代表diffusion coefficient。SDE的解也就代表了数据不断加噪声的过程。 有了正向过程的SDE，我们可以得到 反向的SDE \\[\\text{d}\\mathbf{x}= [\\mathbf{f}(\\mathbf{x}, t) - g^2(t)\\nabla _{\\mathbf{x}}\\log p(\\mathbf{x})]\\text{d}\\mathbf{t} + g(t)\\text{d}\\mathbf{w}\\] 以及score matching的损失函数 \\[\\mathbb{E}_{t\\in \\mathcal{U}(0, T)} \\mathbb{E}_{p_{t}(\\mathbf{x})}[g^2(t)||\\nabla_{\\mathbf{x}}\\log p_t(\\mathbf{x}) - \\mathbf{s}_{\\theta}(\\mathbf{x})||^2]\\] 可以看到，当我们知道了score后，就能解这个反向的SDE了。 整个基于SDE框架就是：我们在正向过程在图像中加噪声训练神经网络做score matching，估计出score function。然后在反向过程中从高斯噪声通过逆向SDE过程生成出数据分布的样本。 从SDE到ODE 对于一个SDE， \\[\\text{d}\\mathbf{x}= \\mathbf{f}(\\mathbf{x}, t)\\text{d}\\mathbf{t} + g(t)\\text{d}\\mathbf{w}\\] 我们写出它的福克-普朗克方程（Fokker-Planck equation）： \\[ \\begin{align*} \\nabla _{t}p(\\mathbf{x}, t) &amp;= -\\nabla _{\\mathbf{x}}[\\mathbf{f}(\\mathbf{x}, t)p(\\mathbf{x}, t)] + \\frac{1}{2}g^{2}(t)\\nabla _{\\mathbf{x}}^{2}p(\\mathbf{x}, t)\\\\ &amp;= -\\nabla _{\\mathbf{x}}[\\mathbf{f}(\\mathbf{x}, t)p(\\mathbf{x}, t) - \\frac{1}{2}(g^{2}(t) - \\sigma^{2}(t))\\nabla_\\mathbf{x}p(\\mathbf{x}, t)] + \\frac{1}{2}\\sigma^{2}(t)\\nabla _{\\mathbf{x}}^{2}p(\\mathbf{x}, t)\\\\ &amp;= -\\nabla _{\\mathbf{x}}[(\\mathbf{f}(\\mathbf{x}, t) - \\frac{1}{2}(g^{2}(t) - \\sigma^{2}(t))\\nabla_\\mathbf{x}\\log p(\\mathbf{x}, t))p(\\mathbf{x})] + \\frac{1}{2}\\sigma^{2}(t)\\nabla _{\\mathbf{x}}^{2}p(\\mathbf{x}, t)\\\\\\end{align*} \\] 现在我们把福克-普朗克方程变成了这样： \\[ \\nabla_{t}p(\\mathbf{x}, t) = -\\nabla_{\\mathbf{x}}[(\\mathbf{f}(\\mathbf{x}, t) - \\frac{1}{2}(g^{2}(t) - \\sigma^{2}(t))\\nabla_\\mathbf{x}\\log p(\\mathbf{x}, t))p(\\mathbf{x})] + \\frac{1}{2}\\sigma^{2}(t)\\nabla _{\\mathbf{x}}^{2}p(\\mathbf{x}, t) \\] 其对应的SDE为： \\[ \\text{d}\\mathbf{x}= [\\mathbf{f}(\\mathbf{x}, t) - \\frac{1}{2}(g^{2}(t) - \\sigma^{2}(t))\\nabla_{\\mathbf{x}}\\log p_{t}(\\mathbf{x})]\\text{d}\\mathbf{t} + \\sigma(t)\\text{d}\\mathbf{w} \\] 因为前后两个SDE是等价的，他们对应的 \\(p_{t}(\\mathbf{x})\\) 是一样的，意味着我们可以改变第二个SDE的方差 \\(\\sigma(t)\\) 。当我们取 \\(\\sigma(t)=0\\) ，可以得到一个常微分方程(Ordinary Differential Equation, ODE), \\[\\text{d}\\mathbf{x}= [\\mathbf{f}(\\mathbf{x}, t) - \\frac{1}{2}g^{2}(t)\\nabla_{\\mathbf{x}}\\log p_{t}(\\mathbf{x})]\\text{d}\\mathbf{t}\\] 下图就展示了SDE和ODE解的过程，可以看到ODE的轨迹是确定光滑的，而SDE的轨迹是随机的。这两个过程中的任意边缘分布 \\({p_{t}(\\mathbf{x})}_{t\\in[0, T]}\\) 都是一样的。 ODE形式有它的优点在于： 因为ODE比SDE好解，所以ODE的采样速度更快。 因为ODE是不带随机噪声的，整个过程是确定的，是可逆的，所以这个ODE也可以看做Normalizing flows，可以用来估计概率密度和似然。 但同时由于没有了随机噪声，可能导致多样性更差，实践中生成效果也不如SDE。","categories":[{"name":"Stanford CS236深度生成模型","slug":"Stanford-CS236深度生成模型","permalink":"https://jia040223.github.io/categories/Stanford-CS236%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}],"tags":[{"name":"生成模型","slug":"生成模型","permalink":"https://jia040223.github.io/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}]},{"title":"Energy Based Models","slug":"Energy Based Models","date":"2024-09-20T11:46:30.000Z","updated":"2024-09-25T11:18:58.626Z","comments":true,"path":"2024/09/20/Energy Based Models/","permalink":"https://jia040223.github.io/2024/09/20/Energy%20Based%20Models/","excerpt":"","text":"本学习笔记用于记录我学习Stanford CS236课程的学习笔记，分享记录，也便于自己实时查看。 引入 生成模型的核心目标是对目标样本的概率分布进行预测。而对于一个概率密度函数 \\(P(x)\\) ，它只需要满足下面两个条件： 非负， \\(P(x)\\) 在任何一个点都不能小于0，这很显然。 积分为1， \\(P(x)\\) 从负无穷积分到正无穷得是1。 其中对于第二点，如果 \\(P(x)\\) 的不是1，是 \\(Z\\) ，我们进行一下归一化，除一下 \\(Z\\) 就是1啦。反正至少得是有限的。那么如果我们有一个函数 \\(f(x)\\) ，我们只需要对其进行变换，满足上面两个特点，便能将其转化为一个概率密度函数。 首先可以让 \\(f(x)\\) 变为非负的 \\(g(x)\\) ，比如 \\(g(x) = f(x)^2\\) \\(g(x) = e^{f(x)}\\) \\(g(x) = log(1 + f(x)^2)\\) ...... 可以看到这样的选择有很多，然后接下来便是归一化了，只需要 \\[P(x) = \\frac{g(x)}{\\int g(x)dx} = \\frac{g(x)}{Z}\\] 那么所谓的Energy Based Model 呢，其实很简单，我们就是假设这个函数 \\(g(x) = e^{f(x)}\\) 。 这个时候，下面那个体积volume呢，也叫做partition function。 为啥要 \\(exp()\\) 呢？因为希望在算概率的时候取log，和这个 $ e x p ( ) $ 很多时候能够抵消。而且也和统计物理（虽然笔者并没有研究过统计物理）也有一些联系，这也是energy名字的最初由来。 基本定义 对数据的概率分布进行描述时，这些概率分布都可以写成基于能量函数的形式(energy funciton)， \\(f(\\mathbf x )\\) 。对于连续变量，每个数据点对应一个概率密度函数值，对应一个能量值，如此概率分布即可写成如下玻尔兹曼分布的形式，也叫作吉布斯分布(Boltzmann/Gibbs distribution)： \\(p(\\mathbf x )=\\frac{e^{f(\\mathbf x)}}{Z}\\) \\(Z\\) 为概率归一化的分母，也称为配分函数(partition function)， \\(Z=\\int e^{f(\\mathbf x)} dx\\) 由以上公式可知，概率值较高的位置对应着能力较低的点。举一个简单的例子看一下，将高斯分布以能量函数的形式表示： \\[f(x;\\mu,\\sigma^2)=-\\frac{1}{2\\sigma^2}(x-\\mu)^2\\] \\[p(x)=\\frac{e^{f(\\mathbf x)}}{\\int e^{f(\\mathbf x)}dx}=\\frac{e^{-\\frac{1}{2\\sigma^2}(x-\\mu)^2}}{\\sqrt {2\\pi \\sigma^2}}\\] 部分应用 分类任务 一般来说，这个partition funtion是不能算的，除非是限制为一些可以积分的函数，得到闭式解，但那样表达力又太弱了。而在实际中，我们的 \\(f(x)\\) 一般是用神经网络进行模拟的，所以很难求出这个积分（你也可以遍历所有的情况，但这对于训练或者推理都是无法接受的）。 有时候呢，除非是要算出具体的概率，我们不需要管这个partition function，反正就是知道它是个常数。 比如我们想知道 \\(p(a)\\) 和 \\(p(b)\\) 哪个概率大，就不用去知道绝对的值，只需要知道相对大小就可以啦。这就可以用在分类任务里了。 比如对于图像识别的任务，我只需要知道一个物体是更有可能像猫还是更有可能像狗，而不一定要知道他们的具体概率。 课程中还列举了一个Ising model 的例子，也很直观，不赘述了： 组合专家系统 通过EBMs，可以把多个专家模型混合起来，用乘法。在对模型采样的时候，就会具有多个生成模型的所有性质，比如又是女人又是年轻又是美貌，就不会生成一个年迈的男人。 受限玻尔兹曼机也是基于能量模型，能量形式如下: \\[f(\\mathbf x;\\theta)=exp(\\mathbf x ^T\\mathbf{Wx}+\\mathbf{b}^T\\mathbf{x} + \\mathbf{c}^T\\mathbf{z})\\] 其它就不赘述了。 训练 损失函数（训练目标） 那么如何优化这个模型，直接想法肯定是极大似然估计 \\[L = \\ log (\\frac{exp({f_\\theta(x_{train})})}{Z(\\theta)}) = f_\\theta(x_{train}) - log(Z(\\theta))\\] 这里有个小问题，直接最大化分子并不能解决问题，因为分母是分子的积分，如果只顾着最大化分子的话，可能分母也跟着变大，那最后这整个分数就可能不变甚至变小！但是积分我们又计算不出来怎么办？蒙特卡洛估计便可以派上用场了，我们对 \\(L\\) 求一下梯度： \\[ \\begin{align*} \\nabla_{\\theta} f_{\\theta}(x_{\\text{train}}) - \\nabla_{\\theta} \\log Z(\\theta) &amp;= \\nabla_{\\theta} f_{\\theta}(x_{\\text{train}}) - \\frac{\\nabla_{\\theta} Z(\\theta)}{Z(\\theta)} \\\\ &amp;= \\nabla_{\\theta} f_{\\theta}(x_{\\text{train}}) - \\frac{1}{Z(\\theta)} \\int \\nabla_{\\theta} \\exp \\{ f_{\\theta}(x) \\} dx \\\\ &amp;= \\nabla_{\\theta} f_{\\theta}(x_{\\text{train}}) - \\frac{1}{Z(\\theta)} \\int \\exp \\{ f_{\\theta}(x) \\} \\nabla_{\\theta} f_{\\theta}(x) dx \\\\ &amp;= \\nabla_{\\theta} f_{\\theta}(x_{\\text{train}}) - \\int \\frac{\\exp \\{ f_{\\theta}(x) \\}}{Z(\\theta)} \\nabla_{\\theta} f_{\\theta}(x) dx \\\\ &amp;= \\nabla_{\\theta} f_{\\theta}(x_{\\text{train}}) - \\mathbb{E}_{x_{\\text{sample}}} [\\nabla_{\\theta} f_{\\theta}(x_{\\text{sample}})] \\\\ &amp;= \\nabla_{\\theta} f_{\\theta}(x_{\\text{train}}) - \\nabla_{\\theta} f_{\\theta}(x_{\\text{sample}}) \\end{align*} \\] 其中 \\(x_{sample} \\sim exp(f_\\theta(x_{sample}))/Z_\\theta\\) 最后一步代表在训练过程中，我们只取一个样本作为期望的估计值。 其实主观上也很好理解，其实就是对比了训练集和从模型中的采样，让训练集中数据的概率比随便采样出来的概率大。 我们对上面公式取个负，就是损失函数了。 如何采样 那么问题来了，我们怎么从这个能量模型中采样呢？你看看上面能量模型的式子，你只知道x比y概率大还是概率小，但你不知道x或者y的准确概率。 这时候，MCMC马尔科夫链蒙特卡罗就出场啦。 这是课程对于MCMC的叙述，没明白的可以复习一下，其实就是MH算法： 课程中没强调这个noise是对称的，就是 \\(x\\) 到 \\(x&#39;\\) 的概率等于 \\(x&#39;\\) 到 \\(x\\) 的概率。这时候上图中的关于 \\(q\\) 的分数就等于一了。那就是说，如果 \\(f(x’)\\) 的值大于当前值，那就无脑接受就好啦（2.1步）。如果没有大于，那就算一下比例咯（2.2步）。所以课程中的这个算法就是MH算法。 MH算法很美妙，但太慢啦。那怎么办呢？我们可以用郎之万Langevin 动力学来帮助MH算法，让随机游走朝着概率更高的地方走。 这就是 Metropolis-adjusted Langevin algorithm。 最后总结一下，先用MH算法抽样，用这些抽样放到contrasive divergence 算法里训练能量模型的参数，来极大似然 Score Matching 上面我们用MH算法给出了一个训练和推理的方法，但缺点很明显，就是收敛的太慢了，随着维度的增加，收敛速度指数级别下降。虽然用了郎之万Langevin 动力学来进行提速，但每次梯度一更新之后，分布就变了。所以对于contrasive divergence 的每一步来说，MCMC都要从头开始采样直到收敛。（MCMC采样不是一开始就能用的，要丢弃前n个样本，叫做burn in） 拿能否训练时候不用sampling呢？ score function 先看一下什么叫score function 就是指向高概率方向的梯度。一个观察是，这个梯度和分母，就是partition function无关。至于为什么叫做score fuction，那是因为我们一般把 \\(f_\\theta(x)\\) 对输入x的梯度称为score。 score matching 在之前的MCMC采样方法训练中，当我们有了一个准确的能量模型后，我们从数据分布里采样就转换成了根据训练好的能量模型的score, 来进行MCMC采样。那么为什么不能换个思路，直接将能量模型建模成score，即用一个神经网络来拟合score! 这个方法就叫score-matching! 如上所示，我们的目标依旧是用score matching 来减小这两个分布的区别。难点在于，对于真实分布Pdata怎么求导呢？先看看一维的情况： \\[ \\begin{align*} \\frac{1}{2} \\mathbb{E}_{x \\sim p_{\\text{data}}} \\left[ (\\nabla_x \\log p_{\\text{data}}(x) - \\nabla_x \\log p_{\\theta}(x))^2 \\right] &amp;= \\frac{1}{2} \\int p_{\\text{data}}(x) \\left[ (\\nabla_x \\log p_{\\text{data}}(x) - \\nabla_x \\log p_{\\theta}(x))^2 \\right] dx &amp;\\\\ &amp;= \\frac{1}{2} \\int p_{\\text{data}}(x) (\\nabla_x \\log p_{\\text{data}}(x))^2 dx + \\frac{1}{2} \\int p_{\\text{data}}(x) (\\nabla_x \\log p_{\\theta}(x))^2 dx - \\int p_{\\text{data}}(x) \\nabla_x \\log p_{\\text{data}}(x) \\nabla_x \\log p_{\\theta}(x) dx &amp; \\end{align*} \\] 其中第一项是常数，我们不用管，第二项也只涉及到 \\(p_\\theta\\) (积分的 \\(p_{data}\\) 直接通过在训练集抽样即可)，第三项比较棘手，涉及到 \\(\\nabla_x \\log p_{\\text{data}}(x)\\) ，这个我们没法直接求出。 但是我们可以通过分布积分来进行化简： \\[ \\begin{align*} -\\int p_{\\text{data}}(x) \\nabla_x \\log p_{\\text{data}}(x) \\nabla_x \\log p_{\\theta}(x) dx &amp;= - \\int p_{\\text{data}}(x) \\frac{1}{p_{\\text{data}}(x)} \\nabla_x p_{\\text{data}}(x) \\nabla_x \\log p_{\\theta}(x) dx \\\\ &amp;= - p_{\\text{data}}(x) \\nabla_x \\log p_{\\theta}(x) \\Big|_{x = -\\infty}^{x = \\infty} + \\int p_{\\text{data}}(x) \\nabla_x^2 \\log p_{\\theta}(x) dx \\\\ &amp;= \\int p_{\\text{data}}(x) \\nabla_x^2 \\log p_{\\theta}(x) dx \\end{align*} \\] 其中我们认为 \\(p_{\\text{data}}(x) \\nabla_x \\log p_{\\theta}(x) \\Big|_{x = -\\infty}^{x = \\infty} = 0\\) ，因为我们假定无穷远处的 \\(p_{data}\\) 为0。 对于多维与一维类似，区别就是我们分部积分得到的结果是 \\(log(p_\\theta(x))\\) 的Hessian的迹，最终我们得到的形式如下： 我们通过分部积分把对 \\(P_{data}\\) 的梯度项给搞没了，就不用像之前那样费劲的去MCMC了。不过缺点是这个Hessian矩阵算起来很麻烦。 Noise contrastive estimation 把NCE用在Energy Based Model其实思想也很简单，我们在GANs中提到，对于一个真实样本和模型样本进行分类的最佳判别器是，对给定 \\(x\\) 的判定为真实样本的概率为\\(\\frac{P_{data}(x)}{P_{data}(x) + P_n(x)}\\)。 所以NCE的想法就是我去用生成器组成一个判别器，这个生成器输出概率为 \\(P_{\\theta^*}(x)\\) ，而判别器的输出则是 \\(\\frac{P_{\\theta^*}(x)}{P_{\\theta^*}(x) + P_n(x)}\\) ，这样当判别器训练成为最佳判别器时， \\(P_{\\theta^*}(x)\\) 就等于 $P_{data}(x) $ 。 注意，这里的 \\(P_n\\) 的概率是我们给定一个特定噪声分布进行采样的概率，所以很好获得。也就是说在NCE中，非真实样本的概率分布是事先指定的，而不是模型学习得到的。 但是，依旧会到那个问题， \\(P_{\\theta^*}(x) = \\frac{exp(f_{\\theta^*}(x))}{Z_{\\theta*}}\\) ，我的分母怎么处理呢？此时我们可以把 \\(Z\\) 也作为一个参数进行训练。假如我们能够得到最佳判别器，由于 \\(\\frac{exp(f_{\\theta^*}(x))}{Z^{*}} = P_{data}\\) ，所以 \\(Z\\) 也就肯定是最佳的分区函数了。 把这个形式带入我们二分类的目标函数（与GANs相同，这里不赘述了）： 当然，对于这个 \\(p_n\\) ，它对于训练效果有很显著的影响，毕竟区分图片和一堆噪声可不用很强的判别能力。所以后面也有对这个的改进工作，具体也就是类似GANs一样，再加一个生成器： 当时，这样就会训练得到两个生成模型了，具体推理阶段都可以使用。 注意，能量模型作为生成模型的一种，建模的是 \\(P(x)\\) ,主要功能是从 \\(P(x)\\) 里面采样。上面说的score matching是在训练的时候不用从中采样，加快训练的脚步，但是真正使用的时候还是得有MCMC。NCE因为显式的训练了partition function \\(Z\\) ，也许可以不用MCMC（但笔者感觉也没有比较好的直接采样方法，个人觉得还是需要靠MCMC，如果读者有好的想法也可以指正我）。","categories":[{"name":"Stanford CS236深度生成模型","slug":"Stanford-CS236深度生成模型","permalink":"https://jia040223.github.io/categories/Stanford-CS236%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}],"tags":[{"name":"生成模型","slug":"生成模型","permalink":"https://jia040223.github.io/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}]},{"title":"蒙特卡洛采样方法","slug":"蒙特卡洛采样方法","date":"2024-09-19T12:04:35.000Z","updated":"2024-09-20T13:29:20.636Z","comments":true,"path":"2024/09/19/蒙特卡洛采样方法/","permalink":"https://jia040223.github.io/2024/09/19/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E9%87%87%E6%A0%B7%E6%96%B9%E6%B3%95/","excerpt":"","text":"最近在学习Stanford CS236课程，里面多次提到了蒙特卡洛采样，但本人之前并没有系统地对蒙特卡洛采样进行过整理学习，所以也就正好趁此机会学习一下蒙特卡洛采样，分享记录，也便于自己实时查看。 蒙特卡洛估计 蒙特卡洛估计的原理 蒙特卡洛估计(Monte Carlo Estimator)的原理很简单，假设现在我们要求解一个一维的积分 \\(\\int_{a}^{b} g(x) dx\\) 。已知一个概率密度为 \\(f(x)\\) 的随机变量 $ X$ ，蒙特卡洛估计可以表示为： \\[G_N = \\frac{1}{N}\\sum_{i=1}^{N}{\\frac{g(X_i)}{f(X_i)}}\\\\\\] 概率密度 \\(f(x)\\) 需要满足 \\[ \\begin{cases} f(x) &gt; 0, x \\in (a, b),\\\\ f(x) = 0, x \\notin (a, b).\\end{cases}\\\\\\] 现在来验证下, 这种方式是正确的： \\[\\begin{align*} E[G_N] &amp; =E\\left [ \\frac{1}{N}\\sum_{i=1}^{N}{\\frac{g(X_i)}{f(X_i)}} \\right]\\\\ &amp; = \\frac{1}{N}\\sum_{i=1}^{N}\\int_{a}^{b}\\frac{g(x)}{f(x)}f(x)dx\\\\ &amp;= \\frac{1}{N}\\sum_{i=1}^{N}\\int_{a}^{b}g(x)dx\\\\ &amp;= \\int_{a}^{b}g(x)dx \\end{align*}\\\\\\] 也就是说， \\(G_N\\) 的期望与 \\(\\int_{a}^{b} g(x) dx\\) 是相同的，而 \\(G_N\\) 的方差如下： \\[\\begin{align*} D[G_N] &amp; =D\\left [ \\frac{1}{N}\\sum_{i=1}^{N}{\\frac{g(X_i)}{f(X_i)}} \\right]\\\\ &amp; = \\frac{1}{N^2}D\\left [ \\sum_{i=1}^{N}{\\frac{g(X_i)}{f(X_i)}} \\right]\\\\ &amp;= \\frac{1}{N^2}\\cdot N \\cdot D\\left [ {\\frac{g(X_i)}{f(X_i)}} \\right]\\\\ &amp;= \\frac{1}{N} D\\left [ {\\frac{g(X_i)}{f(X_i)}} \\right] \\end{align*}\\\\\\] 从上面的式子，可以看出，要减少方差，有两种途径： 增加采样次数 \\(N\\) 减少 \\(D(\\frac{g(X)}{f(X)})\\) 理论上，只要我们采样次数足够多，方差趋近于0，\\(G_N\\) 也就依概率收敛于\\(\\int_{a}^{b} g(x) dx\\) 蒙特卡洛估计的优点 我们在考虑一个积分算法/Estimator 时，通常从两个角度考虑。 一个是计算的准确性，即随着采样次数增大时，结果是否趋近于我们期望的真实值。如果一个 estimator 的期望值和真实值相等，我们说它是无偏的/unbiased。如果一个 estimator 的期望值和真实值不相等，则它是有偏的。大部分 estimator 都是无偏的，在少数情况下，我们会使用一个有偏的但是计算收敛速度很快的 estimator。 另外一个角度是计算结果的方差。随着采样次数增大时，计算结果的方差应该总是减少的。两个estimator 的方差可以比较可以从两个角度来体现。即采样次数相同时的方差大小，以及随着采样次数增大，方差收敛的速度。我们总是期望使用一个方差较小且收敛较快的 estimator，来减少计算的事件。 计算结果表明，蒙特卡洛估计误差收敛的速度为 \\(O(\\sqrt N)\\) (意味着4倍的采样会使误差减少一半)，蒙特卡洛估计不受维度影响，在高维情况下比其他估计方法收敛要快得多。 蒙特卡洛估计的实践使用 在实际使用中，直接使用蒙特卡洛方法要求我们能够从 $p(x) $ 中采样——对于简单分布（如均匀分布）这是容易做到的；对于稍微复杂一些但可写出 PDF 或 CDF 分布，可以利用变量替换定理来直接采样；而对于更复杂的分布，我们则更多选择拒绝采样和重要性采样来实现这一点。。 变量替换定理 $ X$ 服从一个我们能直接进行采样的连续值（例如均匀分布），我们希望找到一个函数 \\(f(x)\\) ，让 \\(Y=f(X)\\) 满足我们需要得到的分布 \\(Y \\sim P_y\\) ，则使用累积分布函数： \\[P_y(y)\\triangleq P(Y\\le y)=P(f(X)\\le y)=P(X\\in(f(x)\\le y))\\] 概率密度函数可以通过累积分布函数求导得到。当单调，因此可逆时，可得： \\[P_y(y)=P(f(X)\\le y)=P(X\\le f^{-1}(y))=P_x(f^{-1}(y))\\] 求导可得： \\[p_y(y)\\triangleq\\frac{d}{dy}P_y(y)=\\frac{d}{dy}P_x(f^{-1}(y))=\\frac{dx}{dy}\\frac{d}{dx}P_x(x)=\\frac{dx}{dy}p_x(x)\\] 其中 \\(x=f^{-1}(y)\\) 。由于符号并不重要，因此可得一般表达式： \\[p_y(y)=p_x(x)|\\frac{dx}{dy}|\\] 可将上述结果拓展为多变量分布。令 \\(f\\) 为 \\(R^n\\) 到 \\(R^n\\) 的映射， \\(\\mathrm y=f(\\mathrm x)\\) 。则雅可比矩阵 \\(J\\) 为： \\[J_{\\mathrm x\\rightarrow\\mathrm y}\\triangleq\\frac{\\partial(y_1,\\ldots,y_n)}{\\partial(x_1,\\ldots,x_n)}\\triangleq \\begin{pmatrix} \\frac{\\partial y_1}{\\partial x_1} &amp;\\dots&amp;\\frac{\\partial y_1}{\\partial x_n}\\\\ \\vdots&amp;\\ddots&amp;\\vdots\\\\ \\frac{\\partial y_n}{\\partial x_1}&amp;\\dots&amp;\\frac{\\partial y_n}{\\partial x_n} \\end{pmatrix}\\] \\(|det J|\\) 度量了单位立方体在应用 \\(f\\) 时的体积变化量。如果 \\(f\\) 是一个可逆映射，可以使用反映射 \\(\\mathrm y\\rightarrow\\mathrm x\\) 的雅可比矩阵定义变换变量的概率密度函数： \\[p_y(\\mathrm y)=p_x(\\mathrm x)|\\det\\left(\\frac{\\partial \\mathrm x}{\\partial\\mathrm y}\\right)|=p_x(\\mathrm x)| \\det J_{\\mathrm y\\rightarrow\\mathrm x}|\\] 这就是随机变量的变量替换定理，通过这个我们可以对一些相对简单的分布进行直接采样了。 例如设 \\(x\\) 服从累积分布函数为 \\(F(x)=1-e^{-x}\\) (可验证是单调不减，且积分为1的函数)的分布，则可以通过逆变换的方法对 \\(F(x)\\) 直接采样，产生服从F(X)分布的样本X。 令 \\(y=1-e^{-x}\\)，则$ e^{-x}=1-y $ .两边求对数可得: \\(x=-ln(1-y)\\) ,则 \\(F^{-1}(x)=-ln(1-x)\\) ，令 \\(x_i\\) 为均匀分布样本，则 \\(X_i=-ln(1-x_i)\\) 为服从累积分布函数为 $ F(x)$ 分布的样本. 拒绝接受采样 拒绝接受采样的目的仍然是得到服从某个概率分布的样本，不过这种方法是直接利用概率密度函数(PDF)得到样本。如下图所示， \\(p(x)\\) 是我们希望采样的分布， \\(q(x)\\) 是我们提议的分布(proposal distribution)， \\(q(x)\\) 分布比较简单，令 \\(kq(x)&gt;p(x)\\) ，我们首先在 \\(kq(x)\\) 中按照直接采样的方法采样粒子，接下来以 \\(\\frac{p(x_i)}{kq(x_i)}\\) 的概率接受这个点，最终得到符合 \\(p(x)\\) 的N个粒子。 可以证明，这样做得到的样本是服从\\(p(x)\\)的，我们可以计算 \\(x_0\\) 对应的样本被取到的概率为： \\[\\frac{q(x_0)\\dfrac{\\tilde p(x_0)}{kq(x_0)}}{\\displaystyle\\int_x q(x)\\frac{\\tilde p(x)}{kq(x)}\\mathrm dx}=\\frac{\\tilde p(x_0)}{\\displaystyle\\int_x \\tilde p(x)\\mathrm dx}=p(x_0)\\] 所以拒绝接受采样的基本步骤： 生成服从 \\(q(x)\\) 的样本 \\(x_i\\) . 生成服从均匀分布 \\(U(0,1)\\) 的样本 \\(u_i\\) . 当 \\(k\\cdot q(x_i)\\cdot u_i&lt;p(x_i)\\) ,也就是二维点落在蓝线以下，此时接受 \\(X_k=x_i\\) 这里乘以 \\(u_i\\) ，是因为我们需要以 \\(\\frac{p(x_i)}{kq(x_i)}\\) 的概率接受这个点，因为如果 \\(k\\cdot q(x_i)\\cdot u_i&lt;p(x_i)\\) ，则 \\(u_i&lt;\\frac{p(x_i)}{k\\cdot q(x_i)}\\) ，而 \\(u_i\\) 服从均匀分布 \\(U(0,1)\\) 最终得到的 \\(X_k\\) 为服从 \\(p(x)\\) 的样本. 我们可以计算一下样本采样的接受率： \\[p(\\text{accept})=\\int_x \\frac{\\tilde p(x)}{kq(x)}q(x)\\mathrm dx=\\frac{1}{k}\\int_x\\tilde p(x)\\mathrm dx\\] 因此 \\(k\\) 越小，总接受率越大，算法效率越高。然而， \\(k\\) 小也意味着 \\(q(x)\\) 本身就要与 \\(p(x)\\) 比较相似，对于复杂的 \\(p(x)\\) 而言寻找到一个合适的 \\(q(x)\\) 非常困难的。 重要性采样 重要性采样的目的：求一个函数 \\(f(x)\\) 在概率密度函数为 $ p(x)$ 分布下的期望，即 \\[\\mathbb{E}[f(x)]=\\int f(x)p(x)dx\\] 当 \\(p(x)\\) 很复杂时，不解析，积分不好求时，可以通过重要性采样来计算。当 \\(f(x)=x\\) ，则可以算 \\(p(x)\\) 的期望。 原理 首先, 当我们想要求一个函数 \\(f(x)\\) 在区间 \\([a, b]\\) 上的积分 \\(\\int_{a}^{b} f(x) d x\\) 时有可能会面临一个问题, 那就是积分曲线难以解析, 无法直接求积分。这时候我们可以采用一种估计的方式, 即在区间 \\([a, b]\\) 上进行采样: \\(\\left\\{x_{1}, x_{2} \\ldots, x_{n}\\right\\}\\) , 值为 \\(\\left\\{f\\left(x_{1}\\right), f\\left(x_{2}\\right), \\ldots, f\\left(x_{n}\\right)\\right\\}\\) 如果采样是均匀的, 即如下图所示: 那么显然可以得到这样的估计: \\(\\int_{a}^{b} f(x) d x=\\frac{b-a}{N} \\sum_{i=1}^{N} f\\left(x_{i}\\right)\\) , 在这里 \\(\\frac{b-a}{N}\\) 可以看作是上面小长方形的底部的 “宽”, 而 \\(f\\left(x_{i}\\right)\\) 则是坚直的 “长”。 上述的估计方法随着取样数的增长而越发精确，那么有什么方法能够在一定的抽样数量基础上来增加准确度，减少方差呢？比如 \\(x\\) 样本数量取10000，那么显然在 \\(f(x)\\) 比较大的地方，有更多的 \\(x_i\\) ，近似的积分更精确。 并且原函数 \\(f(x)\\) 也许本身就是定义在一个分布之上的, 我们定义这个分布为 \\(p(x)\\) , 我们无法直接从 $ p(x)$ 上进行采样, 所以另辟蹊径重新找到一个更加简明的分布 \\(q(x)\\) , 从它进行取样, 希望间接地求出 \\(f(x)\\) 在分布 \\(p(x)\\) 下的期望。 若p(x)归一化 搞清楚了这一点我们可以继续分析了。首先我们知道函数 \\(f(x)\\) 在概率分布 \\(p(x)\\) 下的期望为: \\[\\mathbb{E}[f(x)]=\\int_{x} p(x) f(x) d x \\] 但是这个期望的值我们无法直接得到, 因此我们需要借助 \\(q(x)\\) 来进行采样, \\(q(x)\\) 可以选取简单的分布，比如设q(x)为均匀分布，当我们在 \\(q(x)\\) 上采样得到 \\(\\left\\{x_{1}, x_{2}, \\ldots, x_{n}\\right\\}\\) （即 \\(x_i\\) 服从 \\(q(x)\\) 分布）后，那么我们可以估计 \\(f\\) 在 \\({q(x)}\\) 下的期望为： \\[\\mathbb{E}[f(x)]=\\int_{x} q(x) f(x) d x \\approx \\frac{1}{N} \\sum_{i=1}^{N} f\\left(x_{i}\\right) \\] 上面这个式子就简单很多了，只要我们得到 \\(x_i\\) 然后代入 \\(f(x)\\) 然后求和就行了，而且均匀分布的样本 \\(x_i\\) 很容易获得。接着我们来考虑原问题，对式(1)进行改写, 即： \\(p(x) f(x)=q(x) \\frac{p(x)}{q(x)} f(x)\\) , 所以我们可以得到: \\[\\mathbb{E}[f(x)]=\\int_{x} q(x) \\frac{p(x)}{q(x)} f(x) d x\\] 这个式子我们可以看作是函数 \\(\\frac{p(x)}{q(x)} f(x)\\) 定义在分布 \\(q(x)\\) 上的期望, 当我们在 \\(q(x)\\) 上采样 \\(\\left\\{x_{1}, x_{2}, \\ldots, x_{n}\\right\\}\\) (服从q(x)分布)，可以估计 \\(f\\) 的期望: \\[\\begin{aligned}\\mathbb{E}[f(x)]&amp;=\\frac{1}{N} \\sum_{i=1}^{N} \\frac{p\\left(x_{i}\\right)}{q\\left(x_{i}\\right)} f\\left(x_{i}\\right)\\\\&amp;=\\frac{1}{N} \\sum_{i=1}^{N} w_i f\\left(x_{i}\\right)\\end{aligned}\\] 在这里 \\(w_i=\\frac{p\\left(x_{i}\\right)}{q\\left(x_{i}\\right)}\\) 就是重要性权重。 若p(x)没有归一化 上面的讨论是假设 \\(p(x)\\) 已经完成归一化了，也就是 \\(\\int p(x)=1\\) ,假如 \\(p(x)\\) 没有归一化，那么我们可以在上面的推导中对 \\(p(x)\\) 进行归一化： \\[\\begin{aligned}\\mathbb{E}[f(x)]&amp;=\\int f(x) \\frac{p(x)}{\\int p(x) d x} d x\\\\&amp;=\\frac{\\int f(x) p(x) d x}{\\int p(x) d x}\\\\&amp;=\\frac{\\int f(x) \\frac{p(x)}{q(x)} q(x) d x}{\\int \\frac{p(x)}{q(x)} q(x) d x}.\\end{aligned}\\] 而分子分母可分别得到，下面两式约等于都利用 \\(q(x)\\) 是均匀分布的假设： \\[\\begin{aligned}\\int f(x) \\frac{p(x)}{q(x)} q(x) d x &amp;\\approx \\frac{1}{n} \\sum_{i=1}^{n} W_{i} f\\left(x_{i}\\right), \\\\\\int \\frac{p(x)}{q(x)} q(x) d x &amp;\\approx \\frac{1}{n} \\sum_{i=1}^{n} W_{i}.\\end{aligned}\\] 其中 \\(W_i=\\frac{p(x_i)}{q(x_i)}\\) ，则最终可得 \\(\\mathbb{E}[f(x)]\\) : \\[\\begin{aligned}\\mathbb{E}[f(x)] \\approx \\sum_{i=1}^{n} w_{i} f\\left(x_{i}\\right), w_{i}=\\frac{W_{i}}{\\sum_{i=1}^{n} W_{i}}\\end{aligned}\\] 多重重要性采样 有的时候, 需要积分的方程中可能包含多个需要积分的部分, 这时候就需要用到多重重要性采样(multiple importance sampling/MIS). 比如现在要求解 \\(\\int_{}^{} g_1(x)g_2(x)\\) 这样的积分时, 两个部分分别对应两个概率密度 \\(f_1(x), f_2(x)\\) , MIS给出的新的蒙特卡洛估计为: \\[\\frac{1}{n_1} \\sum_{i=1}^{n_1}{\\frac{g_1(X_1)g_2(X_1)\\omega_1(X_1)}{f(X_1)}} + \\frac{1}{n_2} \\sum_{i=1}^{n_2}{\\frac{g_1(X_2)g_2(X_2)\\omega_2(X_2)}{f(X_2)}}\\\\\\] \\(n_1,n_2\\) 分别是两边的采样次数, $_1, _2 $ 分别是两个部分对应的权重. 一个常用的权重函数为: \\[\\omega_k = \\frac{(n_kf_k(x))^2} {\\sum_{i}^{}{(n_1f_i(x))^2}}\\\\\\] 在上面有两个部分的情况下得: \\[\\omega_1 = \\frac{(n_1f_1(x))^2} {(n_1f_1(x))^2 +(n_2f_2(x))^2 }\\\\ \\omega_2 = \\frac{(n_2f_2(x))^2} {(n_1f_1(x))^2 +(n_2f_2(x))^2 }\\\\\\] 与拒绝采样一样，重要性采样的效果与提议分布 $q(x) $ 同 \\(p(x)\\) 的接近程度紧密相关。当 \\(p(x)\\) 比较复杂时，选择合适的 \\(q(x)\\) 是非常困难的。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://jia040223.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jia040223.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"概率论与数理统计","slug":"概率论与数理统计","permalink":"https://jia040223.github.io/tags/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/"}]},{"title":"GANs","slug":"GANs","date":"2024-09-18T14:10:33.000Z","updated":"2024-09-20T13:27:19.686Z","comments":true,"path":"2024/09/18/GANs/","permalink":"https://jia040223.github.io/2024/09/18/GANs/","excerpt":"","text":"本学习笔记用于记录我学习Stanford CS236课程的学习笔记，分享记录，也便于自己实时查看。 引入 前面我们学习了VAEs和Normalizing Flows，这两种模型都是基于最小化KL散度（对似然进行评估）来进行优化的。我们也可以看到，为了进行生成，我们往往会定义一个潜变量 \\(z\\) ，所以对似然进行评估并不容易。VAEs是通过优化似然的下限ELBO来绕过这个问题，而Normalizing Flows是通过限制映射的形式来计算似然。 直接计算似然来进行评估，要么只能计算其下界，要么需要限制映射的形式。那有没有一种方法能够用间接方法代替这种直接比较，使生成分布变得越来越接近真实分布呢？GANs便是基于一种间接的评估方式进行设计的。 基本思想 GANs的间接方法采用这两个分布的下游任务形式。然后，生成网络的训练是相对于该任务进行的，使生成分布变得越来越接近真实分布。GANs 的下游任务是区分真实样本和生成样本的任务。或者我们可以说是“非区分”任务，因为我们希望区分尽可能失败。 因此，在 GANs 架构中，我们有一个判别器，它接收真实和生成数据样本，并尽可能地对它们进行分类；还有一个生成器，它被训练成尽可能地欺骗判别器。即GANs由2个重要的部分构成： 生成器(Generator)：通过机器生成数据，目的是“骗过”判别器。 判别器(Discriminator)：判断数据是真实的还是生成的，目的是找出生成器做的“假数据”。 训练过程 我们知道GANs的思想后，便能很直观的想到用分类问题的交叉熵作为判别器的损失函数。同时生成器的目的则是最大化这个交叉熵损失函数（混淆判别器），所以我们的训练目标是： \\[\\mathop{\\text{min}}\\limits_{G}\\mathop{\\text{max}}\\limits_{D} \\ V(G,D) = \\mathbb{E}_{x \\sim p_{data}(x)} [\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)} [\\log(1 - D(G(z)))] \\] 其中 \\(G\\) 指的是生成器， \\(D\\) 指的是判别器。 所以我们的训练目标是一个极大极小的优化问题，在实际中，我们只需要从数据集中进行采样，然后用生成器进行采样，然后对上面的目标函数进行近似计算，最后进行梯度上升或者梯度下降即可 与散度的关系 那么为什么这样的设计能够间接地去让生成器生成的样本与真实样本的分布相同呢？ 其实本质上，GANs通过引入判别器来间接地计算了 \\(\\frac{P_\\theta(x)}{P_{data}(x)}\\) ，可以证明，对于一个生成器下的最佳判别器对给定 \\(x\\) 的判定为真实样本的概率是\\(\\frac{P_{data}(x)}{P_{data}(x) + P_\\theta(x)}\\)， 证明如下： *Proof*: 二分类交叉熵损失函数为： \\[\\begin{align} \\mathrm{BCE}(\\mathcal P_1,\\mathcal P_2)&amp;=-\\mathbb E_{x\\sim \\mathcal P_1}[\\log D(x)]-\\mathbb E_{x\\sim \\mathcal P_2}[\\log(1-D(x))]\\\\ &amp;=-\\int \\log D(x)\\cdot p_1(x)\\mathrm d x-\\int\\log(1-D(x))\\cdot p_2(x)\\mathrm d x\\\\ &amp;=-\\int \\left[\\log D(x)\\cdot p_1(x)+\\log(1-D(x))\\cdot p_2(x)\\right]\\mathrm d x\\\\ \\end{align} \\\\\\] 易知 \\(y=a\\log x+b\\log(1-x)\\) 在 \\(x=\\frac{a}{a+b}\\) 处取到唯一极大值（其中 \\(0\\leq a,b\\leq1\\) ），所以欲使上式最小，只需： \\(\\forall x,\\,D(x)=\\frac{p_1(x)}{p_1(x)+p_2(x)} \\\\\\) 这样就证明完成了。 那么，再看我们的训练目标： \\[\\min_G\\max_D V(G, D) \\\\ \\begin{align} V(G, D)&amp;=\\mathbb E_{x\\sim\\mathcal P_{data}}[\\log D(x)]+\\mathbb E_{z\\sim \\mathcal P_z}[\\log(1-D(G(z)))]\\\\ &amp;=\\mathbb E_{x\\sim\\mathcal P_{data}}[\\log D(x)]+\\mathbb E_{x\\sim \\mathcal P_{\\theta}}[\\log(1-D(x))] \\end{align} \\\\\\] 而最优判别器为： \\(D^\\ast(x)=\\frac{p_{data}(x)}{p_{data}(x)+p_\\theta(x)} \\\\\\) 将最优判别器代入 \\(G\\) 的优化目标： \\[\\begin{align} V(G, D^\\ast)&amp;=\\mathbb E_{x\\sim \\mathcal P_{data}}\\left[\\log\\frac{p_{data}(x)}{p_{data}(x)+p_\\theta(x)}\\right]+\\mathbb E_{x\\sim \\mathcal P_\\theta}\\left[\\log\\frac{p_\\theta(x)}{p_{data}(x)+p_\\theta(x)}\\right]\\\\ &amp;=2\\mathrm {JS}(\\mathcal P_{data}\\|\\mathcal P_\\theta)-2\\log2 \\end{align} \\\\ \\] 因此，生成器实际上在最小化 $ P_{data}$ 和 \\(\\mathcal P_\\theta\\) 的 \\(\\mathrm{JS}\\) 散度，从而让生成数据的分布 \\(\\mathcal P_\\theta\\) 接近真实分布 \\(\\mathcal P_{data}\\) 。 注： \\(JS\\) 散度的定义如下： *\\[\\begin{align} \\mathrm{JS}(\\mathcal P_1\\|\\mathcal P_2)&amp;=\\frac{1}{2}\\left[\\mathrm{KL}\\left(\\mathcal P_1\\|\\mathcal P_A\\right)+\\mathrm{KL}\\left(\\mathcal P_2\\|\\mathcal P_A\\right)\\right]\\\\ &amp;=\\log 2+\\frac{1}{2}\\mathbb E_{x\\sim\\mathcal P_1}\\left[\\log\\frac{p_1(x)}{p_1(x)+p_2(x)}\\right]+\\frac{1}{2}\\mathbb E_{x\\sim\\mathcal P_2}\\left[\\log\\frac{p_2(x)}{p_1(x)+p_2(x)}\\right] \\end{align} \\\\\\] 其相比 \\(KL\\) 散度最大的特点便是其是对称的*。 可以看出GANs是通过判别器来巧妙地规避了计算似然的问题，但正是因为在实践中我们很难得到真正的最佳判别器，所以实际上我们很多时候只是在优化 \\(JS\\) 散度的一个下界，笔者认为这是GANs不得不直面的一个问题。 fGAN F-散度(F-divergence) 在概率统计中，f散度是一个函数，这个函数用来衡量两个概率密度\\(p\\)和\\(q\\)的区别，也就是衡量这两个分布多么的相同或者不同。像 \\(KL\\) 散度和 \\(JS\\) 散度都是它的一种特例 f散度定义如下： \\[{D_f}(\\mathcal P_1\\|\\mathcal P_2)=\\int f (\\frac{p_2(x)}{p_1(x)})\\cdot p_1(x)\\mathrm d x=\\mathbb E_{x\\sim\\mathcal P_1}\\left[f(\\frac{p_2(x)}{p_1(x)})\\right] \\\\\\] \\(f(·)\\) 就是不同的散度函数， \\(D_f\\) 就是在f散度函数下，两个分布的差异。规定 \\(f\\) 是凸函数(为了用琴生不等式) $f ( 1 ) = 0 $ (如果两个分布一样，刚好公式=0) 这两个规定保证了 \\(D_f\\) 是非负的，而且当两个分布相同时，其值为0，一些常见散度的 \\(f\\) 定义如下： 共轭函数(Fenchel Conjugate) 一个函数 \\(f:\\;\\mathbb{R}^n\\mapsto\\mathbb{R}\\) 的 Frenchel 共轭为： \\[\\begin{align} f^*( t)=\\sup_{ x}\\big(\\langle t, x\\rangle-f( x)) \\end{align}\\] Fenchel 共轭有几何上的解释。当 $ x$ 固定时， \\(\\langle t, x\\rangle-f( x)\\) 是一个仿射函数，因此 Fenchel 共轭就是一组仿射函数的上确界。如果 \\(f\\) 可微，那么仿射函数取得上确界的位置正好是 \\(f\\) 的切线，此处有 \\(\\nabla f( x)= t\\) 。 我们拿 $f ( x ) = x l o g x $ 来说，当 \\(x=10,1, 0.1\\) 时可以看到相应的函数直线，可以看到最大化y的点连起来是个凸函数，很类似$ e^{t-1}$ 公式图像： 用数学来推一下： 将 \\(f ( x ) = x l o g x\\) 代入 $y ( t ) = x t − f ( x ) $ ，得 $y ( x ) = x t − x l o g x $ ,对于每个给定的 \\(t\\) 都可以求出最大值，求导为0即可。 求导后得： \\(t − l o g x − 1 = 0\\) ,即 \\(x=e^{t-1}\\) ，代入$ f(t) $, 得 $ f^(t)=te{t-1}-e{t-1}(t-1)=e{t-1}$ 读者可以对这个 $ f^*(t)$ 再求一次共轭，可以发现其又变回原函数了。 事实上，可以证明，对于凸函数来说$ f^{**}(x) = f(x)$ 应用于GAN 那这个跟GAN有啥关系呢？ 假如我们用一个 \\(D_f\\) 来评估生成模型，对于 \\(p(x)\\) 和 \\(q(x)\\) 之间的 f-divergence： \\[ \\begin{aligned} D_f(P||Q) &amp;= \\int_{x} q(x) f\\left(\\frac{p(x)}{q(x)}\\right) dx \\\\ &amp;= \\int_{x} q(x) \\left( \\max_{t \\in \\operatorname{dom}(f^*)} \\left\\{\\frac{p(x)}{q(x)}t - f^*(t)\\right\\} \\right) dx \\end{aligned} \\] 记一个函数 D(x)，它输入是 \\(x\\) ，输出是 \\(t\\) ，用该函数代替上式中的 \\(t\\) ，得到 \\[ \\begin{aligned} D_f(P||Q)&amp;\\geq\\int \\limits_{x}q(x)(\\frac{p(x)}{q(x)}D(x)-f^{*}(D(x)))dx\\\\ &amp;= \\int \\limits_{x}p(x)D(x)dx-\\int \\limits_{x}q(x)f^{*}(D(x))dx \\end{aligned} \\] D(x) 其实就是判别器，可以看出，它依然是在解一个求最大值问题，通过这种方法，去逼近 f-divergence。 \\[D_f(P||Q)\\approx\\max \\limits_{D}\\int \\limits_{x}p(x)D(x)dx-\\int \\limits_{x}q(x)f^{*}(D(x))dx\\] p(x) 和 q(x) 本质上是一个概率，于是有 \\[D_f(P||Q)\\approx\\max \\limits_{D}\\{E_{x\\sim P}[D(x)]-E_{x\\sim Q}[f^*(D(x))]\\}\\] 用 \\(P_{data}\\) 和 \\(P_\\theta\\) 来指代 P 和 Q，有 \\[D_f(P_{data}||P_\\theta)\\approx\\max \\limits_{D}\\{E_{x\\sim P_{data}}[D(x)]-E_{x\\sim P_\\theta}[f^*(D(x))]\\}\\] 有没有发现这一套下来很熟悉？其实这还是我们之前训练生成器判别器的那一套流程。也就是 \\[ \\begin{aligned} G^*&amp;=\\mathop{argmin} \\limits_{G}D_f(P_{data}||P_\\theta)\\\\&amp;=\\mathop{argmin} \\limits_{G}\\max \\limits_{D}\\{E_{x\\sim P_{data}}[D(x)]-E_{x\\sim P_\\theta}[f^*(D(x))]\\}\\\\&amp;=\\mathop{argmin} \\limits_{G}\\max \\limits_{D}V(G, D) \\end{aligned} \\] 只不过这次的损失函数更加 general 了。换不同的 \\(f(x)\\) ，就可以量不同的散度（divergence）。 WGAN JS散度 to Wasserstein（Earth-Mover EM）距离 JS散度的问题 考虑两个分布\"完全不相交\"的时候，会发现 \\(JS\\) 散度为常量，梯度为 \\(0\\) 无法优化。 下面一个例子来说明: 假设两个二维空间上的概率分布，记为 \\({P}_d(X_1, Z)\\) 和 \\({P}_g(X_2, Z)\\) 。我们刻画 \\(Z \\sim U(0, 1)\\) 一个 \\([0, 1]\\) 上的均匀分布，而分别令 $ X_1 = 0$ 和 \\(X_2 = \\theta\\) ，因而，它们在二维空间上的概率分布空间就是两条平行线（垂直于 \\(x\\) 的轴，而平行于 \\(z\\) 的轴）。 当 \\(\\theta = 0.5\\) 时，我们考量等价于JS散度的损失函数 \\(V(G, D^*)\\) ，由于两个分布概率大于0的空间范围是完全没有重叠的，因此，对于任意 \\(p_d(x,y) \\ne 0\\) 必然有 \\(p_g(x, y) =0\\) 成立，反之亦然。 因而我们就有，对于任意 \\(x \\in \\mathbb{R}^2\\) ， \\[V(G, D^*)= \\int_x p_d(x) log \\frac{p_{d}(x)}{p_{d}(x) + p_{g}(x)} + p_g(x)log \\frac{p_{g}(x)}{p_{d}(x) + p_{g}(x)} dx \\\\ = \\int_x p_d(x) log (1) + p_g(x)log (1) dx = 0 \\\\ \\] 此时，损失函数恒为常量，无法继续指导生成器 \\(G(x)\\) 的优化。即此时出现了梯度消失的问题。 Wasserstein距离 为了弥补JS散度的局限性，我们需要一种全新的”分布间距离“的度量来进行优化，即使用Wasserstein距离，也被称为“推土机距离”（Earth-Mover），它定义如下： \\(W({P}_d, {P}_g) = inf_{\\gamma \\in \\Pi({P}_d, {P}_g)} {E}[||x - y||] \\\\\\) 这样数学形式的刻画可能会让人看得颇为一头雾水，我们逐步来分析解释它。 其中， \\(\\Pi({P}_d, {P}_g)\\) 代表一个 \\({P}_d, {P}_g\\) 构成的联合分布的集合，且这个集合中的所有联合分布必须满足其边际分布分别为 \\({P}_d, {P}_g\\) 。 \\(||x-y||\\) 是两个分布所在空间 \\(\\mathbb{R}^n\\) 中两点的欧式距离。 我们可以将 \\(\\Pi({P}_d, {P}_g)\\) 中的元素理解为一种“概率的搬运方案”。 而 \\(\\gamma\\) 是上述集合中的一个联合分布，可以使得任意两点的欧式距离期望最小，即将一个分布搬运为另外一个分布的最小开销。 此时，我们再重新观察上面的场景，当概率分布式为两条平行线上的均匀分布时，显然，最佳方案就是直接与x轴平行地进行概率搬运，对应为： \\(W(P_0, P_\\theta) = |\\theta|\\) 。此时，即使两个分布完全没有重叠部分，我们仍然能通过优化Wasserstein距离来实现两个概率分布之间的距离优化。 可以给出证明的是，就像JS散度一样，Wasserstein距离收敛于0时，两个分布也完全一致。 固然，通过Wasserstein距离优化GAN的想法颇为\"美好\"，不过，找到\"最优搬运方案\"的优化问题却是难事，在实现层面上，我们难以直接计算Wasserstein距离。不过，基于对偶理论可以将Wasserstein距离变换为积分概率度量IPM框架下的形式，来方便我们进行优化。 IPM也是用于衡量两个分布之间的距离，它的想法是寻找某种限制下的函数空间 \\(\\mathbb{F}\\) 中的一个函数 \\(f(·)\\) ，使得对任意位置两个分布的差异最大： \\[d_F(p, q) = sup_{f \\in F} \\mathbb{E}_{x \\sim P}[f(x)] - \\mathbb{E}_{x \\sim Q}[f(x)] \\\\\\] 对于Wasserstein距离而言，则变为： \\[W(p, q) = sup_{||f||_L \\le 1} \\mathbb{E}_{x \\sim P}[f(x)] - \\mathbb{E}_{x \\sim Q}[f(x)] \\\\\\] 因而，在函数 $ f(·)$ 满足Lipschitz约束的函数空间中，即 \\(||f(x) - f(y)|| \\le K||x - y||\\) ，找到最佳的函数 \\(f(·)\\) ，该情况下上式的结果则为Wasserstein距离。 这个函数 \\(f(·)\\) 难以求解，但我们可以用神经网络来拟合它。需要注意的是，从此开始，GAN的 \\(D\\) 就不再是先前我们认为的“真假判别器”了，它的意义变成了一个距离的度量。此时，GAN的生成器并不改变仍然生产图片，对生成器的训练则是减小与真实分布的Wasserstein距离，判别器 \\(D\\) 负责给出真实图像和生产图像样本之间的Wasserstein距离，相应的，在固定生成器优化判别器时，化则变为了寻找函数空间 \\(\\mathbb{F}\\) 中最佳的 \\(f(·)\\) 。 下面的图就可以体现传统GAN的判别器梯度和WGAN的判别器梯度的区别 WGAN便有效解决了某些情况下传统GAN的梯度消失的问题","categories":[{"name":"Stanford CS236深度生成模型","slug":"Stanford-CS236深度生成模型","permalink":"https://jia040223.github.io/categories/Stanford-CS236%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}],"tags":[{"name":"生成模型","slug":"生成模型","permalink":"https://jia040223.github.io/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}]},{"title":"Normalizing Flows","slug":"Normalizing Flows","date":"2024-09-17T10:18:14.000Z","updated":"2024-10-08T00:11:51.158Z","comments":true,"path":"2024/09/17/Normalizing Flows/","permalink":"https://jia040223.github.io/2024/09/17/Normalizing%20Flows/","excerpt":"","text":"本学习笔记用于记录我学习Stanford CS236课程的学习笔记，分享记录，也便于自己实时查看。 引入 生成模型模型的目的是让得到的数据分布 \\(P_{\\theta}\\) 与真实的数据分布 \\(P_{data}\\) 相同，也就是需要通过给定的样本来建模对应的分布，使得输入经过该模型后可以生成与给定样本类似的新样本。在这种意义下，评估的最佳方式便是使用极大似然估计，然而VAEs的做法导致计算似然十分复杂，所以我们只能选择计算似然的下界，也就是ELBO。 不妨思考一下，VAEs无法计算似然的原因是什么。不难发现，关键在于需要对所有的潜变量 \\(z\\) 进行积分。所以假如我们有一个可逆映射，使得潜变量 \\(z\\) 和数据 \\(x\\) 之间的是一一对应的，那我们便可以很轻松计算似然了。 Normalizing Flows正是这么做的。但可逆映射意味着潜变量 \\(z\\) 的维度需要和数据 \\(x\\) 的维度一致，所以我们无法利用 \\(z\\) 进行压缩。 简介 正则化流（Normalizing Flow）是一种可逆生成模型，用于将一个原始分布通过学习的变换映射到另一个已知的概率分布。它可以将数据从原始分布转换为目标分布，从而实现数据的生成和采样。 在正则化流中，我们定义一个变换函数，它将输入样本从原始分布映射到目标分布。这个映射是一个可逆函数，确保转换是可逆的，也就是说，在给定目标分布样本的情况下，可以逆向计算出原始分布的样本。这个变换函数通常由一系列的可逆操作组成，每个操作都是可逆的，并且通过组合这些操作可以得到整个变换。常用的可逆操作包括仿射变换、尺度变换、平移变换等。 原理 变量替换 变量替换的形式如下： $ p_{X}(X)=p_{Z}(f(X))|det~J(f(X))|$ \\(Z=f(X)\\) 是一个可逆的变换 \\(J(f(X))\\) 是 \\(f(X)\\) 的雅可比行列式 如何理解呢：即给出一个 \\(X\\) ，使用一个可逆变换 \\(f(\\cdot)\\) 将 \\(X\\) 变为 \\(Z\\) ，那么 \\(p(X)、p(Z)\\) 这两个分布之间相差的就是这样一个雅可比行列式。 1.png 流的组合 基本原理：可导的可逆的函数在进行组合后依然是一个可导且可逆的函数 标准化方向： \\(f=f_{1}\\circ f_{2}\\circ....f_{N}\\) 采样构造概率的方向： \\(g=g_{N} \\circ g_{N-1} \\circ .... \\circ g_{1}\\) 2.png 这种流动的感觉就是标准化流这个名字的由来。 而由 \\(p_{X}(X)=p_{Z}(f(X))|det~J(f(X))|\\) 可知，上面组合出来的 \\(f\\) 的雅可比行列式刚好可以表示为每一个 \\(f_{i}\\) 的雅可比行列式相乘再求行列式。 \\(det~J(f)=det\\prod_{i=1}^{N}J(f_{i})=\\prod_{i=1}^{N}det~J(f_{i})\\) 因为每一个样本都是独立同分布采样出来的，所以它的log likelihood就是把他们的每一个log likelihood加起来。由于做过变量代换，就可以把它变成我们知道的非常简单的分布加上剩下的log 雅可比行列式的和。 3.png 计算 通过最大似然估计，我们便可以训练模型了。但问题在于，如何构建这种可逆映射和如何让雅可比行列式方便计算。因为对于一般的雅可比行列式的计算复杂度是 \\(O(n^3)\\) ，但是我们可以构造半三角的雅可比行矩阵，这样行列式的计算复杂度只有 \\(O(n)\\) 了 NICE: Non-linear Independent Components Estimation NICE的目标是找到一个transformation \\(z=f(x)\\) , 将数据映射到一个新的空间中; 这个空间中的 \\(z\\) 的各个分量 \\(z_d\\) 之间都是独立的, 即 \\(p_\\theta(z)=\\prod_d p_{\\theta_d}(z_d)\\) .在这种\"各分量独立\"的假设下, 模型会自发地学习\"most important factors of variation\"; 否则, 比如 \\(h_1\\) 和 \\(h_2\\) 之间不独立, 那么就浪费了一部分建模能力, 从而无法达到最好的建模效果. 通过 \\(z\\) 的先验分布和 \\(x=f^{-1}(z)\\) , 可以实现 \\(x\\) 的生成(采样)。一般可以假定 \\(z\\) 的分布满足标准高斯分布。 映射构造(Additive coupling layer) 如何构造构造半三角的雅可比行矩阵呢？NICE给出的方法是： \\(z_{1\\sim d} = x_{1\\sim d}\\) \\(z_{ {d\\sim D} } = x_{ {d\\sim D} } + u_{\\theta}(x_{ {1\\sim d} })\\) 这个变换的雅克比矩阵为 \\[ \\frac{\\partial z}{\\partial x}=\\left[ \\begin{array}{cc} I_d &amp; \\bar{0} \\\\ [\\frac{\\partial u_\\theta}{\\partial x_{1\\sim d} }] &amp; I_{n-d} \\\\ \\end{array} \\right] \\] 这个映射的逆变换也很简单，为 \\(x_{1\\sim d} = z_{1\\sim d}\\) \\(x_{ {d\\sim D} } = z_{ {d\\sim D} } - u_{\\theta}(z_{ {1\\sim d} })\\) Combining coupling layers 事实上, 这个 \\(f\\) 是要用很多层叠在一起得到的, 即 \\(f=f_L \\circ ... \\circ f_2 \\circ f_1\\) 。 在堆叠coupling layer的时候, 注意到每个变换有一部分输入是不变的。这样才能让所有部分都能得到变换. 即, 第一层 \\(z_1=x_1\\) , 变 \\(x_2\\) , 那么第二层就 \\(z_2=x_2\\) , 变 \\(z_1\\) . 另外, 堆叠后的雅克比行列式为 \\[ \\left|\\det \\frac{\\partial z}{\\partial x} \\right| = \\left|\\det \\frac{\\partial f_L(x)}{\\partial f_{L-1}(x)}\\right| \\cdot \\left|\\det \\frac{\\partial f_{L-1}(x)}{\\partial f_{L-2}(x)}\\right| \\cdot \\ldots \\cdot \\left|\\det \\frac{\\partial f_2(x)}{\\partial f_1(x)}\\right| \\] 这些行列式的绝对值为1。 Allowing scaling 因为每个行列式的绝对值都是1, 因此 \\(f\\) 是volume preserving（体积不变的）的. 为了消除这个限制, 在 \\(f_L\\) 后又乘了一个diagonal scaling matrix \\(S\\) , 即 \\(z=S \\cdot f_{1, ...,L}(x)\\) . 这样既可以让一些重要特征又更大的变化范围, 又可以让一些不重要的特征减小变化范围(降维). 所以最后目标函数为 \\(\\log p_X(x)=\\sum_{i=1}^D [\\log p_{H_i}(f_i(x)) + \\log |S_{ii}|]\\) Density Estimation Using Real NVP Real NVP将NICE中的每一层的映射改为如下: \\(\\begin{aligned} z_{1:d}&amp;=x_{1:d}\\\\ z_{d+1:D} &amp;=x_{d+1:D} \\odot exp(s(x_{1:d})) +t(x_{1:d}) \\end{aligned}\\) 逆变换为 \\(\\begin{aligned} x_{1:d}&amp;=z_{1:d}\\\\ x_{d+1:D} &amp;=(z_{d+1:D}- t(x_{1:d})) \\odot exp(-s(x_{1:d})) \\end{aligned}\\) 这个变换的雅克比矩阵为 \\[ \\frac{\\partial z}{\\partial x}=\\left[ \\begin{array}{cc} I_d &amp; \\bar{0} \\\\ \\frac{\\partial z_{d+1:D} }{\\partial x_{1:d} } &amp; diag(exp(s(x_{1:d}))) \\\\ \\end{array} \\right] \\] 其中 \\(diag(exp(s(x_{1:d})))\\) 是将 $ exp(s(x_{1:d}))$ 这个向量展开为对角矩阵. 这个雅克比矩阵的log-determinant为 \\[\\prod_{i=1}^d \\log \\exp(s(x_{1:d}))=\\sum_{i=1}^d s(x_{1:d})\\] 其中没有任何 \\(s\\) 和 \\(t\\) 行列式的计算, 因此二者可以任意复杂且hidden layer采用不同于输入的维度. 这样我们便完成了一个更加复杂的构造，同时它的表现也自然比NICE更好。","categories":[{"name":"Stanford CS236深度生成模型","slug":"Stanford-CS236深度生成模型","permalink":"https://jia040223.github.io/categories/Stanford-CS236%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}],"tags":[{"name":"生成模型","slug":"生成模型","permalink":"https://jia040223.github.io/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}]},{"title":"VAEs","slug":"VAEs","date":"2024-09-13T11:31:24.000Z","updated":"2024-10-08T00:28:02.556Z","comments":true,"path":"2024/09/13/VAEs/","permalink":"https://jia040223.github.io/2024/09/13/VAEs/","excerpt":"","text":"本学习笔记用于记录我学习Stanford CS236课程的学习笔记，分享记录，也便于自己实时查看。 潜变量 对于生成模型，我们可以试图寻找一组潜变量z，这个潜变量可以有具体含义，例如对于人脸生成模型的眼睛，鼻子，嘴巴等。通过修改这些潜变量我们可以得到不同风格的生成对象。但是对于图片或者自然语言而言，人为指定这种潜变量极为困难。 所以我们可以并不人为指定潜变量的含义，例如无监督学习的GMM（高斯混合聚类）就并没有指定每个类别具体的含义。但高斯混合聚类人为指定了类别的数量，这对于生成模型也是很难实现定义的。 对于GMM来说，事实上是定义了一组离散的潜变量，在每个潜变量下的数据分布服从高斯分布，它可以给我们一些启示，虽然每个类别的概率只是定义为正态分布，但它组合之后可以形成非常复杂的概率分布。 所以不妨我们可以设定有无穷多个高斯聚类的组合，即设定潜变量 z 是一个连续的随机变量，而每个潜变量 z 的值对应于一个高斯分布，事实上这也正是VAEs做的 核心思想 VAE 的目标是学习一个生成器，将随机向量 $z \\in R^d$ 映射到 $x \\in R^D$ , 使得 $x$ 的分布尽可能接近真实数据的分布。 这里的 $z$ 其实就是上面提到的潜变量，他是一个连续的随机变量，实践中一般定义为服从高斯分布。而对于每个z的值，我们假设x的分布是满足均值为 $\\mu(z)$ ，协方差矩阵为 $\\Sigma(z)$ （可以通过神经网络进行学习）的高斯分布。理论上这样的组合可以逼近任意的概率分布。 当然PPT中的 $z\\sim N(0, I)$ 只是一个例子，也可以有更复杂的定义，但在实践中一般使用标准正态分布。 生成和训练 损失函数 对于一个生成模型来说，生成和评估的难易很大程度上决定了它的实用性和价值。对于上面VAE的假设来说，生成是很简单。即假设我们已经知道了 $p(x|z)$ ，我们只需要先采样 $z$ ，再采样 $x$ 就能得到数据。 但是评估并不容易，这意味着模型的训练可能是一个棘手的问题。对于评估，既然是衡量两个分布的相似度，我们能否直接用各种散度（如 KL 散度）作为损失函数呢？当然可以。在蒙特卡洛抽样（Monte Carlo Sampling）下，最小化KL散度就是最大似然估计。 那么我们的目标是 $θ_∗=argmax \\sum_{i=1}^{n} logp_θ​(x_i) $ ，注意到 $\\sum logP_\\theta(x) =\\sum log(\\sum q(z)P_\\theta(x|z))$ 对于等式右边的计算是非常复杂的，因为 $z$ 的取值理论上具有无穷多个 所以我们需要对这个公式进行简化，注意到 $$ \\begin{align} P_\\theta(x) &amp;= \\sum (q(z) \\frac{p_\\theta(z, x)}{q(z)}) \\nonumber \\ &amp;= E_{z \\sim q(z)}\\left(\\frac{p_\\theta(z,x)}{q(z)}\\right) \\end{align} $$ 通过蒙特卡洛抽样（Monte Carlo Sampling），我们可以从 $q(z)$ 中采样若干数据点，然后进行平均即可估计 $P_\\theta(x)$ 的值。但很可惜，我们无法通过蒙特卡洛抽样来估计 $log(P_\\theta(x))$ , 因为 $log(E_{z \\sim q(z)}(\\frac{p_\\theta(z,x)}{q(z)})) \\ne E_{z \\sim q(z)}(log(\\frac{p_\\theta(z,x)}{q(z)}))$ 但幸运的是，对于对数函数是一个严格的凹函数，所以对于凹函数来说有 $log(px + (1-p)x^{‘}) \\geq plogx +(1-p)logx^{’}$ ，进一步扩展便就是著名的琴生不等式： 琴生不等式 因此 $log(E_{z \\sim q(z)}(\\frac{p_\\theta(z,x)}{q(z)})) \\geq E_{z \\sim q(z)}(log(\\frac{p_\\theta(z,x)}{q(z)}))$ 所以我们可以通过这种方法来估计似然的下限，即上面不等号的右边，叫做ELBO（Evidence Lower Bound） 至于这个界限有多紧，我们对 $logP(x)$ 进行一下推导，就能得到它们之间相差的便是 $D_{KL}(q(z)||p(z|x;\\theta)$ ，也就是说当 $q(z)$ 与我们的后验分布越接近，这个界限越紧。 其实这里的推导就是EM算法里面的推导，最大化ELBO的过程就是对应于EM算法里面的M步（后续有机会可能也会写一写）。非常可惜的是，EM 算法无法直接应用于此，因为 E-step 要求我们能够表达出后验分布 $p_\\theta(z|x)$ ，但没关系，如果我们能够最大化ELBO，也能保证似然的下限被最大化。 问题似乎解决了，但值得注意的是，ELBO 是关于函数 $q$ 的泛函，也就是说 $q$ 可以取任意函数，这并不好直接优化。为了解决这个问题，我们可以将 $q(z)$ 限制为以 $\\phi$ 为参数的某可解分布族 $q_\\phi(z|x)$ ，这样优化变量就从函数 $q$ 变成了参数 $\\phi$ 。不过，由于我们限制了 $q$ 的形式，所以即便能求出最优的参数 $\\phi$ ，也大概率不是 $q$ 的最优解。显然，为了尽可能逼近最优解，我们应该让选取的分布族越复杂越好。 那么这里有一个小问题——为什么 $q(z)$ 参数化后写作 $q_\\phi(z|x)$ 而不是 $q_\\phi(z)$ ? 首先， $q$ 本来就是我们人为引入的，它是否以 $x$ 为条件完全是我们的设计，且并不与之前的推导冲突；其次，ELBO与似然当 $q(z)=p_θ(z|x)$ 时是完全等价的，可见对于不同的 $x$ ，其 $q(z)$ 的最佳形式是不同的，所以这么设定有利于减少ELBO与似然的距离。 在VAE中 的 $p_θ(x|z)$ 和 $q_\\phi(z|x)$ 都由神经网络表示，因此我们用梯度下降来最大化 ELBO 即可。即对ELBO取负数就是最终的损失函数。 注意到这样的形式中并没有 $p_θ(x|z)$ 一项，我们只需要稍微变化一下： $$ \\begin{align} L(x;\\theta, \\phi) &amp;= \\sum q_{\\phi} (z|x)\\left[\\log(p_{\\theta}(z,x;\\theta)) - \\log(q_{\\phi}(z|x))\\right] \\ &amp;= \\sum q_{\\phi} (z|x)\\left[\\log(p_{\\theta}(z,x;\\theta)) - \\log(p(z)) + \\log(p(z)) - \\log(q_{\\phi}(z|x))\\right] \\ &amp;= \\sum q_{\\phi} (z|x)\\left[\\log(p_{\\theta}(x|z)) - \\log\\left(\\frac{q_{\\phi}(z|x)}{p(z)}\\right)\\right] \\ &amp;= E_{z \\sim q_{\\phi}(z|x)}\\left[\\log(p_{\\theta}(x|z))\\right] - D_{KL}(q_{\\phi}(z|x) || p(z)) \\end{align} $$ 这里就把我们的目标分成了两项： 第一项是重构项，要求我们尽可能重构数据本身 第二项是正则项，要求我们的后验与先验接近 所以可以看到，它与自动编码器最大的区别在于有第二项，这保证了隐藏变量 $z$ 的分布，从而我们可以从先验中对 $z$ 取样从而进行生成。换句话来说，VAEs是对潜变量进行了正则化的自动编码器，因为我们知道了潜变量 $z$ 的分布形式，所以它能够用于生成。 按照蒙特卡洛抽样（Monte Carlo Sampling），理论上求这个期望需要对每个样本多次采样进行计算，最后平均。但在具体实践中，往往采样一次进行计算就行。 梯度计算细节：重参数化技巧 有一个细节是现在 $z$ 是从 $q_\\phi(z|x)∼N(μ_ϕ(x)，diag(\\sigma^{2}_ϕ(x)))$ 中采样的，但梯度无法经过采样传播到参数 $\\phi$ 。但其实解决方法很简单，对于高斯函数，只需要先从 $N(0,I)$ 中采样 $\\epsilon$ 再计算 $z=μ_ϕ(x)+\\epsilon⋅σ_ϕ(x)$ 即可。 这种技巧也叫做重参数化技巧，其最开始应该是在强化学习中出现的，后面有时间也可以写一写。","categories":[{"name":"Stanford CS236深度生成模型","slug":"Stanford-CS236深度生成模型","permalink":"https://jia040223.github.io/categories/Stanford-CS236%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}],"tags":[{"name":"生成模型","slug":"生成模型","permalink":"https://jia040223.github.io/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}]}],"categories":[{"name":"更新日志","slug":"更新日志","permalink":"https://jia040223.github.io/categories/%E6%9B%B4%E6%96%B0%E6%97%A5%E5%BF%97/"},{"name":"Stanford CS236深度生成模型","slug":"Stanford-CS236深度生成模型","permalink":"https://jia040223.github.io/categories/Stanford-CS236%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"},{"name":"机器学习","slug":"机器学习","permalink":"https://jia040223.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"更新日志","slug":"更新日志","permalink":"https://jia040223.github.io/tags/%E6%9B%B4%E6%96%B0%E6%97%A5%E5%BF%97/"},{"name":"生成模型","slug":"生成模型","permalink":"https://jia040223.github.io/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"},{"name":"机器学习","slug":"机器学习","permalink":"https://jia040223.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"概率论与数理统计","slug":"概率论与数理统计","permalink":"https://jia040223.github.io/tags/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/"}]}