{"meta":{"title":"Serendipity's Blog","subtitle":"","description":"","author":"Serendipity","url":"https://jia040223.github.io","root":"/"},"pages":[{"title":"关于我","date":"2024-10-08T00:34:06.588Z","updated":"2024-10-08T00:34:06.588Z","comments":false,"path":"about/index.html","permalink":"https://jia040223.github.io/about/index.html","excerpt":"","text":"欢迎来到我的博客！这个博客是我个人的学习和探索之旅的记录，我希望通过它分享我的想法和见解。无论是技术、理论还是生活中的点滴，我都会在这里进行更新。如果你对我的内容有任何疑问或建议，欢迎通过 GitHub 与我联系。期待与你的交流与互动！ 个人简介 教育经历 高中：石门县第一中学 本科：中国科学院大学（计算机科学与技术专业） 目前感兴趣的方向 大模型 深度学习 计算机视觉 关于博客 学习笔记 我会持续更新我目前正在学习的内容和笔记，包括但不限于深度学习与大模型，计算机视觉，微积分与线性代数等。 项目地址：repository 科研日志 我会记录一些科研上的心得体会，分享一些我在科研中的经验和教训。 生活记录 我会记录一些好玩的生活片段，同样包括但不限于旅游日志、生活琐事和所悟所想，分享是一种很好的生活调味剂，希望你会喜欢。 联系方式 Email：jiachenghao21@mails.ucas.ac.cn"}],"posts":[{"title":"[Probabilistic Machine Learning]: Fundamentals-Statistics","slug":"Fundamentals-Statistics","date":"2024-10-14T06:17:48.000Z","updated":"2024-10-14T12:38:03.704Z","comments":true,"path":"2024/10/14/Fundamentals-Statistics/","permalink":"https://jia040223.github.io/2024/10/14/Fundamentals-Statistics/","excerpt":"","text":"本学习笔记用于记录我学习Probabilistic Machine Learning的学习笔记，分享记录，也便于自己实时查看。 前面Probability部分重点是关注给定参数 $\\theta$ 后，数据 $D$ 的分布，即 $P(D|\\theta)$ ，而Statistics部分则是关注给定数据分布下，参数 $\\theta$ 的概率，即 $P(\\theta|D)$ 。 一、贝叶斯统计 贝叶斯统计也是比较熟悉了，主要就是用贝叶斯公式进行计算后验： 这里 $P(\\theta) $ 叫做先验， $P(\\theta|D)$ 是后验， $P(D|\\theta)$ 叫做似然。 书中以抛硬币实验来讲述了贝叶斯统计的众多概率，这里简单总结一下： Prior ：均匀分布或者Beta分布，抛硬币我们可以选择Beta分布来指定更强的先验： $$p(θ)=Beta(θ∣α,β)∝θ ^{α−1} (1−θ) ^{β−1}$$ Posterior： 在Beta分布先验条件下可计算得到后验为： $$p(θ∣D)∝θ^{N_1}​(1−θ)^{N_0}​⋅θ^{α−1}(1−θ)^{β−1}=Beta(θ∣α+N_1​,β+N_0​)$$ MAP 估计：即让后验最大 $$\\hat{\\theta}_{\\text{MAP}} = \\frac{\\alpha + N_1 - 1}{\\alpha + N_1 - 1 + \\beta + N_0 - 1}$$ 用均匀分布先验则和MLE得到的结果一致。 Posterior Mean：很多时候会使用后验的均值而非峰值作为参数，: $$\\hat{\\theta} = \\int \\theta \\cdot p(\\theta|D) d\\theta$$ Posterior Variance：表达估计的不确定性，对于抛硬币可得到标准差 $$\\sigma = \\sqrt{V[\\theta|D]} \\approx \\sqrt{\\frac{\\hat{\\theta}(1 - \\hat{\\theta})}{N}}$$ 所以随着样本量 $N$ 的增大，不确定性以 $\\frac{1}{\\sqrt{N}}$ ​ 的速度下降。不确定性（方差）在 $\\hat{\\theta} = 0.5$ 时达到最大，在 $\\hatθ$ 接近0或1时达到最小。这表明，确定一个硬币偏向比确定它是公平的要容易得多。 Credible Intervals：置信区间，后验分布的 100(1 - α)% 置信区间定义 $$C_\\alpha(D) = (l, u) : P(l \\leq \\theta \\leq u|D) = 1 - \\alpha$$ Posterior Predictive Distribution：假设我们希望预测未来的观测值，贝叶斯最优方法是通过边缘化未知参数来计算后验预测分布： $$p(y|D) = \\int p(y|\\theta) p(\\theta|D) d\\theta$$ 有时计算该积分可能会很困难，这时可以使用点估计方法，选择一个参数估计值 $\\hat{\\theta} = \\delta(D)$ ，例如 MLE 或 MAP，从而近似为： $$p(y|D) \\approx p(y|\\hat{\\theta})$$ Marginal Likelihood：对于优化没有影响，主要在于对模型的选择上： $$p(D|M) = \\int p(\\theta|M) p(D|\\theta, M) d\\theta \\quad $$ 还提到了一个定理**de Finetti’s theorem（德·芬尼蒂定理）：**如果数据是可交换的，那么必然存在一个隐藏的随机变量 $\\theta$ ，数据在给定 $\\theta$ 的条件下是独立同分布的。这个定理为贝叶斯方法提供了理论基础。 二、频率学派统计 与贝叶斯统计不同，频率学派不将参数当作随机变量，而是依赖采样分布来表示不确定性。它通过反复采样来评估数据中的随机性和不确定性，而不是使用先验分布和后验分布。核心思想是重复实验的假设：通过观察如果在不同的数据集上重复实验，估计的量（例如参数）会如何变化，这种变化构成了不确定性的依据。 这个也很熟悉了，简单来说就是频率学派认为参数是一个值，通过不断地实验就能去估计这个值。虽然这个有一定的缺点，但其一些准则在实践中也是被广泛使用的。 1. Sampling distributions 采样分布是对某个估计器（如最大似然估计，MLE）的结果变化进行的描述。 举例来说，假设从一个真实模型 $p(x|\\theta^*)$ 中采样多个数据集 $D^{(s)}$ ，然后对每个数据集应用估计器来得到参数估计 $\\hat{\\theta}(D^{(s)})$ 。通过让数据集的数量 $S$ 趋向无穷，我们可以得到估计器的采样分布。这个分布反映了在不同的样本下，参数估计的变化情况。 2. Bootstrap 自助法 当估计器比较复杂或者样本量较小的时候，可以使用Bootstrap方法来近似采样分布。自助法的核心是通过从原始数据集中随机采样生成多个伪数据集，然后计算每个伪数据集的参数估计，最终得到估计值的经验分布。主要有两种方法： 参数自助法假设我们知道参数 $\\theta^$ ，我们可以生成伪数据集并计算估计值。但现实是 $\\theta^$ 是未知的，所以我们使用从数据中估计出的参数 $\\hat{\\theta}$ ，这就称为“参数自助法”。 另一种是非参数自助法，它不依赖于特定的生成模型，而是直接从原始数据集中进行有放回的采样，这样每个新生成的数据集与原始数据集有相同的大小，但通常会有重复数据点。 3. 渐近正态性（Asymptotic Normality） 当样本量足够大时，最大似然估计（MLE）的采样分布会趋向于正态分布。这称为MLE的渐近正态性。在数学上，它表述为： $$ \\sqrt{N}(\\hat{\\theta} - \\theta^) \\rightarrow N(0, F(\\theta^)^{-1}) $$ Fisher 信息矩阵其中 $F(\\theta^*)$ 是费舍尔信息矩阵。 费舍尔信息矩阵衡量的是似然函数在真参数处的曲率，表明数据中包含的“信息量”。渐近正态性意味着，当样本量 $N$ 趋于无穷时，估计值的分布会收敛于一个以真参数 $\\theta^*$ 为中心的高斯分布。 4. Fisher 信息矩阵 **Fisher 信息矩阵（Fisher Information Matrix, FIM）**与对数似然函数的曲率密切相关。这一矩阵在频率学派统计中有重要作用，主要用于刻画最大似然估计（MLE）的采样分布。此外，Fisher 信息矩阵在贝叶斯统计中也有应用，例如推导 Jeffreys 的无信息先验，以及在优化问题中作为自然梯度下降的一部分。 定义如下： score function ： $$s(\\theta)\\equiv\\nabla_\\theta\\log p(x|\\theta)$$ Fisher 信息矩阵 ： $$F(\\theta) \\equiv \\mathbb{E}{x \\sim p(x|\\theta)} \\left[ \\nabla\\theta \\log p(x|\\theta) \\nabla_\\theta \\log p(x|\\theta)^T \\right] $$ 其第 $i,j$ 项为： $$F_{ij} = \\mathbb{E}_{x \\sim \\theta} \\left[ \\frac{\\partial}{\\partial \\theta_i} \\log p(x|\\theta) \\frac{\\partial}{\\partial \\theta_j} \\log p(x|\\theta) \\right] $$ 可以看到Fisher 信息矩阵与负对数似然函数（NLL, Negative Log Likelihood）有关系： $$\\text{NLL}(\\theta) = - \\log p(D|\\theta) $$ 我们有如下定理： 定理 4.1 如果 $\\log p(x|\\theta)$是二阶可微的，并且在某些正则条件下，Fisher 信息矩阵等于 NLL 的期望 Hessian 矩阵： $$ F(\\theta){ij} = \\mathbb{E}{x \\sim \\theta} \\left[ \\frac{\\partial}{\\partial \\theta_i} \\log p(x|\\theta) \\frac{\\partial}{\\partial \\theta_j} \\log p(x|\\theta) \\right] = - \\mathbb{E}_{x \\sim \\theta} \\left[ \\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} \\log p(x|\\theta) \\right] $$ 然后书上给了一些常见分布的例子 二项分布的 FIM： $F(\\theta) = \\mathbb{E}_{x \\sim \\theta}[-s’(\\theta|x)] = \\frac{n}{\\theta(1 - \\theta)}$ 单变量高斯分布的 FIM： $F(\\theta) = \\begin{pmatrix} \\frac{1}{v} &amp; 0 \\ 0 &amp; \\frac{1}{2v^2} \\end{pmatrix}$ 逻辑回归的 FIM： $F(w) = \\mathbb{E}{p(y|X,w,\\lambda)}[\\nabla^2 L(w)] = X^T \\Lambda X + \\lambda I $ ，其中 $\\Lambda{nn} = \\sigma(w^T x_n)(1 - \\sigma(w^T x_n))$ 。 指数族分布的 FIM： $F_\\eta = \\text{Cov}[T(x)]$ 5. 频率学派的Counterintuitive properties 首先是**频率主义的置信区间，**它基于抽样分布来估计参数的不确定性。其定义是，如果重复抽取样本并计算每个样本的置信区间，那么有 95% 的区间会包含真实参数。但对于一个具体的样本，无法说参数有 95% 的概率落在置信区间内。换句话说，频率主义认为参数是固定的，数据是随机的，所以无法给出“参数在此区间的概率”。 贝叶斯方法则把数据固定，参数看作是随机的。因此，贝叶斯的可信区间给出了参数在某区间内的概率。这是人们在直觉上通常更关心的问题：已知数据后，参数落在某个范围的概率。 文中提到一个具体的例子： 然后就是**p 值的误导性，**p 值是指在原假设（H0）成立时，观察到某个统计量或更极端结果的概率。频率主义的假设检验通过计算 p 值来决定是否拒绝原假设，通常认为 p 值很小就意味着原假设不太可能成立。 问题是p 值经常被错误地解释为“原假设为真的概率”，但实际上它只是给出了在原假设成立的情况下，观察到数据的概率。它并没有告诉我们在看到数据后原假设是否成立，或者备择假设（H1）是否更有可能成立。 文中用了一个类比来说明 p 值的误导性。假设“如果一个人是美国人，他大概率不是国会议员”，我们观测到某人是国会议员，但这并不能推导出“这个人很可能不是美国人”。 这是一个典型的错误推理，类似于依赖 p 值来判断假设的真实性。相反，贝叶斯方法会使用贝叶斯定理结合数据推导出假设的后验概率，更符合人们的直觉。 三、共轭先验 如果先验分布 $p(\\theta)$ 属于某个参数化家族 $F$ ，并且后验分布 $p(\\theta|D)$ 也在该家族中，则称 $p(\\theta)$ 为 $p(D|\\theta)$ 的共轭先验。这意味着贝叶斯更新后，分布保持在同一个家族中，便于计算。 书中详细介绍了常见分布的共轭先验，这里仅做总结： 3.1 二项分布 **共轭先验：**贝塔分布 $$p(\\theta) = \\text{Beta}(\\theta | \\alpha, \\beta)$$ **更新公式：**在观察到 $k$ 次成功和 $n$ 次试验后，后验分布为： $$p(\\theta | k, n) = \\text{Beta}(\\theta | \\alpha + k, \\beta + n - k)$$ 3.2. 多类分布 **共轭先验：**狄利克雷分布： $$p(\\boldsymbol{\\theta}) = \\text{Dirichlet}(\\boldsymbol{\\theta} | \\boldsymbol{\\alpha})$$ **更新公式：**如果观察到类别 $i$ 的次数为 $n_i$ ​，则后验分布为 $$p(\\boldsymbol{\\theta} | \\mathbf{n}) = \\text{Dirichlet}(\\boldsymbol{\\theta} | \\boldsymbol{\\alpha} + \\mathbf{n})$$ 其中 $\\mathbf{n} = (n_1, n_2, \\ldots, n_k)$ 表示每个类别的观察次数。 3.3 单变量高斯模型（Univariate Gaussian Model） 3.3.1 给定 $ \\sigma^2$ 的后验 **共轭先验：**另一个高斯分布： $$N(\\mu|m_0, \\tau_0^2)$$ **更新公式：**后验分布也为高斯分布，参数为： $$ \\begin{align} \\hat{\\tau}^2 &amp;= \\frac{1}{\\frac{1}{\\sigma^2} + \\frac{N}{\\tau_0^2}} \\ \\hat{m} &amp;= \\hat{\\tau}^2 \\left( \\frac{m_0}{\\tau_0^2} + \\frac{N\\bar{y}}{\\sigma^2} \\right) \\end{align} $$ 这里， $\\bar{y}$ 是样本均值。 3.3.2 给定 $μ$ 的后验 共轭先验为逆伽马分布： $$\\text{IG}(\\sigma^2|\\alpha_0, \\beta_0) \\propto (\\sigma^2)^{-\\alpha_0 - 1} \\exp\\left(-\\frac{\\beta_0}{\\sigma^2}\\right)$$ **更新公式：**后验分布也是逆伽马分布，参数为： $$ \\begin{align} &amp;\\hat{\\alpha} = \\alpha_0 + \\frac{N}{2} \\ &amp;\\hat{\\beta} = \\beta_0 + \\frac{1}{2}\\sum_{n=1}^{N}(y_n - \\mu)^2 \\end{align} $$ 3.3.3 对于均值 $\\mu$ 和方差 $\\sigma^2$ 的推断 **共轭先验：**正态-逆伽马分布（NIG）： $$ NIG(\\mu, \\sigma^2 | m, \\kappa, a, b) \\equiv N(\\mu | m, \\frac{\\sigma^2}{\\kappa}) IG(\\sigma^2 | a, b) $$ 一般就用正态-逆卡方分布（NIX）： $$ NI\\chi^2(\\mu, \\sigma^2 | m, \\kappa, \\nu, \\tau^2) \\equiv N(\\mu | m, \\frac{\\sigma^2}{\\kappa}) \\chi^{-2}(\\sigma^2 | \\nu, \\tau^2) $$ **更新公式：**后验参数更新： $$ \\begin{align} &amp;\\hat{m} = \\frac{\\kappa m + N\\hat{x}}{\\hat{\\kappa}} \\ &amp;\\hat{\\kappa} = \\kappa + N \\ &amp;\\hat{\\nu} = \\nu + N \\ &amp;\\hat{\\nu} \\hat{\\tau}^2 = \\nu \\tau^2 + \\sum_{n=1}^{N}(y_n - \\bar{y})^2 + \\frac{N\\kappa}{\\kappa + N}(m - \\bar{y})^2 \\end{align} $$ 方差的后验边际分布： $$p(\\sigma^2 | D) = \\chi^{-2}(\\sigma^2 | \\hat{\\nu}, \\hat{\\tau}^2)$$ 均值的后验边际分布： $$p(\\mu | D) = T(\\mu | \\hat{m}, \\frac{\\hat{\\tau}^2}{\\hat{\\kappa}}, \\hat{\\nu})$$ 3.4. 对于多变量高斯模型 3.4.1 在给定 Σ 的情况下推断 µ **共轭先验：**高斯分布: $$p(\\mu) = N(\\mu | m, V)$$ 后验分布： : $$p(\\mu | D, \\Sigma) = N(\\mu | \\hat{m}, \\hat{V})$$ 更新公式： $$ \\begin{align} &amp;\\hat{V}^{-1} = V^{-1} + N\\Sigma^{-1} \\ &amp;\\hat{m} = \\hat{V}(\\Sigma^{-1}(Ny) + V^{-1}m) \\end{align} $$ 3.4.2 在给定 µ 的情况下推断 Σ **共轭先验：**逆Wishart分布: $$p(\\Sigma) = IW(\\Sigma | \\Psi^{-1}, \\nu)$$ 后验分布： : $$p(\\Sigma | D, \\mu) \\propto IW(\\Sigma | \\hat{\\Psi}, \\hat{\\nu})$$ 更新公式： $$ \\begin{align} &amp;\\hat{\\nu} = \\nu + N\\ &amp;\\hat{\\Psi} = \\Psi + S_\\mu \\end{align} $$ 3.4.3 同时推断 Σ 和 µ 共轭先验： $$p(\\mu, \\Sigma) = N(\\mu | m, V) IW(\\Sigma | \\Psi^{-1}, \\nu)$$ 后验分布： $$p(\\mu, \\Sigma | D) \\propto |\\Sigma|^{-\\frac{N + \\nu + D + 2}{2}} \\exp\\left(-\\frac{1}{2} \\text{tr}(\\Sigma^{-1} M)\\right)$$ 其中 $M$ 是更新后的散点矩阵。 3.5 指数族模型 唯一存在共轭先验的分布族是指数族，具体如下： **共轭先验：**我们可以将先验分布写成与似然函数相似的形式： $$p(\\eta|\\tilde{\\tau}, \\tilde{\\nu}) = \\frac{1}{Z(\\tilde{\\tau}, \\tilde{\\nu})} \\exp\\left(\\tilde{\\tau}^T \\eta - \\tilde{\\nu} A(\\eta)\\right)$$ 其中， $\\tilde{\\nu}$ 是先验的强度， $\\frac{\\tilde{\\tau}}{\\tilde{\\nu}}$ ​ 是先验均值， $Z(\\tilde{\\tau}, \\tilde{\\nu})$ 是归一化因子。 后验分布： $$ \\begin{align} p(\\eta|D) &amp;= \\frac{p(D|\\eta) p(\\eta)}{p(D)} \\ &amp;= \\frac{h(D)}{Z(\\tilde{\\tau}, \\tilde{\\nu})p(D)} \\exp\\left( (\\tilde{\\tau} + s(D))^T \\eta - (\\tilde{\\nu} + N) A(\\eta)\\right) \\ &amp;= \\frac{1}{Z(\\hat{\\tau}, \\hat{\\nu})} \\exp\\left(\\hat{\\tau}^T \\eta - \\hat{\\nu} A(\\eta)\\right) \\end{align} $$ 其中： $$ \\begin{align} &amp;\\hat{\\tau} = \\tilde{\\tau} + s(D)\\ &amp;\\hat{\\nu} = \\tilde{\\nu} + N \\ &amp;Z(\\hat{\\tau}, \\hat{\\nu}) = \\frac{Z(\\tilde{\\tau}, \\tilde{\\nu})h(D)}{p(D)} \\end{align} $$ 我们看到，后验分布与先验分布具有相同的形式，只是更新了充分统计量和样本大小。后验均值为先验均值与经验均值（即最大似然估计）之间的组合： $$\\begin{align*} E[\\eta|D] &amp;= \\frac{\\hat{\\tau}}{\\hat{\\nu}} \\ &amp;= \\frac{\\tilde{\\tau} + s(D)}{\\tilde{\\nu} + N} \\ &amp;= \\frac{\\tilde{\\nu}}{\\tilde{\\nu} + N} \\frac{\\tilde{\\tau}}{\\tilde{\\nu}} + \\frac{N}{\\tilde{\\nu} + N} \\frac{s(D)}{N} \\ &amp;= \\lambda E[\\eta] + (1 - \\lambda) \\hat{\\eta}_{MLE} \\end{align*}$$ 其中， $\\lambda = \\frac{\\tilde{\\nu}}{\\tilde{\\nu} + N}$ ​。 边际似然： $$p(D) = \\frac{Z(\\hat{\\tau}, \\hat{\\nu}) h(D)}{Z(\\tilde{\\tau}, \\tilde{\\nu})}$$ **后验预测密度：**我们现在推导给定过去数据 $D = (x_1, …, x_N)$ 时，未来观测 $D’ = (x’1, …, x’{N’})$ 的预测密度，如下： $$\\begin{align*} p(D’|D) &amp;= \\int p(D’|\\eta) p(\\eta|D) d\\eta \\&amp;=h(D’) \\frac{Z(\\tilde{\\tau} + s(D) + s(D’), \\tilde{\\nu} + N + N’)}{Z(\\tilde{\\tau} + s(D), \\tilde{\\nu} + N)} \\end{align*}$$ 四、无信息先验 在缺乏领域特定知识时，我们不希望主观定义不合理的先验，于是我们便可以选择无信息先验客观。主要有如下几种： 1. 最大熵先验（Maximum entropy priors） 最大熵先验是一种不做过多假设的先验分布，适合在没有充足信息的情况下使用。通过最大化熵来选择先验，这种方法依赖于拉普拉斯提出的“不充分理由原则”，即当我们没有理由偏向某个特定值时，应选择“平坦”的分布。例如，对于伯努利分布的参数 θ（取值范围 [0,1]），最大熵先验是均匀分布。 我们也可以根据已知约束来定义最大熵先验，使其在满足这些约束的同时使得熵最大化。书中举了一个例子： 2. 杰弗里斯先验（Jeffreys priors） Jeffreys priors通过保证对参数化不敏感，即在不同的参数化方式下，后验分布不会改变。杰弗里斯先验的一个关键特性是对参数的变化保持不变，这意味着无论采用何种参数化方式，结果应该是一致的。 参数 $\\theta$ 的 Jeffreys Prior 为以下形式： $$p_{J}(\\theta) \\propto \\sqrt{\\mathcal{I}(\\theta)}$$ 其中， $\\mathcal{I}(\\theta)$ 是我们所熟知的Fisher信息量。证明如下： 例如对于伯努利分布，其杰弗里斯先验是 Beta(1/2, 1/2) 分布。 3. 不变性先验（Invariant priors） 不变性先验是指当我们知道某些不变性时，可以将其编码进先验中。例如： 平移不变先验：对位置参数的推断可以使用平移不变先验，这种先验在任何相同宽度的区间上都分配相同的概率质量。 尺度不变先验：对尺度参数的推断可以使用尺度不变先验，其满足任意比例缩放后保持相同概率质量。 4. 参考先验（Reference priors） 参考先验通过最大化数据集上的后验与先验之间的KL散度来定义。它旨在使先验尽可能远离所有可能的后验分布，从而保持非信息性。参考先验可以看作是对不同数据集的互信息最大化问题。对于一维情况，参考先验等同于Jeffreys priors，而在高维情况下，计算起来则更复杂。 五、层次先验（Hierarchical priors） 贝叶斯模型需要为参数 $\\theta$ 指定先验 $p(\\theta)$ ，而先验的参数（超参数 $ \\xi$ ）也是未知的。为了处理这种不确定性，我们可以对超参数 $\\xi$ 再定义一个先验，从而构建层次贝叶斯模型。这种模型的形式化表达为： $p(\\xi, \\theta, D) = p(\\xi) p(\\theta | \\xi) p(D | \\theta)$ 这表明数据 $D$ 通过参数 $\\theta$ 依赖于超参数 $ \\xi$ ，从而形成一个层次结构。 在实际问题中，如果我们有多个相关的数据集 $D_j$ ​，各个数据集有自己的参数 $\\theta_j$ ，那么分别独立估计每个 $\\theta_j$ ​ 可能会产生不可靠的结果，特别是当某个数据集较小时。层次模型可以通过共享超参数 $\\xi$ 来借用数据量大的群体的信息，帮助数据量小的群体进行更好的估计. 1. 层次二项模型的例子 问题背景：假设我们想估计不同群体中某种疾病的患病率，每个群体的样本量是 $N_j$ ​，阳性病例数是 $ y_j$。我们可以假设 $y_j$ 服从二项分布 $\\text{Bin}(N_j, \\theta_j)$ ，其中 $\\theta_j$ ​ 是该群体的患病率。 如果直接对每个群体单独估计 $\\theta_j$ ，特别是当样本量 $N_j$ 很小时，可能会导致不可靠的结果。比如，如果 $y_j = 0$ ，我们可能会估计 $\\hat{\\theta_j} = 0$ ，尽管实际的患病率可能更高。 解决方案：为了避免这种问题，可以假设所有的 $\\theta_j$ 不是独立的，而是从一个共同的 Beta 分布中抽取，即 $\\theta_j \\sim \\text{Beta}(a, b)$ 。这个假设允许我们通过共享的先验 $\\xi = (a, b)$ 来提高估计的可靠性。这种模型的联合分布可以写作： $$p(D, \\theta, \\xi) = p(\\xi) \\prod_{j=1}^{J} \\text{Beta}(\\theta_j | \\xi) \\prod_{j=1}^{J} \\text{Bin}(y_j | N_j, \\theta_j)$$ 后验推断：可以通过Hamiltonian Monte Carlo（HMC）算法来进行后验推断，生成超参数 $\\xi$ 和群体参数 $\\theta_j$ ​ 的样本。对于每个群体，后验均值 $E[\\theta_j | D]$ 会根据数据量的大小进行调整。对于数据较少的群体，估计值会向全体群体的均值（共享信息）靠拢，这种现象被称为收缩（shrinkage）。 2. 层次高斯模型的例子 问题背景：现在考虑实数值数据的情况，假设我们有多个群体的数据，每个群体的数据 $y_{ij}$ 服从正态分布 $N(\\theta_j, \\sigma^2)$ ，其中 $\\theta_j$ ​ 是该群体的均值， $\\sigma^2$ 是固定的方差。 与二项模型类似，我们可以假设各群体的均值 $\\theta_j$ ​ 来自一个共同的正态分布 $\\theta_j \\sim N(\\mu, \\tau^2)$ 。这个模型的联合分布为： $$p\\propto p(\\mu)p(\\tau^2) \\prod_{j=1}^{J} N(\\theta_j | \\mu, \\tau^2) N(y_j | \\theta_j, \\sigma_j^2)$$ 其中 $p(\\mu)$ 和 $p(\\tau^2)$ 是超参数的先验分布，可以假定 $\\sigma_j^2$ 是知道的。 对于每个群体，后验均值 $E[\\theta_j | D]$ 会介于单独的最大似然估计值 $\\hat{\\theta_j}$ 和全局均值 $\\mu$ 之间。根据公式： $$E[\\theta_j | D, \\mu, \\tau^2] = w_j \\mu + (1 - w_j) \\hat{\\theta_j}$$ ​其中收缩系数 $w_j = \\frac{\\sigma_j^2}{\\sigma_j^2 + \\tau^2}$ ​​。数据量较小或不确定性较高的群体（即 $\\sigma_j^2$ 较大的群体）会有更大的收缩，意味着它们的估计值会更多地依赖于全局均值。 为了解决算法在进行后验推断时的计算效率问题，可以采用非中心化参数化（non-centered parameterization）。这种方法通过重新表达 $\\theta_j = \\mu + \\tau \\eta_j$ ，其中 $\\eta_j \\sim N(0, 1)$ ，从而减少参数之间的依赖性，提升推断的计算效率。 六、经验贝叶斯 对于层次贝叶斯模型，在全贝叶斯推断中，我们对底层参数和超参数同时进行推断，计算 $p(\\theta, \\xi | D)$ 的联合后验分布。虽然这种方法在统计上是更为严格的，但计算量通常较大。经验贝叶斯提供了一种近似方法，首先通过最大化边际似然（如 $p(D|\\xi)$ ）估计超参数 $\\xi$ ，然后在给定这些估计值的条件下推断底层参数的后验分布（如 $p(\\theta|\\hat{\\xi}, D)$ ）。这种方法通过对超参数做点估计，而非推断它们的后验分布，因此简化了计算。 通过边际似然最大化来估计超参数是经验贝叶斯的核心步骤。具体来说，经验贝叶斯在给定数据 $D$ 的条件下，通过最大化边际似然 $p(D|\\xi)$ 来找到最优的超参数估计值 $\\hat{\\xi}$ ​。这种方法有时也被称为II类最大似然，因为它不是直接优化底层参数 $\\theta$ ，而是先优化超参数 $ \\xi$ 。 1. 经验贝叶斯在层次二项模型中的应用 在二项分布的层次模型中，经验贝叶斯的边际似然可以通过积分将底层参数 $\\theta_j$ ​ 消除掉，从而直接用超参数 $\\xi$ 表示边际似然。具体公式是： $$p(D|\\xi) = \\prod_j \\int \\text{Bin}(y_j|N_j, \\theta_j) \\text{Beta}(\\theta_j | a, b) d\\theta_j$$ 经验贝叶斯方法通过最大化此边际似然来估计超参数 $a$ 和 $b$ ，然后在给定这些估计值后，再计算每个 $\\theta_j$ 的后验分布。 2. 经验贝叶斯在层次高斯模型中的应用 在层次高斯模型中，经验贝叶斯可以通过边际化 $\\theta_j$ ​ 得到边际似然，并利用最大似然估计超参数 $\\mu$ 和 $\\tau^2$ 。在这个例子中，边际似然公式是： $$p(D|\\mu, \\tau^2, \\sigma^2) = \\prod_{j=1}^{J} N(y_j | \\mu, \\tau^2 + \\sigma^2)$$ 然后通过矩匹配方法估计 $\\tau^2$ 和 $\\mu$ 。 3. 经验贝叶斯在马尔可夫模型中的应用 经验贝叶斯还可以用于语言模型中的 n-gram 平滑问题。在这个上下文中，经验贝叶斯被用来估计马尔可夫链中状态转移矩阵的先验分布。通过为转移矩阵的每一行设定一个独立的狄利克雷分布作为先验，经验贝叶斯可以通过最大化边际似然来估计先验参数 $\\alpha$ 和 $m$ ，从而得到自适应的平滑方法。这个方法的优势在于，它能够根据数据自动调整平滑参数 $\\lambda_j$ ，从而提高模型的表现。 在 n-gram 语言模型中，我们希望计算不同词之间的转移概率，比如在二元模型（bigram model）中，给定词 $X_t = j$ ，下一个词 $X_{t+1} = k$ 的概率可以由转移矩阵 $A_{jk}$ ​ 来表示。传统的加一平滑方法对每一个可能的词对 $(j, k)$ 都假设了一个等价的概率，但这种假设往往过于简单。**删除插值（deleted interpolation）**是一个更复杂的方案，定义了如下的转移矩阵表示： $$A_{jk} = (1 - \\lambda) f_{jk} + \\lambda f_k$$ 其中， $f_{jk} = \\frac{N_{jk}}{N_j}$ 是从词 $j$ 到词 $k$ 的 bigram 频率， $f_k = \\frac{N_k}{N}$ ​​ 是词 $k$ 的 unigram 频率，而 $\\lambda$ 是一个通过交叉验证选择的平滑参数。 然而，删除插值方法没有考虑不同的上下文在词的频率中可能有不同的重要性。贝叶斯方法则可以为每个上下文动态地调整平滑参数 $\\lambda_j$​。 通过经验贝叶斯的方法对删除插值进行了重新解释。首先，假设转移矩阵的每一行都遵循独立的 Dirichlet 先验分布： $$A_j \\sim Dir(\\alpha_0 m_1, \\dots, \\alpha_0 m_K) = Dir(\\alpha_0 m) = Dir(\\alpha)$$ 其中， $A_j$ 是转移矩阵的第 $j$ 行， $m$ 是先验均值向量，满足 $\\sum_k m_k = 1$ ，而 $\\alpha_0$ ​ 是先验强度。通过贝叶斯推断，可以得到转移矩阵行 $A_j$ 的后验分布： $$A_j \\sim Dir(\\alpha + N_j)$$ 其中 $N_j = (N_{j1}, \\dots, N_{jK})$ 是从状态 $j$ 转移到其他状态的计数向量。在此基础上，后验预测密度为： $$p(X_{t+1} = k | X_t = j, D) = \\frac{N_{jk} + \\alpha_j m_k}{N_j + \\alpha_0}$$ 这个公式可以改写为删除插值的形式： $$p(X_{t+1} = k | X_t = j, D) = (1 - \\lambda_j) f_{jk} + \\lambda_j m_k$$ 其中， $\\lambda_j = \\frac{\\alpha_j}{N_j + \\alpha_0}$ 是动态调整的平滑参数，表示给定上下文 $j$ 时，将先验分布与经验数据相结合的权重。 EB 方法的核心思想是通过数据来估计 Dirichlet 分布的超参数 $\\alpha$ 和 $m$ 。在这个问题中，有一种近似方法来估计先验均值 $m$ ： $$m_k \\propto |{ j : N_{jk} &gt; 0 }|$$ 这个估计意味着某个词 $k$ 的先验概率与该词出现在多少种不同的上下文中有关，而不是它的具体出现次数。这种估计方法可以解决某些平滑方法中的不足。举个例子，如果在一个数据集中“you see”频繁出现，那么虽然 “you” 和 “see” 的 unigram 频率相同，但是它们在新上下文中出现的概率不应该相等。贝叶斯模型通过先验分布的参数 $m_k$ ​ 可以自适应地处理这种情况。 七、模型选择 在统计建模中，选择合适的模型至关重要。所有模型都存在一定的误差，然而某些模型能更好地适应数据，提供有用的预测。选择模型时，我们需要考虑模型的假设和拟合能力，确保它能在现实世界中应用。 1. 贝叶斯模型选择 贝叶斯模型选择的一个自然的想法是利用后验概率来确定最有可能生成数据的模型。公式如下： $$\\hat{m} = \\arg\\max_{m \\in M} p(m|D)$$ 这里的 $p(m|D)$ 表示给定数据 $D$ 下模型 $m$ 的后验概率。根据贝叶斯定理，我们可以表示为： $$p(m|D) = \\frac{p(D|m)p(m)}{\\sum_{m \\in M} p(D|m)p(m)}$$ 如果模型的先验是均匀的，即 $p(m) = \\frac{1}{|M|}$ ，那么最大后验模型是： $$\\hat{m} = \\arg\\max_{m \\in M} p(D|m)$$ 但是不同的模型设计是有好坏之分的。 示例：硬币是否公平？ 假设我们想知道某个硬币是否是公平的。我们可以设定两个模型： $M_0$ ：假设硬币是公平的，即 $\\theta = 0.5$ 。 $M_1$ ​：假设硬币是偏向的，即 $\\theta$ 可以是任意值。 通过比较这两个模型的边际似然，我们可以决定哪个模型更有可能解释观察到的数据。例如： 在公平硬币模型下，观察到 $N$ 次投掷的边际似然是： $$p(D|M_0) = \\left( \\frac{1}{2} \\right)^N$$ 在偏向硬币模型下，边际似然更复杂，需要计算贝塔分布的积分： $$p(D|M_1) = \\int p(D|\\theta)p(\\theta|M_1)d\\theta$$ 如果观察到的正面次数较多，模型 $M_1$ ​ 的可能性会更高。可能说明 $M_0$ 的先验并不好。 2. 贝叶斯模型平均 如果我们的目标是进行准确的预测，综合所有模型的预测结果通常比只依赖单一模型更好。贝叶斯模型平均可以表示为： $$p(y|D) = \\sum_{m \\in M} p(y|m)p(m|D) $$ 这里 $p(y|m)$ 是模型 $m$ 对新数据 $y$ 的预测。通过对所有模型的预测进行加权平均，我们可以得到更稳健的结果。 与机器学习中的集成技术类似，我们取预测器的加权组合。然而，集成的权重不必总和为 1，尤其是在贝叶斯模型平均中，如果有一个最佳模型 $m^$ ，在大样本极限下， $p(m|D)$ 将成为一个在 $m^$ 上的退化分布，其他模型将被忽略。 3 边际似然估计 为了进行贝叶斯模型选择，我们需要计算在给定先验的条件下的边际似然： $$p(D|m) = \\int p(D|\\theta, m)p(\\theta|m)d\\theta$$ 这里的积分通常难以直接计算。对于共轭先验模型，边际似然可以解析计算，这类模型因其先验与后验分布形式一致，使得边际似然的计算变得简单明了。 但对于其它的，我们可以使用变分推断或蒙特卡洛方法来估计。文中还给了一个Harmonic mean estimator的方法： 4. 交叉验证与边际似然的联系 交叉验证是一种评估模型预测能力的常用方法。它通过将数据划分为训练集和验证集来评估模型的表现。**留一交叉验证（LOO-CV）**是一种特殊的情况，其中每次留出一个样本进行测试，其他样本用于训练。 交叉验证的结果可以用来估计模型的泛化能力。 它与**对数边际似然（LML）**有很一定的关系： 5. 条件边际似然 边际似然用于回答“从先验生成训练数据的可能性有多大？”。它适用于在不同的固定先验之间进行假设检验，但很多时候我们更关心的是“后验能够生成数据分布中的新样本的概率是多少？”，这与模型的泛化性能相关联。 研究表明，边际似然有时可能与模型的泛化性能负相关。这是因为边际似然可能会出现先验较差但模型快速适应数据的情况 为了解决这个问题，研究者们提出了条件对数边际似然（CLML），公式为： $$CLML(m) = \\sum_{n=K}^{N} \\log p(D_n|D_{1:n-1}, m)$$ 其中， $K \\in {1, \\dots, N}$ 是算法的一个参数。CLML通过给定前 $K$ 个数据点的后验分布来评估后续 $N - K$ 个数据点的边际似然。这种方法减少了数据点顺序对结果的依赖。特别地，当 $K = N - 1$ 并对所有数据顺序进行平均时，这种方法相当于留一法（LOO）估计。 6. 贝叶斯留一法估计 对于监督模型来说，一个我们关注的点是**ELPD（expected log-pointwise predictive density），**ELPD 是对未来数据的预测性能进行估计的度量： $$ELPD(m) = \\mathbb{E}_{(x^, y^)} \\left[ \\log p(y^|x^, D, m) \\right]$$ 由于未来数据未知，因此可以使用 LOO 近似，即将部分数据点从数据集中移除并计算其预测分布： $$ELPD_{\\text{LOO}}(m) = \\sum_{n=1}^N \\log p(y_n|x_n, D_{-n}, m)$$ 直接计算 $ELPD_{\\text{LOO}}$ ​ 需要计算 $N$ 次不同的后验分布，这比较慢。可以通过只计算一次后验分布 $p(\\theta|D, m)$ ，然后使用重要性采样近似 LOO 积分。 重要性采样的核心思想是定义目标分布 $ f(\\theta) = p(\\theta|D_{-n}, m)$ ，并使用已知的提议分布 $g(\\theta) = p(\\theta|D, m)$ ，计算重要性权重： $$w_{s,-n} = \\frac{f(\\theta_s)}{g(\\theta_s)} \\propto \\frac{1}{p(D_n|\\theta_s)}$$ 将这些权重进行归一化后，可以用来近似 LOO 估计： $$ELPD_{\\text{IS-LOO}}(m) = \\sum_{n=1}^N \\log \\sum_{s=1}^S \\hat{w}_{s,-n} p(y_n|x_n, \\theta_s, m)$$ 重要性采样的一个问题是，权重的方差可能非常大，导致一些权重值过大。为了解决这个问题，可以对每个样本的权重拟合一个 Pareto 分布，从而对权重进行平滑。这样可以减少异常值对 LOO 估计的影响。 八、模型检测与假设检验 1. 后验预测检查（Posterior Predictive Checks）： 通过已知数据和模型生成未来的合成数据，以评估真实数据与模型生成的数据是否相似。如果模型生成的数据与真实数据差异很大，说明模型无法捕捉数据中的某些特征，模型可能不适合。 2.贝叶斯p值 通过计算贝叶斯p值来量化模型的合理性。如果观测到的测试统计量位于预测分布的极端部分（即p值接近0或1），说明模型无法合理解释数据 3. 假设检验 与频率学派的假设检验相对，贝叶斯方法提供了假设检验的两种替代方案： 使用贝叶斯因子进行模型比较：贝叶斯假设检验不再将检验统计量与临界值进行比较，而是评估在两种模型下数据的边际似然比——零假设 $M_0$ ​ 和替代假设 $M_1$ ​。这个比值称为贝叶斯因子，表示为： $B_{1,0} = \\frac{p(D | M_1)}{p(D | M_0)}$ 如果 $B_{1,0} &gt; 1$ ，我们倾向于支持 $ M_1$ ，否则我们更倾向于支持 $M_0$ ​。 基于参数估计：即估计在假设条件下附近的概率。例如，要测试硬币是否公平，我们计算正面概率 $\\theta$ 的后验分布，并检查接近 0.5 的区域中有多少概率质量。","categories":[{"name":"Probabilistic Machine Learning","slug":"Probabilistic-Machine-Learning","permalink":"https://jia040223.github.io/categories/Probabilistic-Machine-Learning/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jia040223.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"数学","slug":"数学","permalink":"https://jia040223.github.io/tags/%E6%95%B0%E5%AD%A6/"}]},{"title":"[旅游日志] :青甘大环线","slug":"青甘大环线","date":"2024-10-09T10:53:32.000Z","updated":"2024-10-09T11:55:06.761Z","comments":true,"path":"2024/10/09/青甘大环线/","permalink":"https://jia040223.github.io/2024/10/09/%E9%9D%92%E7%94%98%E5%A4%A7%E7%8E%AF%E7%BA%BF/","excerpt":"","text":"2024年的国庆假期，笔者忙完了保研，于是和高中同学一起去青甘大环线进行了7天的自驾游，在这记录保存一下笔者看到的祖国大西北的美景。 青海湖 大环线的第一站，青海湖比我想象中大很多，可惜景点只有一小部分。 茶卡盐湖 第二站去的茶卡盐湖，说是中国的天空之镜，可惜去的是晚上，没有欣赏到。btw，去茶卡盐湖一定要买小火车，笔者走了好久但还是没走到终点。 水上雅丹 第二天去的水上雅丹，总体来说比较独特的地貌。 翡翠湖 翡翠湖估计是我觉得这趟旅行最好看的经典了，真的非常好看。 莫高窟 莫高窟洞窟内不让拍照，只拍了外面，总体来说体验一波佛教文化。 鸣沙山和月牙泉 鸣沙山是很高沙丘，笔者玩了滑沙的项目。国庆晚上会有万人演唱会，氛围还是可以的。 嘉峪关 天下第一雄关，也算是人文景观了。 七彩丹霞 最后驿站去的七彩丹霞，还是十分好看的。 其它 一些有意思的图片，第一张是买的沙瓶纪念品。","categories":[{"name":"生活blog","slug":"生活blog","permalink":"https://jia040223.github.io/categories/%E7%94%9F%E6%B4%BBblog/"},{"name":"旅行日志","slug":"生活blog/旅行日志","permalink":"https://jia040223.github.io/categories/%E7%94%9F%E6%B4%BBblog/%E6%97%85%E8%A1%8C%E6%97%A5%E5%BF%97/"}],"tags":[{"name":"旅行日志","slug":"旅行日志","permalink":"https://jia040223.github.io/tags/%E6%97%85%E8%A1%8C%E6%97%A5%E5%BF%97/"}]},{"title":"[Probabilistic Machine Learning]: Fundamentals-Probability","slug":"Fundamentals-Probability","date":"2024-10-08T09:02:34.000Z","updated":"2024-10-09T14:23:37.620Z","comments":true,"path":"2024/10/08/Fundamentals-Probability/","permalink":"https://jia040223.github.io/2024/10/08/Fundamentals-Probability/","excerpt":"","text":"本学习笔记用于记录我学习Probabilistic Machine Learning的学习笔记，分享记录，也便于自己实时查看。 一、Probability基础知识 1. Probability space 概率空间是一个三元组 \\((Ω，F，P)\\) ，其中 \\(Ω\\) 是样本空间，是实验可能结果的集合； \\(F\\) 是事件空间，它是 \\(Ω\\) 所有可能子集的集合； \\(P\\) 是概率函数，它是从事件 \\(E \\subsetΩ\\) 到 \\([0,1]\\) 中的一个数(即 \\(P: F→[0,1]\\) )的映射，它满足一定的一致性等要求，具体如下： 2. 其它 离散随机变量定义，连续随机变量定义，条件概率，贝叶斯公式... 具体略过 二、常见分布 1. Discrete distributions 1.1 Bernoulli and binomial distributions： 伯努利分布和二项分布，也很熟悉了 1.2 Categorical and multinomial distribution： 其实也就是对伯努利分布和二项分布在更多的类别上的分布： 1.3 Poisson distribution 泊松分布，本科课程也重点学习过： 1.4 Negative binomial distribution 负二项分布又称帕斯卡分布（巴斯卡分布），它表示，已知一个事件在伯努利试验中每次的成功的概率是 \\(p\\) ，在一连串伯努利实验中，直到失败 \\(r\\) 次，此时成功次数作为随机变量 \\(x\\) 。 \\(r=1\\) 时，即为几何分布。 2. 分布在R上的Continuous distributions 2.1 Gaussian （Normal） 高斯分布，最经典的分布 2.2 Half-normal 半正态分布即一个高斯分布的绝对值（比如很多时候建模需要非负） 2.3 Student t-distribution t-分布也比较熟悉了： 2.4 Cauchy distribution Cauchy distribution是t-分布的特例： 2.5 Laplace distribution 拉帕拉斯分布也是很有名的分布，把高斯分布的平方改成了绝对值： 2.6 Sub-Gaussian and super-Gaussian distributions 其实就是比较尾部衰减速度。超高斯分布式指随机过程 \\(X\\) 的四阶累计量恒大于零，并且关于其均值对称分布。而亚高斯分布就是恒小于零。 例如拉普拉斯分布就是超高斯分布一种，而均匀分布就是亚高斯分布一种。 具体对比如图： 3. 分布在正实数上的Continuous distributions 3.1 Gamma distribution 伽马分布以伽马函数为基础，非常灵活。 “指数分布”和“ \\(χ^{2}\\) 分布”都是伽马分布的特例。 概率分布的可视化如下： 3.2 Exponential distribution 指数分布，伽马分布的特例。本科课程也重点学习过。 3.3 Chi-squared distribution \\(χ^{2}\\) 分布，伽马分布的特例。也比较熟悉了。 3.4 Inverse gamma 倒伽马分布是伽马分布变量的倒数。倒\\(χ^{2}\\) 分布是其特例。 3.5 Pareto distribution 帕累托分布是以意大利经济学家维弗雷多·帕雷托命名的。 是从大量真实世界的现象中发现的幂定律分布。其形式如下： 可以注意到，对概率分布取对数，则会得到一个线性函数，所以NLP中大名鼎鼎的齐夫定律便服从这个分布： 对于尾部比较大的分布的建模，帕累托分布是有用的，现实中许多形式的数据都具有这种特性。如： 财富在个人之间的分布（80%的人掌握20%的财富） 人类居住区的大小 对维基百科条目的访问 一般认为这是因为数据是由各种潜在因素产生的，当这些潜在因素混合在一起时，自然会导致这种重尾的分布。 其概率分布的直观展示如下： 4. 分布在[0, 1]上的Continuous distributions 4.1 Beta distribution 所谓的以 \\(\\alpha, \\beta\\) 为参数的 Beta 分布 \\(f(x; \\alpha, \\beta)\\) ，其实描述的就是我们在做抛硬币实验的过程中，我们当前如果已经观测到 \\(\\alpha + 1\\) 次正面， \\(\\beta + 1\\) 次反面，那么此时硬币正面朝上的真实概率的可能性分布。 即，Beta 分布是一个作为伯努利分布和二项式分布的共轭先验分布的密度函数。 5.Multivariate continuous distributions 5.1 Multivariate normal (Gaussian) 多元高斯函数是最重要最经典的多元分布了，下面会专门详细学习。 5.2 Multivariate Student distribution 多元t分布的形状与多元高斯比较类似，主要是峰值更低，尾部缩减更慢。 当 \\(v\\) 趋近于无穷时，其逐渐逼近多元高斯分布，其均值和协方差矩阵如下： 5.3 Circular normal (von Mises Fisher) distribution 现实中，有些数据仅仅分布于一个单位球上，而不是欧式空间的任何一点都有概率。此时，冯·米塞斯分布就是针对这种情况。 冯·米塞斯分布就是高斯分布在单位球上的拓展。 5.4 Matrix normal distribution (MN) Matrix normal distribution是作用于矩阵的正态分布，其定义如下： 它可以转化为作用于向量上的多元高斯分布，只要将矩阵正态分布进行向量化处理便可以得到多元正态分布形式，如上所示。 这两者完全等价 (证明过程可以参考https://en.wikipedia.org/wiki/Matrix_normal_distribution)，公式中的符号 $ $ 表示 Kronecker积，\\(V\\otimes U\\)表示多元正态分布的协方差矩阵；符号 $() $ 表示将给定矩阵按列组织成一个向量。 这两者完全等价，但在实践中，考虑到协方差矩阵\\(V\\otimes U\\in\\mathbb{R}^{(mn)\\times (mn)}\\)，假设我们想生成一个大小为\\(100\\times 200\\)的随机矩阵\\(X\\)，并要求矩阵\\(X\\)在概率上服从矩阵正态分布。此时，若利用多元正态分布进行生成，则需要协方差矩阵\\(V\\otimes U\\)的大小为\\((100\\times 200)\\times (100\\times 200)=20000\\times 20000\\)，元素数量为\\(4\\times 10^8\\)，显然，这个数字很惊人，毕竟存储这么大的矩阵就需要消耗计算机比较多的内存了，所以矩阵正态分布有它的优势。 5.5 Wishart distribution Wishart分布是伽马分布的多元形式，也是十分重要的分布。卡方分布也是它的特例。 多元高斯分布和Wishart分布有很紧密的联系，设\\(Y_{1}\\ldots Y_{n}\\ iid\\sim\\ N(0,\\Sigma)\\)，其中\\(Y_{i}(i = 1,\\ldots,n)\\)是 \\(p\\) 维列向量，则随机矩阵\\(W = \\sum_{i = 1}^{n}{Y_{i}Y_{i}^{T}}\\)的分布就是wishart分布，记作\\(W\\sim Wishart(\\Sigma,n)\\)，可以发现，当协方差矩阵退化为单位1，得到的就是卡方分布。 5.6 Inverse Wishart distribution 与倒伽马分布和伽马分布的关系类似，服从Wishart分布的随机变量的倒数就服从倒Wishart分布。 5.7 Dirichlet distribution 狄利克雷分布是Beta分布的多元形式，自然的其也是多项分布的共轭先验分布。 共轭先验在Beta分布里面已经提到过，目前笔者也只是稍微了解了一点，后面笔者也打算专门去深入了解一下。 狄利克雷分布可以用来定义“不确定性”的问题。考虑一个3面骰子。如果我们知道每个结果都是等可能的，我们可以使用“尖峰”对称狄利克雷，如Dir(20,20,20)，即我们确信结果将是不可预测的。相比之下，如果我们不确定结果会是什么样子(例如，它可能是一个有偏的骰子)，那么我们可以使用“平坦”对称狄利克雷，例如Dir(1,1,1)，它可以生成广泛的可能的结果分布。 三、高斯联合分布 实践中最广泛使用的连续随机变量联合概率分布是多元高斯分布了，也叫多元正态分布(MVN)。这部分是因为其在数学上很方便，而且高斯分布假设在许多情况下是相当合理的。 1. The multivariate normal 定义 多元高斯分布的定义应该都很熟悉了： 可以对协方差矩阵进行限制： Gaussian shells 随着维度 $ D$ 的增加，样本 \\(x∼N(0,I_D)\\) 中大部分点并不位于原点附近，而是集中在距离原点 \\(r = \\sqrt{D}\\) ​ 处的一个薄壳或环形区域。这是因为虽然概率密度随着 $ $ 指数衰减（距离增大概率密度减小），但球体的体积 随距离增加而增加，导致大多数点集中在距离原点 \\(\\sqrt{D}\\) 处的一个薄环上。此现象称为“高斯肥皂泡”。 计算点 \\(x\\) 到原点的平方距离 \\(d(x) = \\sum_{i=1}^{D} x_i^2\\) ​，其中 \\(x_i \\sim N(0, 1)\\) 。 期望值： \\(\\mathbb{E}[d^2] = D\\) 。 方差： \\(\\text{Var}(d^2) = D\\) 。 所以随着D的增大，the coefficient of variation（标准差与期望的比值）会趋近于0 Marginals and conditionals of an MVN 对于一个满足多元高斯分布的向量 \\(x\\) 进行分块为 \\(x1，x2\\) ，会发现其边缘分布均为高斯分布，条件分布也均为高斯分布。 并且 \\(p(x_1|x_2)\\) 的后验均值是 \\(x_2\\) 的线性函数，但协方差于 \\(x_2\\) 无关，这是高斯分布的一个特殊性质。具体如下： 其它表达形式 高斯分布有其它表达形式，这些形式有对应的优势，例如边缘化公式在矩形式下更简单，而条件化公式在信息形式下更简单 2. Linear Gaussian systems 线性高斯系统定义如下，一个变量 \\(z\\) 和条件分布 \\(p(y|z)\\) 均为高斯分布： 此时联合分布 \\(p(z,y)\\) 的形式如下： 而后验分布 \\(p(z|y)\\) 也是一个高斯分布： 四、The exponential family 指数族包括了众多上面提到的常见分布，比如高斯分布、二项分布、多项式分布、 泊松分布、gamma分布、beta分布等等。 其在机器学习里面起着至关重要的作用，主要因为其独特的优点： 1. 定义 指数族分布（Exponential Family Distribution）： 指数族分布是一类可以写成如下形式的分布： \\[p(x|\\eta) = h(x) \\exp \\left( \\eta^T T(x) - A(\\eta) \\right)\\] 其中， \\(\\eta\\) 是自然参数， \\(T(x)\\) 是充分统计量， \\(A(\\eta)\\) 是归一化常数，确保概率分布的积分为1。具体定义如下： 在数族分布中，如果自然参数 \\(\\eta\\) 之间相互独立，则可以更方便地进行推导和计算。所谓的独立性意味着没有非零的 \\(\\eta\\) 满足 \\(\\eta^T T(x) = 0\\) ，即自然参数 \\(\\eta\\) 不能通过其他参数线性组合来为零。此时我们称一个指数族分布为最小，因为这意味着我们不能通过减少自然参数的数量来进一步简化分布的参数化，否则分布会变得冗余。 在多项式分布中，由于参数有一个和为1的约束条件，导致自然参数之间并不完全独立。因此，严格来说，多项式分布并不是最小指数族。但尽管多项式分布中自然参数有依赖性，但可以通过重新参数化，将 \\(K\\) 个参数中的一个去掉，使用 \\(K-1\\) 个独立的参数来表示整个分布。这样可以将原来的问题转换为最小指数族的形式，使得参数之间更加独立。 2. 例子 2.1 伯努利分布： \\[\\begin{align*} P(x|\\mu) &amp;=\\mu^x(1-\\mu)^{1-x} \\\\&amp;=exp (ln(\\mu^x(1-\\mu)^{1-x})) \\\\&amp;=exp(xln(\\frac{\\mu}{1-\\mu})+ln(1-\\mu)) \\end{align*}\\] 对比可知有如下关系: [规范参数]\\(\\eta = \\phi(\\mu)=ln(\\frac{\\mu}{1-\\mu})\\) [充分统计量] \\(T(x)=x\\) [累积函数] \\(A(\\eta)=-ln(1-\\mu)\\) [基础度量值]\\(h(x)=1\\) \\(\\lambda = logistic(\\eta)=\\frac{1}{1+e^{-\\eta}}\\) 上面也提到了多项式分布怎么减少参数量（让参数变为互相独立的）。 2.2 Categorical distribution 与伯努利分布类似，由于参数求和为1，所以独立变量只有 \\(K-1\\) 个： 2.3 单变量高斯分布 高斯分布可做如下变换： \\[\\begin{align*} P(x|\\mu,\\sigma^2)&amp;=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{1}{2\\sigma^2}(x-\\mu)^2) \\\\&amp;=\\frac{1}{\\sqrt{2\\pi}}exp(\\frac{\\mu}{\\sigma^2}x-\\frac{1}{2\\sigma^2}x^2-\\frac{1}{2\\sigma^2}\\mu^2-ln\\sigma) \\end{align*}\\] 同样对比可知: [规范参数]\\(\\eta = \\phi(\\lambda)=[\\frac{\\mu}{\\sigma^2},-\\frac{1}{2\\sigma^2}]\\) [充分统计量] \\(T(x)=[x,x^2]\\) [累积函数] \\(A(\\eta)=\\frac{1}{2\\sigma^2}\\mu^2+ln\\sigma\\) [基础度量值]\\(h(x)=\\frac{1}{\\sqrt{2\\pi}}\\) 因为高斯模型有两个参数,所以两个向量长度都为2 如果限制 \\(\\sigma^{2} = 1\\) ，则有如下形式： 此时 \\(h(x)\\) 不再是常数。 2.4 多元高斯分布 与单变量高斯分布推导类似，但比较复杂，如下所示： 2.5 不是指数族的例子 分布族为指数族的必要条件为它有共同支撑集，也即 \\(S_\\theta = \\{x: p(x) &gt; 0\\}\\) 与 \\(\\theta\\) 无关。 比如说均匀分布 \\(R(0, \\theta)\\) 就没有共同支撑集（因为它非零的区域为 \\([0,\\theta]\\) ），所以它不可能是指数族分布。 3. 重要性质 Log partition function 对数配分函数 \\(A(\\eta)\\) 有如下性质： 直接从定义证明即可 自然参数和矩参数转换 对数分区函数 \\(A(\\eta)\\) 的梯度等于充分统计量的期望，也就是矩参数（或均值参数）。即： \\(m = E[T(x)] = \\nabla_\\eta A(\\eta)\\) 这表明我们可以通过计算 \\(A(\\eta)\\) 的梯度，从自然参数 \\(\\eta\\) 得到对应的矩参数 \\(m\\) 。 如果指数族是最小的，则可以从矩参数 \\(m\\) 转换回自然参数 \\(\\eta\\) 。这一过程通过对函数 \\(A(\\eta)\\) 的凸共轭函数（convex conjugate） \\(A^*(m)\\) 实现，公式为： \\(\\eta = \\nabla_m A^*(m)\\) 其中，凸共轭函数 $ A^*(m)$ 定义为： \\(A^*(m) = \\sup_{\\eta \\in \\Omega} \\left( m^T \\eta - A(\\eta) \\right)\\) 这意味着通过 \\(A^*\\) 的梯度可以从矩参数 \\(m\\) 转回自然参数 \\(\\eta\\) 。 4. 指数族的极大似然估计 指数族模型的似然函数形式：对于指数族分布模型，其似然函数可以写成以下形式： \\(p(D|\\eta) = \\prod_{n=1}^N h(x_n) \\exp \\left( \\eta^T \\sum_{n=1}^N T(x_n) - N A(\\eta) \\right)\\) 上式可以化简为： \\[p(D|\\eta) \\propto \\exp \\left( \\eta^T T(D) - N A(\\eta) \\right)\\] 这里 \\(T(D)\\) 是数据集的充分统计量之和： \\[T(D) = \\left[ \\sum_{n=1}^N T_1(x_n), \\ldots, \\sum_{n=1}^N T_K(x_n) \\right]\\] 不同的分布对应不同的充分统计量，例如： 对于Bernoulli分布，充分统计量 \\(T(D)\\) 为： \\(T(D) = \\left[ \\sum_n I(x_n = 1) \\right]\\) 对于一维高斯分布，充分统计量 \\(T(D)\\) 为： \\(T(D) = \\left[ \\sum_n x_n, \\sum_n x_n^2 \\right]\\) Pitman-Koopman-Darmois定理说明在某些正则条件下，指数族分布是唯一具有有限充分统计量的分布族。也就是说，在指数族分布中，充分统计量的个数不依赖于数据集的大小。 给定数据集 \\(D\\)，指数族分布的对数似然函数为： \\[\\log p(D|\\eta) = \\eta^T T(D) - N A(\\eta) + \\text{const}\\] 由于 \\(-A(\\eta)\\) 是自然参数 \\(\\eta\\) 的凸函数，而 \\(\\eta^T T(D)\\) 是线性函数，因此可以得出：对数似然函数是凸的，从而存在唯一的全局最大值。 我们对对数似然函数求导，导数如下： \\[\\nabla_\\eta \\log p(D|\\eta) = T(D) - N E[T(x)]\\] 对于单个数据点 \\(x\\) ，导数为： \\[\\nabla_\\eta \\log p(x|\\eta) = T(x) - E[T(x)]\\] 至于 \\(E[T(x)]\\) ，我们用数据集进行估计即可： \\[E[T(x)] = \\frac{1}{N} \\sum_{n=1}^N T(x_n)\\] 五、随机变量之间的变换 1. 双射 双射的变换公式很熟悉了，主要就是涉及到雅可比矩阵行列式： 2. 蒙特卡罗近似 也很熟悉了，就是采样估计： 3. Probability integral transform 这个其实就是从均匀分布采样，然后通过逆映射进行计算，这样就相当于从原分布中进行采样了。也比较熟悉了： 六、 马尔可夫链 马尔可夫链涉及的知识比较多，书上讲的也都是比较基础的，本科课程也学习过。主要记录一下之前没见过的： 马尔可夫链的最大似然估计： MAP estimation：解决数据稀疏的问题，引入了Dirichlet先验： 七、比较两个分布的相似度 1. f-散度 f散度是一个函数，这个函数用来衡量两个概率密度p和q的区别，也就是衡量这两个分布多么的相同或者不同。像 \\(KL\\) 散度和 \\(JS\\) 散度都是它的一种特例 f散度定义如下： \\[{D_f}(\\mathcal P_1\\|\\mathcal P_2)=\\int f (\\frac{p_2(x)}{p_1(x)})\\cdot p_1(x)\\mathrm d x=\\mathbb E_{x\\sim\\mathcal P_1}\\left[f(\\frac{p_2(x)}{p_1(x)})\\right] \\\\\\] \\(f()\\) 就是不同的散度函数， \\(D_f\\) 就是在f散度函数下，两个分布的差异。规定 \\(f\\) 是凸函数(为了用琴生不等式) $f ( 1 ) = 0 $ (如果两个分布一样，刚好公式=0) 下面给出一些常见的f-散度例子： KL 散度 当 $f( r ) = rlog( r ) $ 时，f-散度变为 KL 散度，公式为： \\[D_{KL}(p || q) = \\int p(x) \\log \\frac{p(x)}{q(x)} dx\\] α-散度 (Alpha Divergence) 当 \\(f(x) = \\frac{4}{1 - \\alpha^2} (1 - x^{\\frac{1+\\alpha}{2}})\\) 时，f-散度变为 α-散度，公式为： \\[D^\\alpha_A (p || q) = \\frac{4}{1 - \\alpha^2} \\left( 1 - \\int p(x)^{\\frac{1+\\alpha}{2}} q(x)^{\\frac{1-\\alpha}{2}} dx \\right)\\] 其中， \\(\\alpha \\neq \\pm 1\\) 。另一种常用的参数化方式（Minka 方式）为： \\[DD^\\alpha_M(p || q) = \\frac{1}{\\alpha(1-\\alpha)} \\left( 1 - \\int p(x)^\\alpha q(x)^{1-\\alpha} dx \\right)\\] 当 \\(\\alpha \\to 0\\) 时，α-散度趋向于 \\(D_{KL}(q||p)\\) 。 当 \\(\\alpha \\to 1\\) 时，α-散度趋向于 \\(D_{KL}(p||q)\\) 。 当 \\(\\alpha = 0.5\\) 时，α-散度等于 Hellinger 距离（见下）。 Hellinger 距离 (Hellinger Distance) 平方的 Hellinger 距离定义为： \\[D_H^2(p || q) = \\frac{1}{2} \\int \\left( \\sqrt{p(x)} - \\sqrt{q(x)} \\right)^2 dx\\] 这相当于 f-散度，其中 \\(f( r ) = (\\sqrt{r} - 1)^2\\) 。 卡方距离 (Chi-Squared Distance) 卡方距离定义为： \\[\\chi^2(p || q) = \\frac{1}{2} \\int \\frac{(q(x) - p(x))^2}{q(x)} dx\\] 这对应于 f-散度，其中 \\(f( r ) = ( r - 1 )^2\\) 。 2. 积分概率度量 (Integral Probability Metrics, IPM) IPM 也用于计算两个分布 \\(P\\) 和 \\(Q\\) 之间的差异，其定义为： \\[ D_F(P, Q) = \\sup_{f \\in F} \\left| \\mathbb{E}_{p(x)}[f(x)] - \\mathbb{E}_{q(x&#39;)}[f(x&#39;)] \\right| \\] 其中， \\(F\\) 是一类“光滑”的函数。常见的 IPM 度量包括： 最大均值差异 (Maximum Mean Discrepancy, MMD) 如果 \\(F\\) 是在正定核函数下的 RKHS（再生核希尔伯特空间），则对应的 IPM 被称为最大均值差异（MMD）。 Wasserstein 距离 (Wasserstein Distance) 如果 \\(F\\) 是满足 Lipschitz 条件的函数类 \\(F = \\{ ||f||_L \\leq 1 \\}\\) ，即 Lipschitz 常数有界（例如为1）的函数集合，则 IPM 变为 Wasserstein-1 距离： \\[ W_1(P, Q) = \\sup_{||f||_L \\leq 1} \\left| \\mathbb{E}_{p(x)}[f(x)] - \\mathbb{E}_{q(x)}[f(x&#39;)] \\right| \\]","categories":[{"name":"Probabilistic Machine Learning","slug":"Probabilistic-Machine-Learning","permalink":"https://jia040223.github.io/categories/Probabilistic-Machine-Learning/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jia040223.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"数学","slug":"数学","permalink":"https://jia040223.github.io/tags/%E6%95%B0%E5%AD%A6/"}]},{"title":"更新说明 2024.9.30","slug":"更新日志","date":"2024-09-29T16:34:30.000Z","updated":"2024-10-09T10:04:48.038Z","comments":true,"path":"2024/09/30/更新日志/","permalink":"https://jia040223.github.io/2024/09/30/%E6%9B%B4%E6%96%B0%E6%97%A5%E5%BF%97/","excerpt":"","text":"国庆假期到了，这学期课程压力比较小，所以也是能为以后的科研学习一下相关知识。但国庆假期还是给自己放了一个大长假，这段时间估计是不太会更新了。 Stanford CS236 最近Stanford CS236课程也算是看完了，后面可能还会有一些内容打算写一写吧。主要还是围绕diffusion，包括 ldm diffusion的condition控制 如何把diffusion用于离散的数据 前面的文章可能也会补一补。感觉diffusion涉及到的数学知识还是挺多的，后面有机会可以来补一补数学基础。 Probabilistic Machine Learning 打算学习一下Probabilistic Machine Learning这本书，后面应该也会边学边记录一下，也强烈给读者推荐这本书，特别对于像我这样致力于在AI领域进行研究但基础比较薄弱的同学。 这本书应该也是将来一段时间我的学习重点了，内容还是很多的。 数学 Stanford CS236课程还是涉及到挺多的数学知识，后面有机会可以来补一补数学基础。之前保研复习了一下微积分，线代，微分方程这些，后面可能会多看一看优化相关（比如什么拉格朗日对偶问题，每次遇到都是混过去了）的知识，同时对diffusion涉及的一些知识也多了解了解，可能包括： SDE和ODE的解法 傅里叶变换 优化理论 科研 国庆之后也可能会具体进行一些导师的项目，后面在科研上的学习有机会也可以记录一下。 碎碎念 感觉还是太菜了，什么都不会。感觉大学四年在课堂上学的东西真的太基础了。 以前本科的实习也就是看了几篇论文就开始做，然后也就用的别人的模型，在上面小修小补，以至于做了一学期的生成模型，现在看了Stanford CS236，感觉以前真的啥都不知道。 虽然可能跟着别人脚步走也能发论文吧，但还是希望能夯实一下理论基础，希望以后科研的日子能过得轻松一点。","categories":[{"name":"更新日志","slug":"更新日志","permalink":"https://jia040223.github.io/categories/%E6%9B%B4%E6%96%B0%E6%97%A5%E5%BF%97/"}],"tags":[{"name":"更新日志","slug":"更新日志","permalink":"https://jia040223.github.io/tags/%E6%9B%B4%E6%96%B0%E6%97%A5%E5%BF%97/"}]},{"title":"Diffusion Model原理","slug":"Diffusion Model原理","date":"2024-09-29T10:34:53.000Z","updated":"2024-09-29T17:08:28.988Z","comments":true,"path":"2024/09/29/Diffusion Model原理/","permalink":"https://jia040223.github.io/2024/09/29/Diffusion%20Model%E5%8E%9F%E7%90%86/","excerpt":"","text":"本学习笔记用于记录我学习Stanford CS236课程的学习笔记，分享记录，也便于自己实时查看。 引入 前面的课程中我们已经学习了许多生成模型的架构，例如VAEs，Score Based Models等。在课程的最后也是总算来到当前最火的生成模型架构：Diffusion Model。其实Diffusion Model与前面模型或多或少都有一定的联系，我们也可以从不同的视角来理解它。 笔者本科科研也算是学习研究了一些Diffusion相关的工作，但之前一直没有去梳理生成模型的发展，也没有深究其背后的数学原理。所以借此几乎，正好对一些知识进行整理，并对生成模型进行部分回顾。首先从DDPM和DDIM入手吧，这两篇文章也是之前科研实践学习过很多次了。 DDPM 首先我们知道，DDPM 是个马尔科夫模型（如下图），DDPM包括两个步骤。这两个步骤在原文中定义为前向加噪（forward，下图从右到左）和后向去噪（reverse，下图从左到右）。 从 \\(x_0\\) 到 \\(x_T\\) 的过程就是前向加噪过程，我们可以看到加噪过程就是对原始图片 \\(x_0\\) 不断添加噪声，使其最后信噪比趋近于0，此时得到的图片也就变成噪声了，而与之相对应的去噪过程就是还原过程，即从噪声不断去噪还原为图片。 我们通过往图片中加入噪声，使得图片变得模糊起来，当加的步骤足够多的时候（也就是T的取值越大的时候，一般取1000），图片已经非常接近一张纯噪声。纯噪声也就意味着多样性，我们的模型在去噪（还原）的过程中能够产生更加多样的图片。 这里的操作实际上就是指在图片加入噪声 \\(noise\\) ，噪声 \\(noise\\) 本身的分布可以是很多样的（btw，保研还被问过这个问题），而论文中采用的是标准正态分布，其理由是考虑到其优良的性质，在接下来的公式推理中见到。 推导 从上面的图可知，DDPM 将前向过程和逆向过程都设计为了马尔可夫链的形式： 称从 \\(x_0\\) 到 \\(x_T\\) 的马尔可夫链为前向过程 (forward process) 或扩散过程 (diffusion process)； 称从 \\(x_T\\) 到 \\(x_0\\) 的马尔可夫链为逆向过程 (reverse process) 或去噪过程 (denoising process). 所以我们的损失函数通过极大似然估计来进行。但这里我们又会遇到和VAE一样的问题， \\(log(P(x))\\) 中的 \\(P(x)\\) 需要对 \\(x_{1:T}\\) 进行积分，此时我们便可以效仿VAE的做法，即把 \\(x_{1:T}\\) 作为类似VAE中的潜变量，去优化对数似然的下界ELBO（为什么是下界可以参考我都VAEs的文章，简单来说就是用琴生不等式即可）： \\[ \\begin{align*} ELBO &amp;= \\mathbb{E}_{\\mathbf{x}_{1:T} \\sim q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_0)} \\left[\\log \\frac{p_\\theta(\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_0)} \\right] \\\\&amp;= \\mathbb{E}_{\\mathbf{x}_{1:T} \\sim q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_0)} \\left[ \\log \\frac{p(\\mathbf{x}_T) \\prod_{t=1}^T p_\\theta(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t)}{\\prod_{t=1}^T q(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1})} \\right] \\end{align*} \\] 至于这里为啥要在给定 \\(x_0\\) 下计算，一方面是单纯的 \\(q(\\mathbf{x}_{1:T})\\) 我们没办法计算得出，而 \\(q(\\mathbf{x}_{1:T}|\\mathbf{x}_0)\\) 我们能求出其闭式解，另一方面在训练时我们的确已经 \\(x_0\\)的信息。 OK，我们继续进行推导 \\[ \\begin{align} &amp;\\ \\ \\ \\ \\ \\text{ELBO}(\\mathbf x_0) \\\\ &amp;=\\mathbb E_{q(\\mathbf x_{1:T}\\vert\\mathbf x_0)}\\left[\\log\\frac{p(\\mathbf x_T)\\prod_{t=1}^{T}p(\\mathbf x_{t-1}\\vert\\mathbf x_t)}{\\prod_{t=1}^{T}q(\\mathbf x_t\\vert\\mathbf x_{t-1})}\\right]\\\\ &amp;=\\mathbb E_{q(\\mathbf x_{1:T}\\vert\\mathbf x_0)}\\left[\\log\\frac{p(\\mathbf x_T)\\prod_{t=1}^{T}p(\\mathbf x_{t-1}\\vert\\mathbf x_t)}{q(\\mathbf x_1\\vert\\mathbf x_0)\\prod_{t=2}^{T}q(\\mathbf x_t\\vert\\mathbf x_{t-1},\\mathbf x_0)}\\right]\\\\ &amp;=\\mathbb E_{q(\\mathbf x_{1:T}\\vert\\mathbf x_0)}\\left[\\log\\frac{p(\\mathbf x_T)\\prod_{t=1}^{T}p(\\mathbf x_{t-1}\\vert\\mathbf x_t)}{q(\\mathbf x_1\\vert\\mathbf x_0)\\prod_{t=2}^{T}\\frac{q(\\mathbf x_t\\vert\\mathbf x_0)q(\\mathbf x_{t-1}\\vert\\mathbf x_t,\\mathbf x_0)}{q(\\mathbf x_{t-1}\\vert\\mathbf x_0)} }\\right]\\\\ &amp;=\\mathbb E_{q(\\mathbf x_{1:T}\\vert\\mathbf x_0)}\\left[\\log\\frac{p(\\mathbf x_T)\\prod_{t=1}^{T}p(\\mathbf x_{t-1}\\vert\\mathbf x_t)}{q(\\mathbf x_T\\vert\\mathbf x_0)\\prod_{t=2}^{T}q(\\mathbf x_{t-1}\\vert\\mathbf x_t,\\mathbf x_0)}\\right]\\\\ &amp;=\\mathbb E_{q(\\mathbf x_{1:T}\\vert\\mathbf x_0)}\\left[\\log p(\\mathbf x_0\\vert\\mathbf x_1)\\right]+\\mathbb E_{q(\\mathbf x_{1:T}\\vert\\mathbf x_0)}\\left[\\log\\frac{p(\\mathbf x_T)}{q(\\mathbf x_T\\vert\\mathbf x_0)}\\right]+\\sum_{t=2}^T\\mathbb E_{q(\\mathbf x_{1:T}\\vert\\mathbf x_0)}\\left[\\log\\frac{p(\\mathbf x_{t-1}\\vert\\mathbf x_t)}{q(\\mathbf x_{t-1}\\vert\\mathbf x_t,\\mathbf x_0)}\\right]\\\\ &amp;=\\mathbb E_{q(\\mathbf x_{1}\\vert\\mathbf x_0)}\\left[\\log p(\\mathbf x_0\\vert\\mathbf x_1)\\right]+\\mathbb E_{q(\\mathbf x_{T}\\vert\\mathbf x_0)}\\left[\\log\\frac{p(\\mathbf x_T)}{q(\\mathbf x_T\\vert\\mathbf x_0)}\\right]+\\sum_{t=2}^T\\mathbb E_{q(\\mathbf x_t\\vert\\mathbf x_0)}\\mathbb E_{q(\\mathbf x_{t-1}\\vert\\mathbf x_t,\\mathbf x_0)}\\left[\\log\\frac{p(\\mathbf x_{t-1}\\vert\\mathbf x_t)}{q(\\mathbf x_{t-1}\\vert\\mathbf x_t,\\mathbf x_0)}\\right]\\\\ &amp;=\\underbrace{\\mathbb E_{q(\\mathbf x_{1}\\vert\\mathbf x_0)}\\left[\\log p(\\mathbf x_0\\vert\\mathbf x_1)\\right]}_\\text{reconstruction term}-\\underbrace{\\text{KL}(q(\\mathbf x_T\\vert\\mathbf x_0)\\Vert p(\\mathbf x_T))}_\\text{regularization term}-\\sum_{t=2}^T\\mathbb E_{q(\\mathbf x_t\\vert\\mathbf x_0)}\\underbrace{\\left[\\text{KL}(q(\\mathbf x_{t-1}\\vert\\mathbf x_t,\\mathbf x_0)\\Vert p(\\mathbf x_{t-1}\\vert\\mathbf x_t))\\right]}_\\text{denoising matching terms} \\end{align} \\] 同样出现了重构项、正则项和匹配项。重构项要求 $x_1 $ 能够重构 \\(x_0\\) ，正则项要求 \\(x_T\\) 的后验分布逼近先验分布，而匹配项则建立起相邻两项 $x_{t−1},x_t $ 之间的联系。 现在，我们只需要为式中出现的所有概率分布设计具体的形式，就可以代入计算了。为了让 KL 散度可解，一个自然的想法就是把它们都设计为正态分布的形式。 前向过程 在DDPM的前向过程中，对于 \\(t \\in [1,T]\\) 时刻， \\(x_t\\) 和 \\(x_{t-1}\\) 满足如下关系： \\[x_t = \\sqrt{1-\\beta_t}x_{t-1} + \\sqrt{\\beta_t }\\epsilon, \\ \\ \\ \\epsilon\\sim N(0,1)\\] 其中 \\(β_t∈(0,1)\\) 是事先指定的超参数，代表从 $x_{t−1} $ 到 $x_t $ 这一步的方差。 这里的系数设定为开根号的 $ $ ，是为了保证马尔科夫链的最后收敛为标准高斯分布。 \\(\\sqrt\\beta\\) 和 \\(\\sqrt{1-\\beta}\\) 是怎么来的： 我们这里先不管 \\(\\beta\\) ，把两个系数分别设为 \\(a\\) 和 \\(b\\) 。 公式变为： \\[x_t = ax_{t-1} + b\\epsilon\\] 我们希望，当 \\(t\\)趋于无穷的时候， \\(x_t \\sim N(0,1), x_{t-1} \\sim N(0,1)\\) 我们知道当两个高斯分布相加时， \\[X\\sim N(\\mu_X,\\sigma_X^2),Y\\sim N(\\mu_Y,\\sigma_Y^2) \\] \\[Z=aX+bY \\] 则 \\[Z \\sim N(a\\mu_X+b\\mu_Y, a^2\\sigma^2+b^2\\sigma^2)\\] 所以此时 \\[x_t~\\sim N(a\\mu_{t-1}+b\\mu_\\epsilon,a^2\\sigma_{t-1}^2+b^2\\sigma_\\epsilon^2)\\] \\[x_t\\sim N(a·0+b·0, a^2·1+b^2·1)\\] \\[x_t \\sim N(0,a^2+b^2) \\] 我们想让 \\(x_{t-1}\\) 和 \\(\\epsilon\\) 得到的 \\(x_{t}\\) 也服从标准正态分布，即 $ x_{t} N(0,1)$ ，那么我们就只能让 \\(a^2+b^2=1\\) 。 再令 \\(\\beta=a^2\\) ，则 \\(a=\\sqrt{\\beta},b=\\sqrt{1-\\beta}\\) 。 或者也可以令 \\(\\alpha=b^2\\) ，则 $ a=x_{t-1}+$ 。 说白了，这俩系数就是为了让两个服从标准正态分布的噪声相加得到的东西还是服从正态分布。 OK，在这基础上我们可以继续推导，让 \\(x_t\\) 用 \\(x_0\\) 来表示： 令 \\(\\alpha_t=1-\\beta_t\\) ，则公式变为： \\[x_t=\\sqrt{\\alpha_t}x_{t-1}+\\sqrt{1-\\alpha_t}\\epsilon\\] 继续推导： \\[\\begin{align*} x_t &amp;=\\sqrt{\\alpha_t}x_{t-1}+\\sqrt{1-\\alpha_t}\\epsilon\\\\ &amp;=\\sqrt{\\alpha_t}(\\sqrt{\\alpha_{t-1} }x_{t-2}+\\sqrt{1-\\alpha_{t-1} }\\epsilon)+\\sqrt{1-\\alpha_t}\\epsilon\\\\ &amp;=\\sqrt{\\alpha_t\\alpha_{t-1} }x_{t-2}+\\sqrt{\\alpha_t(1-\\alpha_{t-1})}\\epsilon + \\sqrt{1-\\alpha_t}\\epsilon\\\\ \\end{align*}\\] 上式最后一行第二项和第三项，可以看做两个正态分布相加。 由于两个正态分布 \\(X\\sim N(\\mu_x,\\sigma_x^2), Y\\sim N(\\mu_y, \\sigma_y^2)\\) ，相加后有 \\(aX+bY\\sim N(a\\mu_x+b\\mu_y,a^2\\sigma_x^2+b^2\\sigma_y^2)\\)。所以，合并两个正态分布，得到： \\[x_t=\\sqrt{\\alpha_t\\alpha_{t-1} }x_{t-2}+\\sqrt{1-\\alpha_t\\alpha_{t-1} }\\epsilon\\] 由数学归纳法，可以推导出： \\[x_t=\\sqrt{\\alpha_t\\alpha_{t-1}...\\alpha_1}x_0+\\sqrt{1-\\alpha_t\\alpha_{t-1}...\\alpha_1}\\epsilon\\] 再令 \\(\\bar\\alpha_t=\\alpha_t\\alpha_{t-1}...\\alpha_1\\) ，则公式可以进一步化简为： \\(x_t=\\sqrt {\\bar\\alpha_t}x_0+\\sqrt{1-\\bar\\alpha_{t} }\\epsilon\\) ，由于 \\[\\lim_{t\\to\\infty}\\sqrt{\\bar\\alpha_t}=0,\\quad\\lim_{t\\to\\infty}\\sqrt{1-\\bar\\alpha_t}=1\\] 所以我们能够保证马尔科夫链最后能够收敛于标准正态分布 逆向过程 这里从我们熟知的贝叶斯公式出发： \\[P(A|B)=\\frac{P(B|A)P(A)}{P(B)}\\] 可知 \\[P(x_{t-1}|x_t)=\\frac{P(x_t|x_{t-1})P(x_{t-1})}{P(x_t)}\\] 这里我们的 \\(P(x_{t-1})\\) 和 \\(P(x_t)\\) 我们都不知道，但在已知 $ x_0$ 的情况下有： \\[P(x_{t-1}|x_t,x_0)=\\frac{P(x_t|x_{t-1},x_0)P(x_{t-1}|x_0)}{P(x_t|x_0)}\\] 把 \\(x_0=\\sqrt{\\bar{\\alpha_t} }x_0\\) 和 \\(x_t=\\sqrt {\\bar\\alpha_t}x_0+\\sqrt{1-\\bar\\alpha_{t} }\\epsilon\\) 带入上式，可得： \\[P(x_{t-1}|x_t,x_0)=\\frac{ N(\\sqrt{\\alpha_t}x_0,1-\\bar\\alpha_t) N(\\sqrt{\\bar\\alpha_{t-1} }x_0,1-\\bar\\alpha_{t-1}) }{ N(\\sqrt{\\bar\\alpha_{t} }x_0,1-\\bar\\alpha_{t}) }\\] 已知高斯分布的概率密度函数为： \\[f(x)=\\frac{1}{\\sqrt{2\\pi\\sigma} }exp(-\\frac{(x-\\mu)^2}{2\\sigma^2})\\] 所以 \\[P(x_{t-1}|x_t,x_0) \\propto exp-\\frac{1}{2} [ \\frac{(x_t-\\sqrt{\\alpha_t}x_{t-1})^2}{1-\\alpha_t} +\\frac{(x_{t-1}-\\sqrt{\\bar\\alpha_{t-1} }x_0)^2}{1-\\bar\\alpha_{t-1} } -\\frac{(x_{t}-\\sqrt{\\bar\\alpha_{t} }x_0)^2}{1-\\bar\\alpha_{t} } ]\\] 此时由于 \\(x_{t-1}\\) 是我们关注的变量，所以整理成关于 \\(x_{t-1}\\) 的形式： \\[P(x_{t-1}|x_t,x_0) \\propto exp-\\frac{1}{2} [ (\\frac{\\alpha_t}{1-\\alpha_t}+\\frac{1}{1-\\bar\\alpha_{t-1} })x_{t-1}^2 -(\\frac{-2\\sqrt{\\alpha_t}x_t}{1-\\alpha_t} + \\frac{-2\\sqrt{\\bar\\alpha_{t-1} }x_0}{1-\\bar\\alpha_{t-1} })x_{t-1} +C(x_t,x_0) ]\\] 其中第三项 \\(C(x_t,x_0)\\) 与 \\(x_{t-1}\\) 无关，作为指数上相加的部分，可以拿到最前面只影响最前面的系数。 所以此时： \\[P(x_{t-1}|x_t,x_0) \\propto exp-\\frac{1}{2} [ (\\frac{\\alpha_t}{1-\\alpha_t}+\\frac{1}{1-\\bar\\alpha_{t-1} })x_{t-1}^2 -(\\frac{-2\\sqrt{\\alpha_t}x_t}{1-\\alpha_t} + \\frac{-2\\sqrt{\\bar\\alpha_{t-1} }x_0}{1-\\bar\\alpha_{t-1} })x_{t-1}]\\] 又因为标准正态分布满足 \\(\\propto exp - \\frac{x^2-2\\mu x + \\mu^2}{2\\sigma^2}\\) ，所以我们可以得到 \\(P(x_{t-1}|x_t,x_0)\\) 对应的方差 \\[ \\frac{1}{\\sigma^2}=\\frac{\\alpha_t}{1-\\alpha_t}+\\frac{1}{1-\\bar\\alpha_{t-1} } =\\frac{1-\\alpha_t\\bar\\alpha_{t-1} }{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})} =\\frac{1-\\bar\\alpha_{t} }{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}\\] 这里 \\(\\alpha_t\\bar\\alpha_{t-1}=\\bar\\alpha_t\\) 。所以： \\[\\sigma^2=\\frac{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}\\] 再看 \\(x_{t-1}\\) 的一次项，得到： \\[\\frac{2\\mu}{\\sigma^2}= (\\frac{-2\\sqrt{\\alpha_t}x_t}{1-\\alpha_t} + \\frac{-2\\sqrt{\\bar\\alpha_{t-1} }x_0}{1-\\bar\\alpha_{t-1} })\\] 把 \\(\\sigma^2\\) 和 \\(x_0\\) 带入上式，化简得到： \\[\\mu=\\frac{1}{\\sqrt{\\alpha_t} }(x_t-\\frac{1-\\alpha_t}{\\sqrt{1-\\bar\\alpha_t} }\\epsilon)\\] 所以说： \\[P(x_{t-1}|x_t, x_0)\\sim N(\\frac{1}{\\sqrt{\\alpha_t} }(x_t-\\frac{1-\\alpha_t}{\\sqrt{1-\\bar\\alpha_t} }\\epsilon), \\frac{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t})\\] 回顾一下我们写的这一大段公式，也就是说，我们已知了先验概率，推导出了后验概率的表达式，得到了在给定 \\(x_0\\) 后的\\(x_{t-1}\\) 的分布的均值和方差。也就是说，上面公式中，我们的 \\[q(x_{t-1}\\vert x_t,x_0)\\sim N(\\frac{1}{\\sqrt{\\alpha_t} }(x_t-\\frac{1-\\alpha_t}{\\sqrt{1-\\bar\\alpha_t} }\\epsilon), \\frac{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t})\\] 接下来， \\(\\epsilon\\) 的具体值，我们让模型去拟合就好了。 损失函数 我们之前已经推导了ELBO的具体形式： \\[\\text{ELBO}= \\underbrace{E_{x_1\\sim q(x_1\\vert x_0)}[\\log p_\\theta(x_0\\vert x_1)]}_{ {L_0} }- \\underbrace{KL(q(x_T \\vert x_0)\\|p(x_T))}_{ {L_T} }- \\sum_{t=2}^T\\underbrace{E_{x_t\\sim q(x_t\\vert x_0)}\\left[KL(q(x_{t-1}\\vert x_t,x_0)\\|p_\\theta(x_{t-1}\\vert x_t))\\right]}_{ {L_{t-1} }}\\] 这里 \\(q(x_{t-1}\\vert x_t,x_0)\\) 我们已经得到了， \\(q(x_{t}|x_0)\\) 也是我们定义的。只需要定义 \\(p_\\theta(x_{t-1}|x_t)\\) 即可，为了计算方便，我们也选择与 \\(q(x_{t-1}\\vert x_t,x_0)\\) 一样的形式。 \\[p_\\theta(\\textbf{x}_{t-1}|\\textbf{x}_t) = \\mathcal{N}(\\textbf{x}_{t-1}; \\mu_\\theta(\\textbf{x}_t, t), \\frac{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}I)\\] 其中 \\(\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t) = \\frac{1}{\\sqrt{\\alpha_t} } \\Big( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t} } \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t) \\Big)\\) ，而 \\({\\epsilon}_\\theta(\\mathbf{x}_t, t)\\) 就是我们模型的输出。此时，我们带入可以得到 \\[\\begin{align} \\mathbf{x}_{t-1} &amp;= \\mathcal{N}(\\mathbf{x}_{t-1}; \\frac{1}{\\sqrt{\\alpha_t} } ( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t} } {\\epsilon}_\\theta(\\mathbf{x}_t, t) ), \\frac{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}I) \\end{align}\\] 带入上面KL散度的公式，可以得到损失函数 \\(L_t\\) 便为： \\[\\begin{aligned} L_t &amp;= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon} } \\Big[\\frac{1}{2 \\| \\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t) \\|^2_2} \\| \\color{blue}{\\tilde{\\boldsymbol{\\mu} }_t(\\mathbf{x}_t, \\mathbf{x}_0)} - \\color{green}{\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t)} \\|^2 \\Big] \\\\ &amp;= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon} } \\Big[\\frac{1}{2 \\|\\boldsymbol{\\Sigma}_\\theta \\|^2_2} \\| \\color{blue}{\\frac{1}{\\sqrt{\\alpha_t} } \\Big( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t} } \\boldsymbol{\\epsilon}_t \\Big)} - \\color{green}{\\frac{1}{\\sqrt{\\alpha_t} } \\Big( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t} } \\boldsymbol{\\boldsymbol{\\epsilon} }_\\theta(\\mathbf{x}_t, t) \\Big)} \\|^2 \\Big] \\\\ &amp;= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon} } \\Big[\\frac{ (1 - \\alpha_t)^2 }{2 \\alpha_t (1 - \\bar{\\alpha}_t) \\| \\boldsymbol{\\Sigma}_\\theta \\|^2_2} \\|\\boldsymbol{\\epsilon}_t - \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)\\|^2 \\Big] \\\\ &amp;= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon} } \\Big[\\frac{ (1 - \\alpha_t)^2 }{2 \\alpha_t (1 - \\bar{\\alpha}_t) \\| \\boldsymbol{\\Sigma}_\\theta \\|^2_2} \\|\\boldsymbol{\\epsilon}_t - \\boldsymbol{\\epsilon}_\\theta(\\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon}_t, t)\\|^2 \\Big] \\end{aligned}\\] 发现可以使用不用权重的简单形式就可以训练得到好的结果，即 \\[\\begin{aligned} L_\\text{simple} &amp;= \\mathbb{E}_{t \\sim [1, T], \\mathbf{x}_0, \\boldsymbol{\\epsilon}_t} \\Big[\\|\\boldsymbol{\\epsilon}_t - \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)\\|^2 \\Big] \\\\ &amp;= \\mathbb{E}_{t \\sim [1, T], \\mathbf{x}_0, \\boldsymbol{\\epsilon}_t} \\Big[\\|\\boldsymbol{\\epsilon}_t - \\boldsymbol{\\epsilon}_\\theta(\\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon}_t, t)\\|^2 \\Big] \\end{aligned}\\] 这样，我们就获得了DDPM的最终目标函数： \\[ L_\\text{simple}(\\theta)=\\mathbb E_{t,x_0,\\epsilon}\\left[\\Vert\\epsilon-\\epsilon_\\theta(x_t,t)\\Vert^2\\right]\\] 具体训练流程和采样流程如下： DDIM DDPM虽好，但它只能一步一步老老实实通过 \\(x_{t}\\) 预测 \\(x_{t-1}\\) ，不能跨步运算，如果 \\(T =1000\\) ，那么生成一整图像就需要用网络推理1000次，效率很低。于是为了结局这个问题，DDIM出现了，而且最巧妙的是它不需要重新训练模型。 DDIM始于一个假设，它假设了 \\[P(x_{prev}|x_t,x_0)\\sim N(kx_0+mx_t,\\sigma_2)\\] \\[x_{prev}=kx_0+mx_t+\\sigma\\epsilon,\\ \\ \\ \\ \\ \\epsilon\\sim N(0,1)\\] 又因为加噪过程满足公式 \\(x_t=\\sqrt {\\bar\\alpha_t}x_0+\\sqrt{1-\\bar\\alpha_{t} }\\epsilon\\) 把 \\(x_t\\) 带入 \\(x_{t-1}\\) 合并同类项得到： \\[\\begin{align*} x_{prev}&amp;=kx_0+m(\\sqrt{\\bar\\alpha_t}x_0+\\sqrt{1-\\bar\\alpha_t}\\epsilon)+\\sigma\\epsilon\\\\ &amp;=(k+m\\sqrt{\\bar\\alpha_t})x_0+\\epsilon&#39; \\end{align*}\\] \\[\\epsilon&#39;\\sim N(0,m^2(1-\\bar\\alpha_t)+\\sigma^2)\\] 又因为 \\(x_{prev}=\\sqrt {\\bar\\alpha_{prev} }x_0+\\sqrt{1-\\bar\\alpha_{prev} }\\epsilon\\) ，满足对应系数相同，有： \\[k+m\\sqrt{\\bar\\alpha_t}=\\sqrt{\\bar{\\alpha_{prev} }}\\\\ m^2(1-\\bar\\alpha_t)+\\sigma^2=1-\\bar\\alpha_{prev}\\] 求得： \\[m=\\frac{\\sqrt{1-\\bar\\alpha_{prev}-\\sigma^2} }{\\sqrt{1-\\bar\\alpha_t} }\\\\ k=\\sqrt{\\bar\\alpha_{prev} }-\\frac{\\sqrt{1-\\bar\\alpha_{prev}-\\sigma^2} }{\\sqrt{1-\\bar\\alpha_t} }\\sqrt{\\bar\\alpha_t}\\] 带入公式最终化简得： \\[x_{prev}=\\sqrt{\\bar{\\alpha_{prev} }} (\\frac{x_t-\\sqrt{1-\\bar\\alpha_t}\\epsilon_t}{\\sqrt{\\bar\\alpha_t} }) +\\sqrt{1-\\bar\\alpha_{prev}-\\sigma^2}\\epsilon_t+\\sigma^2\\epsilon\\] 其中 \\(t\\) 和 \\(prev\\) 可以相隔多个迭代步数，一般相隔20可以做到采样速度和采样质量比较好地平衡。所以一般DDPM要做1000步，而DDIM是需要50步就可以完成采样。 当这里的 \\(\\sigma\\) 选取0的时候，也就意味着变成了一个确定性采样的过程。此时的DDIM就变成了一个Flow Models，事实上论文里也是这么做的。 从不同角度看扩散模型 前面我们DDPM的推导过程中，其实可以把扩散模型看成一个给定后验的多层VAE。即认为设定了 \\(p(x_{1:T}|x_0)\\) 的形式，然后让模型来从潜变量中采样，最终生成图片。 而DDIM把这个过程变成了一个确定性过程，也就是说把潜变量和数据之间做了一个双射，所以此时也就可以看成Flow Models的一个了 事实上，扩散模型的连续和离散其实对应着随机过程里的概念。一般来说，discrete time指的是随机过程中的时间 \\(t\\) 只能取离散整数值，而continous-time则指的是时间参数 \\(t\\) 可以取连续值。discrete time随机过程中的参数在一个离散的时间点只能改变一次；而continuous-time随机过程的参数则可以随时发生变化。 DDPM和SDE 我们在DDPM里的加噪过程。每一个time step，我们都会按照如下的离散马尔可夫链进行加噪： \\[x_i = \\sqrt{1 - \\beta_i}x_{i-1} + \\sqrt{\\beta_i} \\epsilon_{i-1}, i=1,..., N\\] 为了将上述过程连续化，我们需要引入连续时间随机过程。而连续时间其实就是让每个离散的时间间隔 \\(\\Delta t\\) 无限趋近于0，其实也等价于求出 \\(N \\to \\infty\\) ​时，上述马尔可夫链的极限 在求极限之前，我们需要先引入一组辅助的noise scale \\(\\{\\bar{\\beta}_i = N \\beta_i\\}_{i=1}^N\\) ，并将上面的式子改写如下： \\[x_i = \\sqrt{1 - \\frac{\\bar{\\beta}_i}{N} }x_{i-1} + \\sqrt{\\frac{\\bar{\\beta}_i}{N} }\\epsilon_{i-1}, i = 1,..., N\\] 在 \\(N \\to \\infty\\) ​时，上面的 \\(\\{\\bar{\\beta}_i\\}_{i=1}^{N}\\) 就成了一个关于时间 \\(t\\) 的连续函数 $ (t)$ ​，并且 \\(t \\in [0, 1]\\) 。随后，我们可以假设 \\(\\Delta t = \\frac{1}{N}\\) ​，在每个 \\(i\\Delta t\\) 时刻，连续函数 \\(\\beta(t), x(t), \\epsilon(t)\\) 都等于之前的离散值，即： \\[\\beta(\\frac{i}{N}) = \\bar{\\beta}_i, x(\\frac{i}{N}) = x_i, \\epsilon(\\frac{i}{N})=\\epsilon_i \\] 在 \\(t \\in \\{0, 1, ..., \\frac{N-1}{N}\\}\\) ​以及 \\(\\Delta t=\\frac{1}{N}\\) 的情况下，我们就可以用连续函数改写之前的式子： \\[\\begin{align} x(t+ \\Delta t) &amp;= \\sqrt{1-\\beta(t+\\Delta t)\\Delta t}\\ x(t) + \\sqrt{\\beta(t+\\Delta t)\\Delta t}\\ \\epsilon(t) \\\\ &amp; \\approx x(t) - \\frac{1}{2}\\beta(t+\\Delta t) \\Delta t\\ x(t) + \\sqrt{\\beta(t+\\Delta t)\\Delta t}\\ \\epsilon(t) \\\\ &amp; \\approx x(t) - \\frac{1}{2}\\beta(t)\\Delta t\\ x(t) + \\sqrt{\\beta(t)\\Delta t}\\ \\epsilon(t) \\end{align} \\] 上面的近似只有在 \\(\\Delta t \\ll 1\\) 时成立。我们将其再移项后就可以得到下式： \\[x(t+\\Delta t) - x(t) \\approx -\\frac{1}{2} \\beta(t)\\Delta t\\ x(t) + \\sqrt{\\beta(t)\\Delta t}\\ \\epsilon(t) \\\\ \\mathrm{d} x = -\\frac{1}{2}\\beta(t)x \\mathrm{d}t + \\sqrt{\\beta(t)} \\mathrm{d}w \\] 其中， \\(w\\) ​表示的就是Wiener Process。这里面的第二个式子，就是一SDE方程。 至此，我们证明了DDPM连续化之后，就可以得到一个SDE方程，并且它是一种Variance Preserving的SDE。Variance Preserving的含义是当 \\(t \\to \\infty\\) 时，它的方差依然有界。 与此反向过程也是一个SDE方程，称为reverse SDE： \\[\\text{d}\\mathbf{x}= [\\mathbf{f}(\\mathbf{x}, t) - g^2(t)\\nabla _{\\mathbf{x} }\\log p(\\mathbf{x})]\\text{d}\\mathbf{t} + g(t)\\text{d}\\mathbf{w}\\] 这个反向过程中的未知量就只有分数函数 $ x p{t}(x)$ ​。至此，DDPM和分数模型也产生了联系，实际上二者之间是相互等价的。而DDPM和分数模型本质上都是在学习这个reverse SDE的解。 我们可以看到，DDPM每一步的去噪其实本质上与Annealed Langevin dynamics是一模一样的。 DDIM与ODE 首先对于一个SDE， \\[\\text{d}\\mathbf{x}= \\mathbf{f}(\\mathbf{x}, t)\\text{d}\\mathbf{t} + g(t)\\text{d}\\mathbf{w}\\] 我们写出它的福克-普朗克方程（Fokker-Planck equation）： \\[\\begin{align*} \\nabla _{t}p(\\mathbf{x}, t) &amp;= -\\nabla _{\\mathbf{x} }[\\mathbf{f}(\\mathbf{x}, t)p(\\mathbf{x}, t)] + \\frac{1}{2}g^{2}(t)\\nabla _{\\mathbf{x} }^{2}p(\\mathbf{x}, t)\\\\ &amp;= -\\nabla _{\\mathbf{x} }[\\mathbf{f}(\\mathbf{x}, t)p(\\mathbf{x}, t) - \\frac{1}{2}(g^{2}(t) - \\sigma^{2}(t))\\nabla_\\mathbf{x}p(\\mathbf{x}, t)] + \\frac{1}{2}\\sigma^{2}(t)\\nabla _{\\mathbf{x} }^{2}p(\\mathbf{x}, t)\\\\ &amp;= -\\nabla _{\\mathbf{x} }[(\\mathbf{f}(\\mathbf{x}, t) - \\frac{1}{2}(g^{2}(t) - \\sigma^{2}(t))\\nabla_\\mathbf{x}\\log p(\\mathbf{x}, t))p(\\mathbf{x})] + \\frac{1}{2}\\sigma^{2}(t)\\nabla _{\\mathbf{x} }^{2}p(\\mathbf{x}, t)\\\\\\end{align*}\\] 现在我们把福克-普朗克方程变成了这样： \\[\\nabla_{t}p(\\mathbf{x}, t) = -\\nabla_{\\mathbf{x} }[(\\mathbf{f}(\\mathbf{x}, t) - \\frac{1}{2}(g^{2}(t) - \\sigma^{2}(t))\\nabla_\\mathbf{x}\\log p(\\mathbf{x}, t))p(\\mathbf{x})] + \\frac{1}{2}\\sigma^{2}(t)\\nabla _{\\mathbf{x} }^{2}p(\\mathbf{x}, t)\\] 其对应的SDE为： \\[\\text{d}\\mathbf{x}= [\\mathbf{f}(\\mathbf{x}, t) - \\frac{1}{2}(g^{2}(t) - \\sigma^{2}(t))\\nabla_{\\mathbf{x} }\\log p_{t}(\\mathbf{x})]\\text{d}\\mathbf{t} + \\sigma(t)\\text{d}\\mathbf{w}\\] 因为前后两个SDE是等价的，他们对应的 \\(p_{t}(\\mathbf{x})\\) 是一样的，意味着我们可以改变第二个SDE的方差 \\(\\sigma(t)\\) 。当我们取 \\(\\sigma(t)=0\\) ，可以得到一个常微分方程(Ordinary Differential Equation, ODE), \\[\\text{d}\\mathbf{x}= [\\mathbf{f}(\\mathbf{x}, t) - \\frac{1}{2}g^{2}(t)\\nabla_ {\\mathbf{x} }\\log p_{t}(\\mathbf{x})]\\text{d}\\mathbf{t}\\] 这个结论有什么作用呢？首先，我们其实更在乎的是边缘概率分布 \\(q_t(x)\\) ，因为我们需要保证它在足够长的时刻 \\(T\\) ， \\(q_T(x)\\) 可以变成一个纯噪声，同时我们还需要 \\(q_0(x)\\) ​符合原始数据分布。上述结论可以保证这一点。同时，扩散模型本质上是在学习一个扩散过程的逆过程，既然前向SDE存在一个对应的ODE，那么反向过程reverse SDE其实也有一个对应的ODE，这个反向过程对应的ODE形式也是上面的式子。 而 DDIM 恰是一种确定性情形，所以我们自然会想到——能不能用 ODE 来描述一个 DDIM 呢？答案是肯定的。DDIM的公式如下： \\[\\begin{align} x_{t-1}&amp;=\\sqrt{\\bar\\alpha_{t-1} }x_\\theta(x_t,t)+\\sqrt{1-\\bar\\alpha_{t-1} }\\epsilon_\\theta(x_t,t)\\\\ &amp;=\\frac{\\sqrt{\\bar\\alpha_{t-1} }}{\\sqrt{\\bar\\alpha_t} }\\left(x_t-\\sqrt{1-\\bar\\alpha_t}\\epsilon_\\theta(x_t,t)\\right)+\\sqrt{1-\\bar\\alpha_{t-1} }\\epsilon_\\theta(x_t,t) \\end{align}\\] 两边均减去 \\(x_t\\) ，得： \\[\\begin{align} x_{t-1}-x_t&amp;=\\frac{1}{\\sqrt{\\bar\\alpha_t} }\\left[\\left(\\sqrt{\\bar\\alpha_{t-1} }-\\sqrt{\\bar\\alpha_t}\\right)x_t-\\left(\\sqrt{\\bar\\alpha_{t-1}(1-\\bar\\alpha_t)}-\\sqrt{\\bar\\alpha_t(1-\\bar\\alpha_{t-1})}\\right)\\epsilon_\\theta(\\mathbf x_t,t)\\right]\\\\ &amp;=\\frac{1}{\\sqrt{\\bar\\alpha_t} }\\left(\\frac{\\bar\\alpha_{t-1}-\\bar\\alpha_t}{\\sqrt{\\bar\\alpha_{t-1} }+\\sqrt{\\bar\\alpha_t} }x_t-\\frac{\\bar\\alpha_{t-1}-\\bar\\alpha_t}{\\sqrt{\\bar\\alpha_{t-1}(1-\\bar\\alpha_t)}+\\sqrt{\\bar\\alpha_t(1-\\bar\\alpha_{t-1})} }\\epsilon_\\theta(x_t,t)\\right)\\\\ &amp;=\\frac{\\bar\\alpha_{t-1}-\\bar\\alpha_t}{\\sqrt{\\bar\\alpha_t} }\\left(\\frac{x_t}{\\sqrt{\\bar\\alpha_{t-1} }+\\sqrt{\\bar\\alpha_t} }-\\frac{\\epsilon_\\theta(\\mathbf x_t,t)}{\\sqrt{\\bar\\alpha_{t-1}(1-\\bar\\alpha_t)}+\\sqrt{\\bar\\alpha_t(1-\\bar\\alpha_{t-1})} }\\right) \\end{align}\\] 记 \\(x(t)=x_t,\\barα(t)=\\barα_t\\) ，将 \\(t-1\\) 换成 \\(t−Δt\\) 并令 \\(Δt→0\\) ，得： \\[\\mathrm dx=\\frac{\\mathrm d\\bar\\alpha(t)}{\\sqrt{\\bar\\alpha(t)} }\\left(\\frac{x(t)}{2\\sqrt{\\bar\\alpha(t)} }-\\frac{\\epsilon_\\theta(x(t),t)}{2\\sqrt{\\bar\\alpha(t)(1-\\bar\\alpha(t))} }\\right)=\\frac{\\bar\\alpha&#39;(t)}{2\\bar\\alpha(t)}\\left(x(t)-\\frac{\\epsilon_\\theta(x(t),t)}{\\sqrt{1-\\bar\\alpha(t)} }\\right)\\mathrm dt \\] 这就是 DDIM 的 ODE 描述。 在 DDPM 的设置下，有 $f(x,t)=−β(t)x,g(t)= $ ，代入 \\[\\text{d}\\mathbf{x}= [\\mathbf{f}(\\mathbf{x}, t) - \\frac{1}{2}g^{2}(t)\\nabla_ {\\mathbf{x} }\\log p_{t}(\\mathbf{x})]\\text{d}\\mathbf{t}\\] 得： \\[\\mathrm dx=\\left[-\\frac{1}{2}\\beta(t)x-\\frac{1}{2}\\beta(t)\\nabla_{\\mathbf{x} }\\log p_{t}(\\mathbf{x})\\right]\\mathrm dt=-\\frac{1}{2}\\beta(t)\\left[x+\\nabla_{\\mathbf{x} }\\log p_{t}(\\mathbf{x})\\right]\\mathrm dt\\] 与我们上面的式子对应。 既然引入了ODE，那么我们的模型就可以去学习如何解这个ODE，同时也可以引入各种传统的ODE solver例如：Euler method, Runge–Kutta method等一些方法。这就是为什么我们可以看到像Stable Diffusion之类的模型会有那么多sampler的原因，本质上都是一些ODE solver和SDE solver。但是后面的研究者发现，传统的ODE solver在采样效果上比不过DDIM，这就非常奇怪了。DPM-Solver的作者在他们的论文中给出了原因：DDIM充分利用了diffusion ODE的半线性结构（semi-linear structure），并且它是一个semi-linear ODE的一阶Solver，而传统的ODE solver并没有利用好这个半线性结构，因此DDIM的准确度会更高一些，因此采样效果也更好。 这里还需要注意的点是，diffusion ODE这类模型相比diffusion SDE存在着诸多好处，比如： 没有随机性，ODE是一个确定性过程，可以以更快的速度收敛，因此可以达到更快的采样速度 由于是确定性过程，可以计算数据似然（likelihood）等。","categories":[{"name":"Stanford CS236深度生成模型","slug":"Stanford-CS236深度生成模型","permalink":"https://jia040223.github.io/categories/Stanford-CS236%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}],"tags":[{"name":"生成模型","slug":"生成模型","permalink":"https://jia040223.github.io/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}]},{"title":"Score Based Models","slug":"Score Based Models","date":"2024-09-24T12:40:58.000Z","updated":"2024-09-25T10:50:29.370Z","comments":true,"path":"2024/09/24/Score Based Models/","permalink":"https://jia040223.github.io/2024/09/24/Score%20Based%20Models/","excerpt":"","text":"本学习笔记用于记录我学习Stanford CS236课程的学习笔记，分享记录，也便于自己实时查看。 引入 Score function 上一次我们学习了Energy Based Model。其核心做法是对一个数据集 \\({x_{1}, x_{2}, ..., x_{N}}\\) ，我们把数据的概率分布 \\(p(x)\\) 建模为： \\[p_{\\theta}(\\mathbf{x}) = \\frac{e^{-f_{\\theta}(\\mathbf{x})}}{Z_{\\theta}}\\] 这里 \\(f_{\\theta}(\\mathbf{x})\\in \\mathbb{R}\\) 。 \\(Z_{\\theta}\\) 是归一化项保证 \\(p_{\\theta}(\\mathbf{x})\\) 是概率。 \\(\\theta\\) 是他们的参数。 我们一般可以通过最大似然估计的方式来训练参数 \\(\\theta\\) ， \\[\\max_{\\theta}\\sum\\limits_{i=1}^{N}\\log_{\\theta}(\\mathbf{x}_{i})\\] 但是因为 \\[\\log p_{\\theta}(\\mathbf{x}) = -f_{\\theta}(\\mathbf{x}) - \\log Z_{\\theta}\\] \\(Z_{\\theta}\\) 是intractable的，我们无法求出 \\(\\log p_{\\theta}(\\mathbf{x})\\) ，自然也就无法优化参数 \\(\\theta\\) 。 为了解决归一化项无法计算的问题，我们引入score function。 score function的定义为 \\(\\nabla _{\\mathbf{x}}\\log p(\\mathbf{x})\\) 所以我们可以发现，score function是与 \\(Z _{\\theta}\\) 无关的： \\[\\mathbf{s}_{\\theta}(\\mathbf{x}) = \\nabla_{\\mathbf{x}}\\log(\\mathbf{x}_{\\theta}) = -\\nabla_{\\mathbf{x}}f_{\\theta}(\\mathbf{x}) - \\nabla_{\\mathbf{x}}\\log Z_{\\theta} = -\\nabla_{\\mathbf{x}}f _{\\theta}(\\mathbf{x})\\] Score Based Model Score matching 现在我们想要训练一个网络来估计出真实的score function。自然地，我们可以最小化真实的score function和网络输出的MSE： \\[\\mathcal{L} =\\frac{1}{2} \\mathbb{E}_{p(\\mathbf{x})}[||\\nabla_{\\mathbf{x}}\\log p(\\mathbf{x}) - \\mathbf{s} _{\\theta}(\\mathbf{x})||^{2}]\\] 但是这样的一个loss我们是算不出来的，因为我们并不知道真实的 \\(p(\\mathbf{x})\\) 是什么。而score matching方法就可以让我们在不知道真实的 \\(p(\\mathbf{x})\\) 的情况下最小化这个loss。Score matching的推导如下： 我们把上面loss的期望写开，二次项打开，可以得到 \\[\\begin{align*}\\mathcal{L} =&amp; \\frac{1}{2}\\mathbb{E}_{p(\\mathbf{x})}[||\\nabla _{\\mathbf{x}}\\log p(\\mathbf{x}) - \\mathbf{s} _{\\theta}(\\mathbf{x})||^{2}]\\\\=&amp; \\frac{1}{2}\\int p(\\mathbf{x}) [||\\nabla _{\\mathbf{x}}\\log p(\\mathbf{x})||^{2} + ||\\mathbf{s} _{\\theta}(\\mathbf{x})||^{2} - 2(\\nabla _{\\mathbf{x}}\\log p(\\mathbf{x}))^{T}\\mathbf{s} _{\\theta}(\\mathbf{x})] d \\mathbf{x}\\end{align*}\\] 第一项对于 \\(\\theta\\) 来说是常数可以忽略。 第二项为 \\[\\int p(\\mathbf{x}) ||\\mathbf{s} _{\\theta}(\\mathbf{x})||^{2} d \\mathbf{x}\\] 对于第三项，若 \\(\\mathbf{x}\\) 的维度为 \\(N\\) ： \\[ \\begin{align*}&amp; -2\\int p(\\mathbf{x}) (\\nabla _{\\mathbf{x}}\\log p(\\mathbf{x}))^{T}\\mathbf{s} _{\\theta}(\\mathbf{x}) d \\mathbf{x}\\\\ =&amp; -2 \\int p(\\mathbf{x}) \\sum\\limits_{i=1}^{N}\\frac{\\partial \\log p(\\mathbf{x})}{\\partial \\mathbf{x}_{i}}\\mathbf{s}_{\\theta i}(\\mathbf{x}) d \\mathbf{x}\\\\ =&amp; -2 \\sum\\limits_{i=1}^{N} \\int p(\\mathbf{x}) \\frac{1}{p(\\mathbf{x})} \\frac{\\partial p(\\mathbf{x})}{\\partial \\mathbf{x}_{i}}\\mathbf{s}_{\\theta i}(\\mathbf{x}) d \\mathbf{x}\\\\ =&amp; -2 \\sum\\limits_{i=1}^{N} \\int \\frac{\\partial p(\\mathbf{x})}{\\partial \\mathbf{x}_{i}}\\mathbf{s}_{\\theta i}(\\mathbf{x}) d \\mathbf{x}\\\\ =&amp; 2 \\sum\\limits_{i=1}^{N} - \\int \\frac{\\partial p(\\mathbf{x})\\mathbf{s}_{\\theta i}(\\mathbf{x})}{\\partial \\mathbf{x}_{i}} d \\mathbf{x} + \\int p(\\mathbf{x}) \\frac{\\partial \\mathbf{s}_{\\theta i}(\\mathbf{x})}{\\partial \\mathbf{x}_{i}} d \\mathbf{x}\\\\ =&amp; 2 \\sum\\limits_{i=1}^{N} - \\int p(\\mathbf{x})\\mathbf{s}_{\\theta i}(\\mathbf{x})\\bigg\\rvert^{\\infty}_{-\\infty} d \\mathbf{x_{/i}} + \\int p(\\mathbf{x}) \\frac{\\partial \\mathbf{s}_{\\theta i}(\\mathbf{x})}{\\partial \\mathbf{x}_{i}} d \\mathbf{x}\\\\ =&amp; 2 \\sum\\limits_{i=1}^{N} \\int p(\\mathbf{x}) \\frac{\\partial \\mathbf{s}_{\\theta i}(\\mathbf{x})}{\\partial \\mathbf{x}_{i}} d \\mathbf{x}\\\\ =&amp; 2\\int p(\\mathbf{x}) \\sum\\limits_{i=1}^{N} \\frac{\\partial \\mathbf{s}_{\\theta i}(\\mathbf{x})}{\\partial \\mathbf{x}_{i}} d \\mathbf{x}\\\\ =&amp; 2\\int p(\\mathbf{x}) \\text{tr}(\\nabla _{\\mathbf{x}}\\mathbf{s}_{\\theta}(\\mathbf{x})) d \\mathbf{x}\\end{align*} \\] 所以最后的loss是第二和第三项的和： \\[ \\begin{align*} \\mathcal{L} &amp;=\\frac{1}{2} \\int p(\\mathbf{x}) ||\\mathbf{s} _{\\theta}(\\mathbf{x})||^{2} d \\mathbf{x} + \\int p(\\mathbf{x}) \\text{tr}(\\nabla _{\\mathbf{x}}\\mathbf{s}_{\\theta}(\\mathbf{x})) d \\mathbf{x}\\\\\\\\ &amp;= \\mathbb{E}_{p(\\mathbf{x})}[\\frac{1}{2}||\\mathbf{s} _{\\theta}(\\mathbf{x})||^{2} + \\text{tr}(\\nabla _{\\mathbf{x}}\\mathbf{s}_{\\theta}(\\mathbf{x}))]\\end{align*} \\] 当然，这个推导虽然是从能量模型引入的，但并不局限于能量模型，事实上，他是一个更大的模型家族。 Score Matching Langevin Dynamics (SMLD) 现在我们已经通过神经网络学习到了数据分布的score function，那么如何用score function从这个数据分布中得到样本呢？答案就是朗之万动力学采样(Langevin Dynamics): \\[ \\mathbf{x}_{i+1} = \\mathbf{x}_{i} + \\epsilon \\nabla_{\\mathbf{x}}\\log p(\\mathbf{x}) + \\sqrt{2 \\epsilon}\\mathbf{z}_{i}, \\quad \\mathbf{z} _{i} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}), \\quad i=0,1,\\cdots K\\ \\] 这里的采样是一个迭代的过程。 \\(\\epsilon\\) 是一个很小的量。 \\(\\mathbf{x}_{0}\\) 随机初始，通过上面的迭代式更新。当迭代次数 \\(K\\) 足够大的时候， \\(\\mathbf{x}\\) 就收敛于该分布的一个样本。 上图的具体解释我就不再赘述了。 这样我们其实就得到了一个生成模型。我们可以先训练一个网络用来估计score function，然后用Langevin Dynamics和网络估计的score function采样，就可以得到原分布的样本。因为整个方法由score matching和Langevin Dynamics两部分组成，所以叫SMLD。 训练 说完了损失函数和采样过程，那么对这个模型我们怎么训练呢？相信敏锐的读者已经注意到了，我们损失函数： \\[ \\begin{align*} \\mathcal{L} &amp;= \\mathbb{E}_{p(\\mathbf{x})}[\\frac{1}{2}||\\mathbf{s} _{\\theta}(\\mathbf{x})||^{2} + \\text{tr}(\\nabla _{\\mathbf{x}}\\mathbf{s}_{\\theta}(\\mathbf{x}))]\\end{align*} \\] 这个第二项并不是很好计算。对于维度为 \\(N\\) 的数据，我们计算雅可比矩阵的迹需要进行 \\(N\\) 次反向传播，这对于高维度的数据的训练是不能接受的。 对于这个问题，主要有两种解决方法。 Denoising score matching Denoising score matching的做法就是在 score matching 的基础上，对输入数据加噪。需要注意的是，此时的 score 是对加噪后的数据进行求导，而非原输入数据。score 的方向是(对数)概率密度增长最快的方向，也就是最接近真实数据的方向。 Denoising score matching 的玩法是：在给定输入 \\(x\\) 的情况下，将条件分布$ q(|x)$建模为高斯分布，其中 \\(\\tilde{x}\\) 代表加噪后的数据，并且边缘化这个条件分布，以 \\(p(\\tilde{x}) \\equiv \\int q(\\tilde{x}|x)p(x) dx\\) 来近似原数据分布，因此噪声强度不太大时，我们可以认为加噪后数据的概率分布与原数据的概率分布大致相同。 此时，score \\(\\frac{\\partial log(p(\\tilde{x}))}{\\partial \\tilde{x}}\\) 中由于 $ p(x)$ 项在求导时与 \\(\\tilde{x}\\) 无关，可以略去了，具体推导如下： \\[ \\begin{align*} \\frac{1}{2} \\mathbb{E}_{\\tilde{x} \\sim q_{\\sigma}} \\left[ \\| \\nabla_{\\tilde{x}} \\log q_{\\sigma}(\\tilde{x}) - s_{\\theta}(\\tilde{x}) \\|_2^2 \\right] &amp;= \\frac{1}{2} \\int q_{\\sigma}(\\tilde{x}) \\| \\nabla_{\\tilde{x}} \\log q_{\\sigma}(\\tilde{x}) - s_{\\theta}(\\tilde{x}) \\|_2^2 d\\tilde{x} \\\\ &amp;= \\frac{1}{2} \\int q_{\\sigma}(\\tilde{x}) \\| \\nabla_{\\tilde{x}} \\log q_{\\sigma}(\\tilde{x}) \\|_2^2 d\\tilde{x} + \\frac{1}{2} \\int q_{\\sigma}(\\tilde{x}) \\| s_{\\theta}(\\tilde{x}) \\|_2^2 d\\tilde{x}- \\int q_{\\sigma}(\\tilde{x}) \\nabla_{\\tilde{x}} \\log q_{\\sigma}(\\tilde{x})^T s_{\\theta}(\\tilde{x}) d\\tilde{x} \\end{align*} \\] 这里一样的，第一项是常数，第二项只涉及 \\(s_{\\theta}(\\tilde{x})\\) ，我们可以处理，第三项比较棘手。但我们可以类似地用分布积分法进行处理： \\[ \\begin{align*} &amp;- \\int q_{\\sigma}(\\tilde{x}) \\nabla_{\\tilde{x}} \\log q_{\\sigma}(\\tilde{x})^T s_{\\theta}(\\tilde{x}) d\\tilde{x} \\\\ &amp;= - \\int q_{\\sigma}(\\tilde{x}) \\frac{1}{q_{\\sigma}(\\tilde{x})} \\nabla_{\\tilde{x}} q_{\\sigma}(\\tilde{x})^T s_{\\theta}(\\tilde{x}) d\\tilde{x} \\\\ &amp;= - \\int \\nabla_{\\tilde{x}} q_{\\sigma}(\\tilde{x})^T s_{\\theta}(\\tilde{x}) d\\tilde{x} \\\\ &amp;= - \\int \\nabla_{\\tilde{x}} \\left( \\int p_{\\text{data}}(x) q_{\\sigma}(\\tilde{x} | x) dx \\right)^T s_{\\theta}(\\tilde{x}) d\\tilde{x} \\\\ &amp;= - \\int \\left( \\int p_{\\text{data}}(x) \\nabla_{\\tilde{x}} q_{\\sigma}(\\tilde{x} | x) dx \\right)^T s_{\\theta}(\\tilde{x}) d\\tilde{x} \\\\ &amp;= - \\int \\left( \\int p_{\\text{data}}(x) q_{\\sigma}(\\tilde{x} | x) \\nabla_{\\tilde{x}} \\log q_{\\sigma}(\\tilde{x} | x) dx \\right)^T s_{\\theta}(\\tilde{x}) d\\tilde{x} \\\\ &amp;= - \\int \\int p_{\\text{data}}(x) q_{\\sigma}(\\tilde{x} | x) \\nabla_{\\tilde{x}} \\log q_{\\sigma}(\\tilde{x} | x)^Ts_{\\theta}(\\tilde{x}) dx \\ d\\tilde{x} \\end{align*} \\] 这里我们 \\(q(\\tilde{x}|x)\\) 是已知的，也就可以计算了。 OK，让我们代入原式之中： \\[ \\begin{align*} &amp;\\frac{1}{2} \\mathbb{E}_{\\tilde{\\mathbf{x}} \\sim q_{\\sigma}} \\left[ \\|\\nabla_{\\tilde{\\mathbf{x}}} \\log q_{\\sigma} (\\tilde{\\mathbf{x}}) - s_{\\theta} (\\tilde{\\mathbf{x}}) \\|_2^2 \\right] \\\\ &amp;= \\text{const.} + \\frac{1}{2} \\mathbb{E}_{\\mathbf{x} \\sim q_{\\sigma}} \\left[ \\| s_{\\theta} (\\mathbf{x}) \\|_2^2 \\right] - \\int q_{\\sigma} (\\tilde{\\mathbf{x}}) \\nabla_{\\tilde{\\mathbf{x}}} \\log q_{\\sigma} (\\tilde{\\mathbf{x}})^{\\top} s_{\\theta} (\\tilde{\\mathbf{x}}) d\\tilde{\\mathbf{x}} \\\\ &amp;= \\text{const.} + \\frac{1}{2} \\mathbb{E}_{\\mathbf{x} \\sim q_{\\sigma}} \\left[ \\| s_{\\theta} (\\tilde{\\mathbf{x}}) \\|_2^2 \\right] - \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}(\\mathbf{x}), \\tilde{\\mathbf{x}} \\sim q_{\\sigma}(\\tilde{\\mathbf{x}}|\\mathbf{x})} \\left[ \\nabla_{\\tilde{\\mathbf{x}}} \\log q_{\\sigma} (\\tilde{\\mathbf{x}}|\\mathbf{x})^{\\top} s_{\\theta} (\\tilde{\\mathbf{x}}) \\right] \\\\ &amp;= \\text{const.} + \\frac{1}{2} \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}(\\mathbf{x}), \\tilde{\\mathbf{x}} \\sim q_{\\sigma}(\\tilde{\\mathbf{x}}|\\mathbf{x})} \\left[ \\| s_{\\theta} (\\tilde{\\mathbf{x}}) - \\nabla_{\\tilde{\\mathbf{x}}} \\log q_{\\sigma} (\\tilde{\\mathbf{x}}|\\mathbf{x}) \\|_2^2 \\right] - \\frac{1}{2} \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}(\\mathbf{x}), \\tilde{\\mathbf{x}} \\sim q_{\\sigma}(\\tilde{\\mathbf{x}})} \\left[ \\| \\nabla_{\\tilde{\\mathbf{x}}} \\log q_{\\sigma} (\\tilde{\\mathbf{x}}) \\|_2^2 \\right] \\\\ &amp;= \\text{const.} + \\frac{1}{2} \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}(\\mathbf{x}), \\tilde{\\mathbf{x}} \\sim q_{\\sigma}(\\tilde{\\mathbf{x}}|\\mathbf{x})} \\left[ \\| s_{\\theta} (\\tilde{\\mathbf{x}}) - \\nabla_{\\tilde{\\mathbf{x}}} \\log q_{\\sigma} (\\tilde{\\mathbf{x}}|\\mathbf{x}) \\|_2^2 \\right] + \\text{const.} \\end{align*} \\] 看到没有！这也就是说，score 的方向与所加噪声的方向是相反的。 于是，在 denoising score matching 的体制下，朝着 score 的方向走，其实就是在去噪，在做 denoising。 在实践中，我们可以选择将 \\(q(\\tilde{x}|x)\\) 建模为 \\(N(\\tilde{x};x;\\sigma^2)\\) ，即均值为原数据 \\(x\\) ，方差为预设的 \\(\\sigma^2\\) 的高斯分布。于是，根据高斯分布的性质，有： \\[\\tilde{x}=x + \\sigma \\epsilon, \\epsilon\\sim N(0,I)\\] 其中， \\(\\epsilon\\) 是从标准高斯分布中采样出来的噪声。 接着，在以上化简出的 score 中代入高斯分布的概率密度函数，可以得到 score 为： \\[\\frac{\\partial log (q(\\tilde{x}|x))}{\\partial \\tilde{x}} = -(\\frac{\\tilde{x}-x}{\\sigma^2})=-\\frac{\\epsilon}{\\sigma}\\] 虽然我们对计算进行了大幅度简化，但这也导致了我们估计的是加噪数据的梯度。具体训练流程如下： Sliced score matching Sliced score matching的思想是，如果模型预测的梯度与真实梯度相同等价于他们在不同方向下的投影均相同，所以我们引入一个投影向量用于训练。这样我们的目标和最终化简（用分部积分即可）的格式如下： goal： \\[ \\frac{1}{2} \\mathbb{E}_{\\mathbf{v} \\sim p_v} \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}} \\left[ \\left( \\mathbf{v}^{\\top} \\nabla_{\\mathbf{x}} \\log p_{\\text{data}} (\\mathbf{x}) - \\mathbf{v}^{\\top} s_{\\theta} (\\mathbf{x}) \\right)^2 \\right] \\] loss： \\[\\mathbb{E}_{\\mathbf{v} \\sim p_v} \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}} \\left[ \\mathbf{v}^{\\top} \\nabla_{\\mathbf{x}} s_{\\theta} (\\mathbf{x}) \\mathbf{v} + \\frac{1}{2} (\\mathbf{v}^{\\top} s_{\\theta} (\\mathbf{x}))^2 \\right] \\] 这样我们便只需要进行一次反向传播了，大大减少了训练需要的计算量，计算图如下： 具体训练过程如下： 虽然这种方法的训练计算量会比Denoising score matching大，但它是对真实数据梯度进行的估计 问题 现在我们得到了SMLD生成模型，但实际上这个模型由很大的问题。首先看一下其在实践中的效果： 可以看到效果并不好。我们不妨从损失函数来分析一下原因： \\[ \\mathcal{L} = \\mathbb{E}_{p(\\mathbf{x})}[||\\nabla_{\\mathbf{x}}\\log p(\\mathbf{x}) - \\mathbf{s}_{\\theta}(\\mathbf{x})||^{2}] = \\int p(\\mathbf{x})||\\nabla_{\\mathbf{x}}\\log p(\\mathbf{x}) - \\mathbf{s} _{\\theta}(\\mathbf{x})||^{2} d \\mathbf{x}\\ \\] 观察我们用来训练神经网络的损失函数，我们可以发现这个L2项其实是被 \\(p(\\mathbf{x})\\) 加权了。所以对于低概率的区域，估计出来的score function就很不准确： 对于上面这张图来说，只有在高概率的红色区域，loss才高，score function可以被准确地估计出来。但如果我们采样的初始点在低概率区域的话，因为估计出的score function不准确，很有可能生成不出真实分布的样本。 此外，在现实中，比如对于图片来说，其往往是分布在一个低维度流型上，也就是大部分空间的概率密度几乎为0，此时我们的梯度定义已经失去了意义： 同时，我们通过Langevin Dynamics进行采样并不能很好还原聚点的样本比： SMLD的改进 那怎么样才能解决上面的问题呢？Denoising score matching给我们给了一定的启发。 其实可以通过给数据增加噪声扰动的方式扩大高概率区域的面积。给原始分布加上高斯噪声，原始分布的方差会变大。这样相当于高概率区域的面积就增大了，更多区域的score function可以被准确地估计出来。 但是噪声扰动的强度如何控制是个问题： 强度太小起不到效果，高概率区域的面积还是太小 强度太大会破坏数据的原始分布，估计出来的score function就和原分布关系不大了 所以噪声强度越高，高概率区域面积越大，训练得到的梯度越准，但与原始数据的梯度差距也就越大。所以我们不妨加不同程度的噪声，让网络可以学到加了不同噪声的原始分布的score function。这样既保证了原始低概率密度地区能学习到有效的梯度，同时原始高概率密度区的梯度估计是准确的。 说起来很拗口，其实很好理解。我们定义序列 \\({\\sigma_{1 \\sim L}} , \\quad \\sigma {1} \\lt \\sigma {2} \\lt \\cdots \\lt \\sigma _{L}\\) ，代表从小到大的噪声强度。这样我们可以定义经过噪声扰动之后的数据样本，服从一个经过噪声扰动之后的分布， \\[ \\mathbf{x} + \\sigma_{i}\\mathbf{z} = \\int p(\\mathbf{y}) \\mathcal{N}(\\mathbf{x}|\\mathbf{y}, \\sigma {i}^{2}\\mathbf{I})d \\mathbf{y}\\ \\] 我们用神经网络来估计经过噪声扰动过的分布的score function，并把噪声强度 \\(\\sigma_i\\) 作为一个输入： \\[ \\mathcal{L} = \\frac{1}{L}\\sum_\\limits {i=1}^{L} \\lambda (i) \\mathbb{E}_{p _{\\sigma {i}}(\\mathbf{x})}[||\\nabla_{\\mathbf{x}}\\log p_{\\sigma _ {i}}(\\mathbf{x}) - \\mathbf{s} _{\\theta}(\\mathbf{x, \\sigma_i})||^{2}] \\] 其中 \\(\\lambda(i)\\) 是权重，在实践中可以取 \\(\\sigma_{i}^{2}\\) 采样方式也要做出相应的变化，我们对于不同的噪声强度 \\(L, L-1, \\cdots, 1\\) 做Langevin采样，上一个scale的结果作为这一次的初始化。这样我们每一次的初始化都能在梯度估计的有效区域。 这种采样方式也叫做Annealed Langevin dynamics，具体训练流程如下： 从离散到连续 当我们做Langevin dynamics迭代次数足够多时，我们可以用随机微分方程(Stochastic Differential Equation, SDE)来建模这个采样过程。 \\[\\mathbf{x}_{i+1} = \\mathbf{x}_{i} + \\epsilon \\nabla_{\\mathbf{x}}\\log p(\\mathbf{x}_i) + \\sqrt{2 \\epsilon}\\mathbf{z}_{i}, \\quad i=0,1,\\cdots K\\] 当 \\(K\\to\\infty\\) 时，我们定义 \\(\\Delta t = \\epsilon,\\; \\Delta t \\to 0\\) \\[\\mathbf{x}_{t+\\Delta t} - \\mathbf{x}_{t}= \\nabla_{\\mathbf{x}}\\log p(\\mathbf{x}_i)\\Delta t + \\sqrt{2 \\Delta t}\\mathbf{z}_{i}\\] 我们将 \\(\\nabla _{\\mathbf{x}}\\log p(\\mathbf{x}_i)\\) 和 \\(\\sqrt{2}\\) 一般化为 \\(\\mathbf{f}(\\mathbf{x}, t)\\) 和 \\(g(t)\\) ，这样上面就变成了 \\[\\mathbf{x} _{t+\\Delta t} - \\mathbf{x}_{t}= \\mathbf{f}(\\mathbf{x}, t)\\Delta t + g(t) \\sqrt{\\Delta t}\\mathbf{z} _{i}\\] 其中 \\[\\sqrt{\\Delta t}\\mathbf{z} _{i} \\sim \\mathcal{N}(\\mathbf{0}, \\Delta t\\mathbf{I})\\] 这里可以引入布朗运动，如果我们定义 \\(\\mathbf{w}\\) 是一个布朗运动，那么 \\[ \\begin{gather*}\\mathbf{w}_{t+\\Delta t} = \\mathbf{w}_{t} + \\mathcal{N}(\\mathbf{0}, \\Delta t\\mathbf{I}),\\\\ \\sqrt{\\Delta t}\\mathbf{z} _{i} = \\mathbf{w}_{t+\\Delta t} - \\mathbf{w}_{t}.\\end{gather*} \\] 讲布朗运动带入到上面，得到 \\[\\mathbf{x}_{t+\\Delta t} - \\mathbf{x}_{t}= \\mathbf{f}(\\mathbf{x}, t)\\Delta t + g(t)(\\mathbf{w}_{t+\\Delta t} - \\mathbf{w}_{t})\\] 当 \\(\\Delta t \\to 0\\) , \\[\\text{d}\\mathbf{x}= \\mathbf{f}(\\mathbf{x}, t)\\text{d}\\mathbf{t} + g(t)\\text{d}\\mathbf{w}\\] 这里 \\(\\mathbf{f}(\\mathbf{x}, t)\\) 叫做drift coefficient, \\(g(t)\\) 代表diffusion coefficient。SDE的解也就代表了数据不断加噪声的过程。 有了正向过程的SDE，我们可以得到 反向的SDE \\[\\text{d}\\mathbf{x}= [\\mathbf{f}(\\mathbf{x}, t) - g^2(t)\\nabla _{\\mathbf{x}}\\log p(\\mathbf{x})]\\text{d}\\mathbf{t} + g(t)\\text{d}\\mathbf{w}\\] 以及score matching的损失函数 \\[\\mathbb{E}_{t\\in \\mathcal{U}(0, T)} \\mathbb{E}_{p_{t}(\\mathbf{x})}[g^2(t)||\\nabla_{\\mathbf{x}}\\log p_t(\\mathbf{x}) - \\mathbf{s}_{\\theta}(\\mathbf{x})||^2]\\] 可以看到，当我们知道了score后，就能解这个反向的SDE了。 整个基于SDE框架就是：我们在正向过程在图像中加噪声训练神经网络做score matching，估计出score function。然后在反向过程中从高斯噪声通过逆向SDE过程生成出数据分布的样本。 从SDE到ODE 对于一个SDE， \\[\\text{d}\\mathbf{x}= \\mathbf{f}(\\mathbf{x}, t)\\text{d}\\mathbf{t} + g(t)\\text{d}\\mathbf{w}\\] 我们写出它的福克-普朗克方程（Fokker-Planck equation）： \\[ \\begin{align*} \\nabla _{t}p(\\mathbf{x}, t) &amp;= -\\nabla _{\\mathbf{x}}[\\mathbf{f}(\\mathbf{x}, t)p(\\mathbf{x}, t)] + \\frac{1}{2}g^{2}(t)\\nabla _{\\mathbf{x}}^{2}p(\\mathbf{x}, t)\\\\ &amp;= -\\nabla _{\\mathbf{x}}[\\mathbf{f}(\\mathbf{x}, t)p(\\mathbf{x}, t) - \\frac{1}{2}(g^{2}(t) - \\sigma^{2}(t))\\nabla_\\mathbf{x}p(\\mathbf{x}, t)] + \\frac{1}{2}\\sigma^{2}(t)\\nabla _{\\mathbf{x}}^{2}p(\\mathbf{x}, t)\\\\ &amp;= -\\nabla _{\\mathbf{x}}[(\\mathbf{f}(\\mathbf{x}, t) - \\frac{1}{2}(g^{2}(t) - \\sigma^{2}(t))\\nabla_\\mathbf{x}\\log p(\\mathbf{x}, t))p(\\mathbf{x})] + \\frac{1}{2}\\sigma^{2}(t)\\nabla _{\\mathbf{x}}^{2}p(\\mathbf{x}, t)\\\\\\end{align*} \\] 现在我们把福克-普朗克方程变成了这样： \\[ \\nabla_{t}p(\\mathbf{x}, t) = -\\nabla_{\\mathbf{x}}[(\\mathbf{f}(\\mathbf{x}, t) - \\frac{1}{2}(g^{2}(t) - \\sigma^{2}(t))\\nabla_\\mathbf{x}\\log p(\\mathbf{x}, t))p(\\mathbf{x})] + \\frac{1}{2}\\sigma^{2}(t)\\nabla _{\\mathbf{x}}^{2}p(\\mathbf{x}, t) \\] 其对应的SDE为： \\[ \\text{d}\\mathbf{x}= [\\mathbf{f}(\\mathbf{x}, t) - \\frac{1}{2}(g^{2}(t) - \\sigma^{2}(t))\\nabla_{\\mathbf{x}}\\log p_{t}(\\mathbf{x})]\\text{d}\\mathbf{t} + \\sigma(t)\\text{d}\\mathbf{w} \\] 因为前后两个SDE是等价的，他们对应的 \\(p_{t}(\\mathbf{x})\\) 是一样的，意味着我们可以改变第二个SDE的方差 \\(\\sigma(t)\\) 。当我们取 \\(\\sigma(t)=0\\) ，可以得到一个常微分方程(Ordinary Differential Equation, ODE), \\[\\text{d}\\mathbf{x}= [\\mathbf{f}(\\mathbf{x}, t) - \\frac{1}{2}g^{2}(t)\\nabla_{\\mathbf{x}}\\log p_{t}(\\mathbf{x})]\\text{d}\\mathbf{t}\\] 下图就展示了SDE和ODE解的过程，可以看到ODE的轨迹是确定光滑的，而SDE的轨迹是随机的。这两个过程中的任意边缘分布 \\({p_{t}(\\mathbf{x})}_{t\\in[0, T]}\\) 都是一样的。 ODE形式有它的优点在于： 因为ODE比SDE好解，所以ODE的采样速度更快。 因为ODE是不带随机噪声的，整个过程是确定的，是可逆的，所以这个ODE也可以看做Normalizing flows，可以用来估计概率密度和似然。 但同时由于没有了随机噪声，可能导致多样性更差，实践中生成效果也不如SDE。","categories":[{"name":"Stanford CS236深度生成模型","slug":"Stanford-CS236深度生成模型","permalink":"https://jia040223.github.io/categories/Stanford-CS236%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}],"tags":[{"name":"生成模型","slug":"生成模型","permalink":"https://jia040223.github.io/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}]},{"title":"Energy Based Models","slug":"Energy Based Models","date":"2024-09-20T11:46:30.000Z","updated":"2024-09-25T11:18:58.626Z","comments":true,"path":"2024/09/20/Energy Based Models/","permalink":"https://jia040223.github.io/2024/09/20/Energy%20Based%20Models/","excerpt":"","text":"本学习笔记用于记录我学习Stanford CS236课程的学习笔记，分享记录，也便于自己实时查看。 引入 生成模型的核心目标是对目标样本的概率分布进行预测。而对于一个概率密度函数 \\(P(x)\\) ，它只需要满足下面两个条件： 非负， \\(P(x)\\) 在任何一个点都不能小于0，这很显然。 积分为1， \\(P(x)\\) 从负无穷积分到正无穷得是1。 其中对于第二点，如果 \\(P(x)\\) 的不是1，是 \\(Z\\) ，我们进行一下归一化，除一下 \\(Z\\) 就是1啦。反正至少得是有限的。那么如果我们有一个函数 \\(f(x)\\) ，我们只需要对其进行变换，满足上面两个特点，便能将其转化为一个概率密度函数。 首先可以让 \\(f(x)\\) 变为非负的 \\(g(x)\\) ，比如 \\(g(x) = f(x)^2\\) \\(g(x) = e^{f(x)}\\) \\(g(x) = log(1 + f(x)^2)\\) ...... 可以看到这样的选择有很多，然后接下来便是归一化了，只需要 \\[P(x) = \\frac{g(x)}{\\int g(x)dx} = \\frac{g(x)}{Z}\\] 那么所谓的Energy Based Model 呢，其实很简单，我们就是假设这个函数 \\(g(x) = e^{f(x)}\\) 。 这个时候，下面那个体积volume呢，也叫做partition function。 为啥要 \\(exp()\\) 呢？因为希望在算概率的时候取log，和这个 $ e x p ( ) $ 很多时候能够抵消。而且也和统计物理（虽然笔者并没有研究过统计物理）也有一些联系，这也是energy名字的最初由来。 基本定义 对数据的概率分布进行描述时，这些概率分布都可以写成基于能量函数的形式(energy funciton)， \\(f(\\mathbf x )\\) 。对于连续变量，每个数据点对应一个概率密度函数值，对应一个能量值，如此概率分布即可写成如下玻尔兹曼分布的形式，也叫作吉布斯分布(Boltzmann/Gibbs distribution)： \\(p(\\mathbf x )=\\frac{e^{f(\\mathbf x)}}{Z}\\) \\(Z\\) 为概率归一化的分母，也称为配分函数(partition function)， \\(Z=\\int e^{f(\\mathbf x)} dx\\) 由以上公式可知，概率值较高的位置对应着能力较低的点。举一个简单的例子看一下，将高斯分布以能量函数的形式表示： \\[f(x;\\mu,\\sigma^2)=-\\frac{1}{2\\sigma^2}(x-\\mu)^2\\] \\[p(x)=\\frac{e^{f(\\mathbf x)}}{\\int e^{f(\\mathbf x)}dx}=\\frac{e^{-\\frac{1}{2\\sigma^2}(x-\\mu)^2}}{\\sqrt {2\\pi \\sigma^2}}\\] 部分应用 分类任务 一般来说，这个partition funtion是不能算的，除非是限制为一些可以积分的函数，得到闭式解，但那样表达力又太弱了。而在实际中，我们的 \\(f(x)\\) 一般是用神经网络进行模拟的，所以很难求出这个积分（你也可以遍历所有的情况，但这对于训练或者推理都是无法接受的）。 有时候呢，除非是要算出具体的概率，我们不需要管这个partition function，反正就是知道它是个常数。 比如我们想知道 \\(p(a)\\) 和 \\(p(b)\\) 哪个概率大，就不用去知道绝对的值，只需要知道相对大小就可以啦。这就可以用在分类任务里了。 比如对于图像识别的任务，我只需要知道一个物体是更有可能像猫还是更有可能像狗，而不一定要知道他们的具体概率。 课程中还列举了一个Ising model 的例子，也很直观，不赘述了： 组合专家系统 通过EBMs，可以把多个专家模型混合起来，用乘法。在对模型采样的时候，就会具有多个生成模型的所有性质，比如又是女人又是年轻又是美貌，就不会生成一个年迈的男人。 受限玻尔兹曼机也是基于能量模型，能量形式如下: \\[f(\\mathbf x;\\theta)=exp(\\mathbf x ^T\\mathbf{Wx}+\\mathbf{b}^T\\mathbf{x} + \\mathbf{c}^T\\mathbf{z})\\] 其它就不赘述了。 训练 损失函数（训练目标） 那么如何优化这个模型，直接想法肯定是极大似然估计 \\[L = \\ log (\\frac{exp({f_\\theta(x_{train})})}{Z(\\theta)}) = f_\\theta(x_{train}) - log(Z(\\theta))\\] 这里有个小问题，直接最大化分子并不能解决问题，因为分母是分子的积分，如果只顾着最大化分子的话，可能分母也跟着变大，那最后这整个分数就可能不变甚至变小！但是积分我们又计算不出来怎么办？蒙特卡洛估计便可以派上用场了，我们对 \\(L\\) 求一下梯度： \\[ \\begin{align*} \\nabla_{\\theta} f_{\\theta}(x_{\\text{train}}) - \\nabla_{\\theta} \\log Z(\\theta) &amp;= \\nabla_{\\theta} f_{\\theta}(x_{\\text{train}}) - \\frac{\\nabla_{\\theta} Z(\\theta)}{Z(\\theta)} \\\\ &amp;= \\nabla_{\\theta} f_{\\theta}(x_{\\text{train}}) - \\frac{1}{Z(\\theta)} \\int \\nabla_{\\theta} \\exp \\{ f_{\\theta}(x) \\} dx \\\\ &amp;= \\nabla_{\\theta} f_{\\theta}(x_{\\text{train}}) - \\frac{1}{Z(\\theta)} \\int \\exp \\{ f_{\\theta}(x) \\} \\nabla_{\\theta} f_{\\theta}(x) dx \\\\ &amp;= \\nabla_{\\theta} f_{\\theta}(x_{\\text{train}}) - \\int \\frac{\\exp \\{ f_{\\theta}(x) \\}}{Z(\\theta)} \\nabla_{\\theta} f_{\\theta}(x) dx \\\\ &amp;= \\nabla_{\\theta} f_{\\theta}(x_{\\text{train}}) - \\mathbb{E}_{x_{\\text{sample}}} [\\nabla_{\\theta} f_{\\theta}(x_{\\text{sample}})] \\\\ &amp;= \\nabla_{\\theta} f_{\\theta}(x_{\\text{train}}) - \\nabla_{\\theta} f_{\\theta}(x_{\\text{sample}}) \\end{align*} \\] 其中 \\(x_{sample} \\sim exp(f_\\theta(x_{sample}))/Z_\\theta\\) 最后一步代表在训练过程中，我们只取一个样本作为期望的估计值。 其实主观上也很好理解，其实就是对比了训练集和从模型中的采样，让训练集中数据的概率比随便采样出来的概率大。 我们对上面公式取个负，就是损失函数了。 如何采样 那么问题来了，我们怎么从这个能量模型中采样呢？你看看上面能量模型的式子，你只知道x比y概率大还是概率小，但你不知道x或者y的准确概率。 这时候，MCMC马尔科夫链蒙特卡罗就出场啦。 这是课程对于MCMC的叙述，没明白的可以复习一下，其实就是MH算法： 课程中没强调这个noise是对称的，就是 \\(x\\) 到 \\(x&#39;\\) 的概率等于 \\(x&#39;\\) 到 \\(x\\) 的概率。这时候上图中的关于 \\(q\\) 的分数就等于一了。那就是说，如果 \\(f(x’)\\) 的值大于当前值，那就无脑接受就好啦（2.1步）。如果没有大于，那就算一下比例咯（2.2步）。所以课程中的这个算法就是MH算法。 MH算法很美妙，但太慢啦。那怎么办呢？我们可以用郎之万Langevin 动力学来帮助MH算法，让随机游走朝着概率更高的地方走。 这就是 Metropolis-adjusted Langevin algorithm。 最后总结一下，先用MH算法抽样，用这些抽样放到contrasive divergence 算法里训练能量模型的参数，来极大似然 Score Matching 上面我们用MH算法给出了一个训练和推理的方法，但缺点很明显，就是收敛的太慢了，随着维度的增加，收敛速度指数级别下降。虽然用了郎之万Langevin 动力学来进行提速，但每次梯度一更新之后，分布就变了。所以对于contrasive divergence 的每一步来说，MCMC都要从头开始采样直到收敛。（MCMC采样不是一开始就能用的，要丢弃前n个样本，叫做burn in） 拿能否训练时候不用sampling呢？ score function 先看一下什么叫score function 就是指向高概率方向的梯度。一个观察是，这个梯度和分母，就是partition function无关。至于为什么叫做score fuction，那是因为我们一般把 \\(f_\\theta(x)\\) 对输入x的梯度称为score。 score matching 在之前的MCMC采样方法训练中，当我们有了一个准确的能量模型后，我们从数据分布里采样就转换成了根据训练好的能量模型的score, 来进行MCMC采样。那么为什么不能换个思路，直接将能量模型建模成score，即用一个神经网络来拟合score! 这个方法就叫score-matching! 如上所示，我们的目标依旧是用score matching 来减小这两个分布的区别。难点在于，对于真实分布Pdata怎么求导呢？先看看一维的情况： \\[ \\begin{align*} \\frac{1}{2} \\mathbb{E}_{x \\sim p_{\\text{data}}} \\left[ (\\nabla_x \\log p_{\\text{data}}(x) - \\nabla_x \\log p_{\\theta}(x))^2 \\right] &amp;= \\frac{1}{2} \\int p_{\\text{data}}(x) \\left[ (\\nabla_x \\log p_{\\text{data}}(x) - \\nabla_x \\log p_{\\theta}(x))^2 \\right] dx &amp;\\\\ &amp;= \\frac{1}{2} \\int p_{\\text{data}}(x) (\\nabla_x \\log p_{\\text{data}}(x))^2 dx + \\frac{1}{2} \\int p_{\\text{data}}(x) (\\nabla_x \\log p_{\\theta}(x))^2 dx - \\int p_{\\text{data}}(x) \\nabla_x \\log p_{\\text{data}}(x) \\nabla_x \\log p_{\\theta}(x) dx &amp; \\end{align*} \\] 其中第一项是常数，我们不用管，第二项也只涉及到 \\(p_\\theta\\) (积分的 \\(p_{data}\\) 直接通过在训练集抽样即可)，第三项比较棘手，涉及到 \\(\\nabla_x \\log p_{\\text{data}}(x)\\) ，这个我们没法直接求出。 但是我们可以通过分布积分来进行化简： \\[ \\begin{align*} -\\int p_{\\text{data}}(x) \\nabla_x \\log p_{\\text{data}}(x) \\nabla_x \\log p_{\\theta}(x) dx &amp;= - \\int p_{\\text{data}}(x) \\frac{1}{p_{\\text{data}}(x)} \\nabla_x p_{\\text{data}}(x) \\nabla_x \\log p_{\\theta}(x) dx \\\\ &amp;= - p_{\\text{data}}(x) \\nabla_x \\log p_{\\theta}(x) \\Big|_{x = -\\infty}^{x = \\infty} + \\int p_{\\text{data}}(x) \\nabla_x^2 \\log p_{\\theta}(x) dx \\\\ &amp;= \\int p_{\\text{data}}(x) \\nabla_x^2 \\log p_{\\theta}(x) dx \\end{align*} \\] 其中我们认为 \\(p_{\\text{data}}(x) \\nabla_x \\log p_{\\theta}(x) \\Big|_{x = -\\infty}^{x = \\infty} = 0\\) ，因为我们假定无穷远处的 \\(p_{data}\\) 为0。 对于多维与一维类似，区别就是我们分部积分得到的结果是 \\(log(p_\\theta(x))\\) 的Hessian的迹，最终我们得到的形式如下： 我们通过分部积分把对 \\(P_{data}\\) 的梯度项给搞没了，就不用像之前那样费劲的去MCMC了。不过缺点是这个Hessian矩阵算起来很麻烦。 Noise contrastive estimation 把NCE用在Energy Based Model其实思想也很简单，我们在GANs中提到，对于一个真实样本和模型样本进行分类的最佳判别器是，对给定 \\(x\\) 的判定为真实样本的概率为\\(\\frac{P_{data}(x)}{P_{data}(x) + P_n(x)}\\)。 所以NCE的想法就是我去用生成器组成一个判别器，这个生成器输出概率为 \\(P_{\\theta^*}(x)\\) ，而判别器的输出则是 \\(\\frac{P_{\\theta^*}(x)}{P_{\\theta^*}(x) + P_n(x)}\\) ，这样当判别器训练成为最佳判别器时， \\(P_{\\theta^*}(x)\\) 就等于 $P_{data}(x) $ 。 注意，这里的 \\(P_n\\) 的概率是我们给定一个特定噪声分布进行采样的概率，所以很好获得。也就是说在NCE中，非真实样本的概率分布是事先指定的，而不是模型学习得到的。 但是，依旧会到那个问题， \\(P_{\\theta^*}(x) = \\frac{exp(f_{\\theta^*}(x))}{Z_{\\theta*}}\\) ，我的分母怎么处理呢？此时我们可以把 \\(Z\\) 也作为一个参数进行训练。假如我们能够得到最佳判别器，由于 \\(\\frac{exp(f_{\\theta^*}(x))}{Z^{*}} = P_{data}\\) ，所以 \\(Z\\) 也就肯定是最佳的分区函数了。 把这个形式带入我们二分类的目标函数（与GANs相同，这里不赘述了）： 当然，对于这个 \\(p_n\\) ，它对于训练效果有很显著的影响，毕竟区分图片和一堆噪声可不用很强的判别能力。所以后面也有对这个的改进工作，具体也就是类似GANs一样，再加一个生成器： 当时，这样就会训练得到两个生成模型了，具体推理阶段都可以使用。 注意，能量模型作为生成模型的一种，建模的是 \\(P(x)\\) ,主要功能是从 \\(P(x)\\) 里面采样。上面说的score matching是在训练的时候不用从中采样，加快训练的脚步，但是真正使用的时候还是得有MCMC。NCE因为显式的训练了partition function \\(Z\\) ，也许可以不用MCMC（但笔者感觉也没有比较好的直接采样方法，个人觉得还是需要靠MCMC，如果读者有好的想法也可以指正我）。","categories":[{"name":"Stanford CS236深度生成模型","slug":"Stanford-CS236深度生成模型","permalink":"https://jia040223.github.io/categories/Stanford-CS236%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}],"tags":[{"name":"生成模型","slug":"生成模型","permalink":"https://jia040223.github.io/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}]},{"title":"蒙特卡洛采样方法","slug":"蒙特卡洛采样方法","date":"2024-09-19T12:04:35.000Z","updated":"2024-09-20T13:29:20.636Z","comments":true,"path":"2024/09/19/蒙特卡洛采样方法/","permalink":"https://jia040223.github.io/2024/09/19/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E9%87%87%E6%A0%B7%E6%96%B9%E6%B3%95/","excerpt":"","text":"最近在学习Stanford CS236课程，里面多次提到了蒙特卡洛采样，但本人之前并没有系统地对蒙特卡洛采样进行过整理学习，所以也就正好趁此机会学习一下蒙特卡洛采样，分享记录，也便于自己实时查看。 蒙特卡洛估计 蒙特卡洛估计的原理 蒙特卡洛估计(Monte Carlo Estimator)的原理很简单，假设现在我们要求解一个一维的积分 \\(\\int_{a}^{b} g(x) dx\\) 。已知一个概率密度为 \\(f(x)\\) 的随机变量 $ X$ ，蒙特卡洛估计可以表示为： \\[G_N = \\frac{1}{N}\\sum_{i=1}^{N}{\\frac{g(X_i)}{f(X_i)}}\\\\\\] 概率密度 \\(f(x)\\) 需要满足 \\[ \\begin{cases} f(x) &gt; 0, x \\in (a, b),\\\\ f(x) = 0, x \\notin (a, b).\\end{cases}\\\\\\] 现在来验证下, 这种方式是正确的： \\[\\begin{align*} E[G_N] &amp; =E\\left [ \\frac{1}{N}\\sum_{i=1}^{N}{\\frac{g(X_i)}{f(X_i)}} \\right]\\\\ &amp; = \\frac{1}{N}\\sum_{i=1}^{N}\\int_{a}^{b}\\frac{g(x)}{f(x)}f(x)dx\\\\ &amp;= \\frac{1}{N}\\sum_{i=1}^{N}\\int_{a}^{b}g(x)dx\\\\ &amp;= \\int_{a}^{b}g(x)dx \\end{align*}\\\\\\] 也就是说， \\(G_N\\) 的期望与 \\(\\int_{a}^{b} g(x) dx\\) 是相同的，而 \\(G_N\\) 的方差如下： \\[\\begin{align*} D[G_N] &amp; =D\\left [ \\frac{1}{N}\\sum_{i=1}^{N}{\\frac{g(X_i)}{f(X_i)}} \\right]\\\\ &amp; = \\frac{1}{N^2}D\\left [ \\sum_{i=1}^{N}{\\frac{g(X_i)}{f(X_i)}} \\right]\\\\ &amp;= \\frac{1}{N^2}\\cdot N \\cdot D\\left [ {\\frac{g(X_i)}{f(X_i)}} \\right]\\\\ &amp;= \\frac{1}{N} D\\left [ {\\frac{g(X_i)}{f(X_i)}} \\right] \\end{align*}\\\\\\] 从上面的式子，可以看出，要减少方差，有两种途径： 增加采样次数 \\(N\\) 减少 \\(D(\\frac{g(X)}{f(X)})\\) 理论上，只要我们采样次数足够多，方差趋近于0，\\(G_N\\) 也就依概率收敛于\\(\\int_{a}^{b} g(x) dx\\) 蒙特卡洛估计的优点 我们在考虑一个积分算法/Estimator 时，通常从两个角度考虑。 一个是计算的准确性，即随着采样次数增大时，结果是否趋近于我们期望的真实值。如果一个 estimator 的期望值和真实值相等，我们说它是无偏的/unbiased。如果一个 estimator 的期望值和真实值不相等，则它是有偏的。大部分 estimator 都是无偏的，在少数情况下，我们会使用一个有偏的但是计算收敛速度很快的 estimator。 另外一个角度是计算结果的方差。随着采样次数增大时，计算结果的方差应该总是减少的。两个estimator 的方差可以比较可以从两个角度来体现。即采样次数相同时的方差大小，以及随着采样次数增大，方差收敛的速度。我们总是期望使用一个方差较小且收敛较快的 estimator，来减少计算的事件。 计算结果表明，蒙特卡洛估计误差收敛的速度为 \\(O(\\sqrt N)\\) (意味着4倍的采样会使误差减少一半)，蒙特卡洛估计不受维度影响，在高维情况下比其他估计方法收敛要快得多。 蒙特卡洛估计的实践使用 在实际使用中，直接使用蒙特卡洛方法要求我们能够从 $p(x) $ 中采样——对于简单分布（如均匀分布）这是容易做到的；对于稍微复杂一些但可写出 PDF 或 CDF 分布，可以利用变量替换定理来直接采样；而对于更复杂的分布，我们则更多选择拒绝采样和重要性采样来实现这一点。。 变量替换定理 $ X$ 服从一个我们能直接进行采样的连续值（例如均匀分布），我们希望找到一个函数 \\(f(x)\\) ，让 \\(Y=f(X)\\) 满足我们需要得到的分布 \\(Y \\sim P_y\\) ，则使用累积分布函数： \\[P_y(y)\\triangleq P(Y\\le y)=P(f(X)\\le y)=P(X\\in(f(x)\\le y))\\] 概率密度函数可以通过累积分布函数求导得到。当单调，因此可逆时，可得： \\[P_y(y)=P(f(X)\\le y)=P(X\\le f^{-1}(y))=P_x(f^{-1}(y))\\] 求导可得： \\[p_y(y)\\triangleq\\frac{d}{dy}P_y(y)=\\frac{d}{dy}P_x(f^{-1}(y))=\\frac{dx}{dy}\\frac{d}{dx}P_x(x)=\\frac{dx}{dy}p_x(x)\\] 其中 \\(x=f^{-1}(y)\\) 。由于符号并不重要，因此可得一般表达式： \\[p_y(y)=p_x(x)|\\frac{dx}{dy}|\\] 可将上述结果拓展为多变量分布。令 \\(f\\) 为 \\(R^n\\) 到 \\(R^n\\) 的映射， \\(\\mathrm y=f(\\mathrm x)\\) 。则雅可比矩阵 \\(J\\) 为： \\[J_{\\mathrm x\\rightarrow\\mathrm y}\\triangleq\\frac{\\partial(y_1,\\ldots,y_n)}{\\partial(x_1,\\ldots,x_n)}\\triangleq \\begin{pmatrix} \\frac{\\partial y_1}{\\partial x_1} &amp;\\dots&amp;\\frac{\\partial y_1}{\\partial x_n}\\\\ \\vdots&amp;\\ddots&amp;\\vdots\\\\ \\frac{\\partial y_n}{\\partial x_1}&amp;\\dots&amp;\\frac{\\partial y_n}{\\partial x_n} \\end{pmatrix}\\] \\(|det J|\\) 度量了单位立方体在应用 \\(f\\) 时的体积变化量。如果 \\(f\\) 是一个可逆映射，可以使用反映射 \\(\\mathrm y\\rightarrow\\mathrm x\\) 的雅可比矩阵定义变换变量的概率密度函数： \\[p_y(\\mathrm y)=p_x(\\mathrm x)|\\det\\left(\\frac{\\partial \\mathrm x}{\\partial\\mathrm y}\\right)|=p_x(\\mathrm x)| \\det J_{\\mathrm y\\rightarrow\\mathrm x}|\\] 这就是随机变量的变量替换定理，通过这个我们可以对一些相对简单的分布进行直接采样了。 例如设 \\(x\\) 服从累积分布函数为 \\(F(x)=1-e^{-x}\\) (可验证是单调不减，且积分为1的函数)的分布，则可以通过逆变换的方法对 \\(F(x)\\) 直接采样，产生服从F(X)分布的样本X。 令 \\(y=1-e^{-x}\\)，则$ e^{-x}=1-y $ .两边求对数可得: \\(x=-ln(1-y)\\) ,则 \\(F^{-1}(x)=-ln(1-x)\\) ，令 \\(x_i\\) 为均匀分布样本，则 \\(X_i=-ln(1-x_i)\\) 为服从累积分布函数为 $ F(x)$ 分布的样本. 拒绝接受采样 拒绝接受采样的目的仍然是得到服从某个概率分布的样本，不过这种方法是直接利用概率密度函数(PDF)得到样本。如下图所示， \\(p(x)\\) 是我们希望采样的分布， \\(q(x)\\) 是我们提议的分布(proposal distribution)， \\(q(x)\\) 分布比较简单，令 \\(kq(x)&gt;p(x)\\) ，我们首先在 \\(kq(x)\\) 中按照直接采样的方法采样粒子，接下来以 \\(\\frac{p(x_i)}{kq(x_i)}\\) 的概率接受这个点，最终得到符合 \\(p(x)\\) 的N个粒子。 可以证明，这样做得到的样本是服从\\(p(x)\\)的，我们可以计算 \\(x_0\\) 对应的样本被取到的概率为： \\[\\frac{q(x_0)\\dfrac{\\tilde p(x_0)}{kq(x_0)}}{\\displaystyle\\int_x q(x)\\frac{\\tilde p(x)}{kq(x)}\\mathrm dx}=\\frac{\\tilde p(x_0)}{\\displaystyle\\int_x \\tilde p(x)\\mathrm dx}=p(x_0)\\] 所以拒绝接受采样的基本步骤： 生成服从 \\(q(x)\\) 的样本 \\(x_i\\) . 生成服从均匀分布 \\(U(0,1)\\) 的样本 \\(u_i\\) . 当 \\(k\\cdot q(x_i)\\cdot u_i&lt;p(x_i)\\) ,也就是二维点落在蓝线以下，此时接受 \\(X_k=x_i\\) 这里乘以 \\(u_i\\) ，是因为我们需要以 \\(\\frac{p(x_i)}{kq(x_i)}\\) 的概率接受这个点，因为如果 \\(k\\cdot q(x_i)\\cdot u_i&lt;p(x_i)\\) ，则 \\(u_i&lt;\\frac{p(x_i)}{k\\cdot q(x_i)}\\) ，而 \\(u_i\\) 服从均匀分布 \\(U(0,1)\\) 最终得到的 \\(X_k\\) 为服从 \\(p(x)\\) 的样本. 我们可以计算一下样本采样的接受率： \\[p(\\text{accept})=\\int_x \\frac{\\tilde p(x)}{kq(x)}q(x)\\mathrm dx=\\frac{1}{k}\\int_x\\tilde p(x)\\mathrm dx\\] 因此 \\(k\\) 越小，总接受率越大，算法效率越高。然而， \\(k\\) 小也意味着 \\(q(x)\\) 本身就要与 \\(p(x)\\) 比较相似，对于复杂的 \\(p(x)\\) 而言寻找到一个合适的 \\(q(x)\\) 非常困难的。 重要性采样 重要性采样的目的：求一个函数 \\(f(x)\\) 在概率密度函数为 $ p(x)$ 分布下的期望，即 \\[\\mathbb{E}[f(x)]=\\int f(x)p(x)dx\\] 当 \\(p(x)\\) 很复杂时，不解析，积分不好求时，可以通过重要性采样来计算。当 \\(f(x)=x\\) ，则可以算 \\(p(x)\\) 的期望。 原理 首先, 当我们想要求一个函数 \\(f(x)\\) 在区间 \\([a, b]\\) 上的积分 \\(\\int_{a}^{b} f(x) d x\\) 时有可能会面临一个问题, 那就是积分曲线难以解析, 无法直接求积分。这时候我们可以采用一种估计的方式, 即在区间 \\([a, b]\\) 上进行采样: \\(\\left\\{x_{1}, x_{2} \\ldots, x_{n}\\right\\}\\) , 值为 \\(\\left\\{f\\left(x_{1}\\right), f\\left(x_{2}\\right), \\ldots, f\\left(x_{n}\\right)\\right\\}\\) 如果采样是均匀的, 即如下图所示: 那么显然可以得到这样的估计: \\(\\int_{a}^{b} f(x) d x=\\frac{b-a}{N} \\sum_{i=1}^{N} f\\left(x_{i}\\right)\\) , 在这里 \\(\\frac{b-a}{N}\\) 可以看作是上面小长方形的底部的 “宽”, 而 \\(f\\left(x_{i}\\right)\\) 则是坚直的 “长”。 上述的估计方法随着取样数的增长而越发精确，那么有什么方法能够在一定的抽样数量基础上来增加准确度，减少方差呢？比如 \\(x\\) 样本数量取10000，那么显然在 \\(f(x)\\) 比较大的地方，有更多的 \\(x_i\\) ，近似的积分更精确。 并且原函数 \\(f(x)\\) 也许本身就是定义在一个分布之上的, 我们定义这个分布为 \\(p(x)\\) , 我们无法直接从 $ p(x)$ 上进行采样, 所以另辟蹊径重新找到一个更加简明的分布 \\(q(x)\\) , 从它进行取样, 希望间接地求出 \\(f(x)\\) 在分布 \\(p(x)\\) 下的期望。 若p(x)归一化 搞清楚了这一点我们可以继续分析了。首先我们知道函数 \\(f(x)\\) 在概率分布 \\(p(x)\\) 下的期望为: \\[\\mathbb{E}[f(x)]=\\int_{x} p(x) f(x) d x \\] 但是这个期望的值我们无法直接得到, 因此我们需要借助 \\(q(x)\\) 来进行采样, \\(q(x)\\) 可以选取简单的分布，比如设q(x)为均匀分布，当我们在 \\(q(x)\\) 上采样得到 \\(\\left\\{x_{1}, x_{2}, \\ldots, x_{n}\\right\\}\\) （即 \\(x_i\\) 服从 \\(q(x)\\) 分布）后，那么我们可以估计 \\(f\\) 在 \\({q(x)}\\) 下的期望为： \\[\\mathbb{E}[f(x)]=\\int_{x} q(x) f(x) d x \\approx \\frac{1}{N} \\sum_{i=1}^{N} f\\left(x_{i}\\right) \\] 上面这个式子就简单很多了，只要我们得到 \\(x_i\\) 然后代入 \\(f(x)\\) 然后求和就行了，而且均匀分布的样本 \\(x_i\\) 很容易获得。接着我们来考虑原问题，对式(1)进行改写, 即： \\(p(x) f(x)=q(x) \\frac{p(x)}{q(x)} f(x)\\) , 所以我们可以得到: \\[\\mathbb{E}[f(x)]=\\int_{x} q(x) \\frac{p(x)}{q(x)} f(x) d x\\] 这个式子我们可以看作是函数 \\(\\frac{p(x)}{q(x)} f(x)\\) 定义在分布 \\(q(x)\\) 上的期望, 当我们在 \\(q(x)\\) 上采样 \\(\\left\\{x_{1}, x_{2}, \\ldots, x_{n}\\right\\}\\) (服从q(x)分布)，可以估计 \\(f\\) 的期望: \\[\\begin{aligned}\\mathbb{E}[f(x)]&amp;=\\frac{1}{N} \\sum_{i=1}^{N} \\frac{p\\left(x_{i}\\right)}{q\\left(x_{i}\\right)} f\\left(x_{i}\\right)\\\\&amp;=\\frac{1}{N} \\sum_{i=1}^{N} w_i f\\left(x_{i}\\right)\\end{aligned}\\] 在这里 \\(w_i=\\frac{p\\left(x_{i}\\right)}{q\\left(x_{i}\\right)}\\) 就是重要性权重。 若p(x)没有归一化 上面的讨论是假设 \\(p(x)\\) 已经完成归一化了，也就是 \\(\\int p(x)=1\\) ,假如 \\(p(x)\\) 没有归一化，那么我们可以在上面的推导中对 \\(p(x)\\) 进行归一化： \\[\\begin{aligned}\\mathbb{E}[f(x)]&amp;=\\int f(x) \\frac{p(x)}{\\int p(x) d x} d x\\\\&amp;=\\frac{\\int f(x) p(x) d x}{\\int p(x) d x}\\\\&amp;=\\frac{\\int f(x) \\frac{p(x)}{q(x)} q(x) d x}{\\int \\frac{p(x)}{q(x)} q(x) d x}.\\end{aligned}\\] 而分子分母可分别得到，下面两式约等于都利用 \\(q(x)\\) 是均匀分布的假设： \\[\\begin{aligned}\\int f(x) \\frac{p(x)}{q(x)} q(x) d x &amp;\\approx \\frac{1}{n} \\sum_{i=1}^{n} W_{i} f\\left(x_{i}\\right), \\\\\\int \\frac{p(x)}{q(x)} q(x) d x &amp;\\approx \\frac{1}{n} \\sum_{i=1}^{n} W_{i}.\\end{aligned}\\] 其中 \\(W_i=\\frac{p(x_i)}{q(x_i)}\\) ，则最终可得 \\(\\mathbb{E}[f(x)]\\) : \\[\\begin{aligned}\\mathbb{E}[f(x)] \\approx \\sum_{i=1}^{n} w_{i} f\\left(x_{i}\\right), w_{i}=\\frac{W_{i}}{\\sum_{i=1}^{n} W_{i}}\\end{aligned}\\] 多重重要性采样 有的时候, 需要积分的方程中可能包含多个需要积分的部分, 这时候就需要用到多重重要性采样(multiple importance sampling/MIS). 比如现在要求解 \\(\\int_{}^{} g_1(x)g_2(x)\\) 这样的积分时, 两个部分分别对应两个概率密度 \\(f_1(x), f_2(x)\\) , MIS给出的新的蒙特卡洛估计为: \\[\\frac{1}{n_1} \\sum_{i=1}^{n_1}{\\frac{g_1(X_1)g_2(X_1)\\omega_1(X_1)}{f(X_1)}} + \\frac{1}{n_2} \\sum_{i=1}^{n_2}{\\frac{g_1(X_2)g_2(X_2)\\omega_2(X_2)}{f(X_2)}}\\\\\\] \\(n_1,n_2\\) 分别是两边的采样次数, $_1, _2 $ 分别是两个部分对应的权重. 一个常用的权重函数为: \\[\\omega_k = \\frac{(n_kf_k(x))^2} {\\sum_{i}^{}{(n_1f_i(x))^2}}\\\\\\] 在上面有两个部分的情况下得: \\[\\omega_1 = \\frac{(n_1f_1(x))^2} {(n_1f_1(x))^2 +(n_2f_2(x))^2 }\\\\ \\omega_2 = \\frac{(n_2f_2(x))^2} {(n_1f_1(x))^2 +(n_2f_2(x))^2 }\\\\\\] 与拒绝采样一样，重要性采样的效果与提议分布 $q(x) $ 同 \\(p(x)\\) 的接近程度紧密相关。当 \\(p(x)\\) 比较复杂时，选择合适的 \\(q(x)\\) 是非常困难的。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://jia040223.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jia040223.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"概率论与数理统计","slug":"概率论与数理统计","permalink":"https://jia040223.github.io/tags/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/"}]},{"title":"GANs","slug":"GANs","date":"2024-09-18T14:10:33.000Z","updated":"2024-09-20T13:27:19.686Z","comments":true,"path":"2024/09/18/GANs/","permalink":"https://jia040223.github.io/2024/09/18/GANs/","excerpt":"","text":"本学习笔记用于记录我学习Stanford CS236课程的学习笔记，分享记录，也便于自己实时查看。 引入 前面我们学习了VAEs和Normalizing Flows，这两种模型都是基于最小化KL散度（对似然进行评估）来进行优化的。我们也可以看到，为了进行生成，我们往往会定义一个潜变量 \\(z\\) ，所以对似然进行评估并不容易。VAEs是通过优化似然的下限ELBO来绕过这个问题，而Normalizing Flows是通过限制映射的形式来计算似然。 直接计算似然来进行评估，要么只能计算其下界，要么需要限制映射的形式。那有没有一种方法能够用间接方法代替这种直接比较，使生成分布变得越来越接近真实分布呢？GANs便是基于一种间接的评估方式进行设计的。 基本思想 GANs的间接方法采用这两个分布的下游任务形式。然后，生成网络的训练是相对于该任务进行的，使生成分布变得越来越接近真实分布。GANs 的下游任务是区分真实样本和生成样本的任务。或者我们可以说是“非区分”任务，因为我们希望区分尽可能失败。 因此，在 GANs 架构中，我们有一个判别器，它接收真实和生成数据样本，并尽可能地对它们进行分类；还有一个生成器，它被训练成尽可能地欺骗判别器。即GANs由2个重要的部分构成： 生成器(Generator)：通过机器生成数据，目的是“骗过”判别器。 判别器(Discriminator)：判断数据是真实的还是生成的，目的是找出生成器做的“假数据”。 训练过程 我们知道GANs的思想后，便能很直观的想到用分类问题的交叉熵作为判别器的损失函数。同时生成器的目的则是最大化这个交叉熵损失函数（混淆判别器），所以我们的训练目标是： \\[\\mathop{\\text{min}}\\limits_{G}\\mathop{\\text{max}}\\limits_{D} \\ V(G,D) = \\mathbb{E}_{x \\sim p_{data}(x)} [\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)} [\\log(1 - D(G(z)))] \\] 其中 \\(G\\) 指的是生成器， \\(D\\) 指的是判别器。 所以我们的训练目标是一个极大极小的优化问题，在实际中，我们只需要从数据集中进行采样，然后用生成器进行采样，然后对上面的目标函数进行近似计算，最后进行梯度上升或者梯度下降即可 与散度的关系 那么为什么这样的设计能够间接地去让生成器生成的样本与真实样本的分布相同呢？ 其实本质上，GANs通过引入判别器来间接地计算了 \\(\\frac{P_\\theta(x)}{P_{data}(x)}\\) ，可以证明，对于一个生成器下的最佳判别器对给定 \\(x\\) 的判定为真实样本的概率是\\(\\frac{P_{data}(x)}{P_{data}(x) + P_\\theta(x)}\\)， 证明如下： *Proof*: 二分类交叉熵损失函数为： \\[\\begin{align} \\mathrm{BCE}(\\mathcal P_1,\\mathcal P_2)&amp;=-\\mathbb E_{x\\sim \\mathcal P_1}[\\log D(x)]-\\mathbb E_{x\\sim \\mathcal P_2}[\\log(1-D(x))]\\\\ &amp;=-\\int \\log D(x)\\cdot p_1(x)\\mathrm d x-\\int\\log(1-D(x))\\cdot p_2(x)\\mathrm d x\\\\ &amp;=-\\int \\left[\\log D(x)\\cdot p_1(x)+\\log(1-D(x))\\cdot p_2(x)\\right]\\mathrm d x\\\\ \\end{align} \\\\\\] 易知 \\(y=a\\log x+b\\log(1-x)\\) 在 \\(x=\\frac{a}{a+b}\\) 处取到唯一极大值（其中 \\(0\\leq a,b\\leq1\\) ），所以欲使上式最小，只需： \\(\\forall x,\\,D(x)=\\frac{p_1(x)}{p_1(x)+p_2(x)} \\\\\\) 这样就证明完成了。 那么，再看我们的训练目标： \\[\\min_G\\max_D V(G, D) \\\\ \\begin{align} V(G, D)&amp;=\\mathbb E_{x\\sim\\mathcal P_{data}}[\\log D(x)]+\\mathbb E_{z\\sim \\mathcal P_z}[\\log(1-D(G(z)))]\\\\ &amp;=\\mathbb E_{x\\sim\\mathcal P_{data}}[\\log D(x)]+\\mathbb E_{x\\sim \\mathcal P_{\\theta}}[\\log(1-D(x))] \\end{align} \\\\\\] 而最优判别器为： \\(D^\\ast(x)=\\frac{p_{data}(x)}{p_{data}(x)+p_\\theta(x)} \\\\\\) 将最优判别器代入 \\(G\\) 的优化目标： \\[\\begin{align} V(G, D^\\ast)&amp;=\\mathbb E_{x\\sim \\mathcal P_{data}}\\left[\\log\\frac{p_{data}(x)}{p_{data}(x)+p_\\theta(x)}\\right]+\\mathbb E_{x\\sim \\mathcal P_\\theta}\\left[\\log\\frac{p_\\theta(x)}{p_{data}(x)+p_\\theta(x)}\\right]\\\\ &amp;=2\\mathrm {JS}(\\mathcal P_{data}\\|\\mathcal P_\\theta)-2\\log2 \\end{align} \\\\ \\] 因此，生成器实际上在最小化 $ P_{data}$ 和 \\(\\mathcal P_\\theta\\) 的 \\(\\mathrm{JS}\\) 散度，从而让生成数据的分布 \\(\\mathcal P_\\theta\\) 接近真实分布 \\(\\mathcal P_{data}\\) 。 注： \\(JS\\) 散度的定义如下： *\\[\\begin{align} \\mathrm{JS}(\\mathcal P_1\\|\\mathcal P_2)&amp;=\\frac{1}{2}\\left[\\mathrm{KL}\\left(\\mathcal P_1\\|\\mathcal P_A\\right)+\\mathrm{KL}\\left(\\mathcal P_2\\|\\mathcal P_A\\right)\\right]\\\\ &amp;=\\log 2+\\frac{1}{2}\\mathbb E_{x\\sim\\mathcal P_1}\\left[\\log\\frac{p_1(x)}{p_1(x)+p_2(x)}\\right]+\\frac{1}{2}\\mathbb E_{x\\sim\\mathcal P_2}\\left[\\log\\frac{p_2(x)}{p_1(x)+p_2(x)}\\right] \\end{align} \\\\\\] 其相比 \\(KL\\) 散度最大的特点便是其是对称的*。 可以看出GANs是通过判别器来巧妙地规避了计算似然的问题，但正是因为在实践中我们很难得到真正的最佳判别器，所以实际上我们很多时候只是在优化 \\(JS\\) 散度的一个下界，笔者认为这是GANs不得不直面的一个问题。 fGAN F-散度(F-divergence) 在概率统计中，f散度是一个函数，这个函数用来衡量两个概率密度\\(p\\)和\\(q\\)的区别，也就是衡量这两个分布多么的相同或者不同。像 \\(KL\\) 散度和 \\(JS\\) 散度都是它的一种特例 f散度定义如下： \\[{D_f}(\\mathcal P_1\\|\\mathcal P_2)=\\int f (\\frac{p_2(x)}{p_1(x)})\\cdot p_1(x)\\mathrm d x=\\mathbb E_{x\\sim\\mathcal P_1}\\left[f(\\frac{p_2(x)}{p_1(x)})\\right] \\\\\\] \\(f(·)\\) 就是不同的散度函数， \\(D_f\\) 就是在f散度函数下，两个分布的差异。规定 \\(f\\) 是凸函数(为了用琴生不等式) $f ( 1 ) = 0 $ (如果两个分布一样，刚好公式=0) 这两个规定保证了 \\(D_f\\) 是非负的，而且当两个分布相同时，其值为0，一些常见散度的 \\(f\\) 定义如下： 共轭函数(Fenchel Conjugate) 一个函数 \\(f:\\;\\mathbb{R}^n\\mapsto\\mathbb{R}\\) 的 Frenchel 共轭为： \\[\\begin{align} f^*( t)=\\sup_{ x}\\big(\\langle t, x\\rangle-f( x)) \\end{align}\\] Fenchel 共轭有几何上的解释。当 $ x$ 固定时， \\(\\langle t, x\\rangle-f( x)\\) 是一个仿射函数，因此 Fenchel 共轭就是一组仿射函数的上确界。如果 \\(f\\) 可微，那么仿射函数取得上确界的位置正好是 \\(f\\) 的切线，此处有 \\(\\nabla f( x)= t\\) 。 我们拿 $f ( x ) = x l o g x $ 来说，当 \\(x=10,1, 0.1\\) 时可以看到相应的函数直线，可以看到最大化y的点连起来是个凸函数，很类似$ e^{t-1}$ 公式图像： 用数学来推一下： 将 \\(f ( x ) = x l o g x\\) 代入 $y ( t ) = x t − f ( x ) $ ，得 $y ( x ) = x t − x l o g x $ ,对于每个给定的 \\(t\\) 都可以求出最大值，求导为0即可。 求导后得： \\(t − l o g x − 1 = 0\\) ,即 \\(x=e^{t-1}\\) ，代入$ f(t) $, 得 $ f^(t)=te{t-1}-e{t-1}(t-1)=e{t-1}$ 读者可以对这个 $ f^*(t)$ 再求一次共轭，可以发现其又变回原函数了。 事实上，可以证明，对于凸函数来说$ f^{**}(x) = f(x)$ 应用于GAN 那这个跟GAN有啥关系呢？ 假如我们用一个 \\(D_f\\) 来评估生成模型，对于 \\(p(x)\\) 和 \\(q(x)\\) 之间的 f-divergence： \\[ \\begin{aligned} D_f(P||Q) &amp;= \\int_{x} q(x) f\\left(\\frac{p(x)}{q(x)}\\right) dx \\\\ &amp;= \\int_{x} q(x) \\left( \\max_{t \\in \\operatorname{dom}(f^*)} \\left\\{\\frac{p(x)}{q(x)}t - f^*(t)\\right\\} \\right) dx \\end{aligned} \\] 记一个函数 D(x)，它输入是 \\(x\\) ，输出是 \\(t\\) ，用该函数代替上式中的 \\(t\\) ，得到 \\[ \\begin{aligned} D_f(P||Q)&amp;\\geq\\int \\limits_{x}q(x)(\\frac{p(x)}{q(x)}D(x)-f^{*}(D(x)))dx\\\\ &amp;= \\int \\limits_{x}p(x)D(x)dx-\\int \\limits_{x}q(x)f^{*}(D(x))dx \\end{aligned} \\] D(x) 其实就是判别器，可以看出，它依然是在解一个求最大值问题，通过这种方法，去逼近 f-divergence。 \\[D_f(P||Q)\\approx\\max \\limits_{D}\\int \\limits_{x}p(x)D(x)dx-\\int \\limits_{x}q(x)f^{*}(D(x))dx\\] p(x) 和 q(x) 本质上是一个概率，于是有 \\[D_f(P||Q)\\approx\\max \\limits_{D}\\{E_{x\\sim P}[D(x)]-E_{x\\sim Q}[f^*(D(x))]\\}\\] 用 \\(P_{data}\\) 和 \\(P_\\theta\\) 来指代 P 和 Q，有 \\[D_f(P_{data}||P_\\theta)\\approx\\max \\limits_{D}\\{E_{x\\sim P_{data}}[D(x)]-E_{x\\sim P_\\theta}[f^*(D(x))]\\}\\] 有没有发现这一套下来很熟悉？其实这还是我们之前训练生成器判别器的那一套流程。也就是 \\[ \\begin{aligned} G^*&amp;=\\mathop{argmin} \\limits_{G}D_f(P_{data}||P_\\theta)\\\\&amp;=\\mathop{argmin} \\limits_{G}\\max \\limits_{D}\\{E_{x\\sim P_{data}}[D(x)]-E_{x\\sim P_\\theta}[f^*(D(x))]\\}\\\\&amp;=\\mathop{argmin} \\limits_{G}\\max \\limits_{D}V(G, D) \\end{aligned} \\] 只不过这次的损失函数更加 general 了。换不同的 \\(f(x)\\) ，就可以量不同的散度（divergence）。 WGAN JS散度 to Wasserstein（Earth-Mover EM）距离 JS散度的问题 考虑两个分布\"完全不相交\"的时候，会发现 \\(JS\\) 散度为常量，梯度为 \\(0\\) 无法优化。 下面一个例子来说明: 假设两个二维空间上的概率分布，记为 \\({P}_d(X_1, Z)\\) 和 \\({P}_g(X_2, Z)\\) 。我们刻画 \\(Z \\sim U(0, 1)\\) 一个 \\([0, 1]\\) 上的均匀分布，而分别令 $ X_1 = 0$ 和 \\(X_2 = \\theta\\) ，因而，它们在二维空间上的概率分布空间就是两条平行线（垂直于 \\(x\\) 的轴，而平行于 \\(z\\) 的轴）。 当 \\(\\theta = 0.5\\) 时，我们考量等价于JS散度的损失函数 \\(V(G, D^*)\\) ，由于两个分布概率大于0的空间范围是完全没有重叠的，因此，对于任意 \\(p_d(x,y) \\ne 0\\) 必然有 \\(p_g(x, y) =0\\) 成立，反之亦然。 因而我们就有，对于任意 \\(x \\in \\mathbb{R}^2\\) ， \\[V(G, D^*)= \\int_x p_d(x) log \\frac{p_{d}(x)}{p_{d}(x) + p_{g}(x)} + p_g(x)log \\frac{p_{g}(x)}{p_{d}(x) + p_{g}(x)} dx \\\\ = \\int_x p_d(x) log (1) + p_g(x)log (1) dx = 0 \\\\ \\] 此时，损失函数恒为常量，无法继续指导生成器 \\(G(x)\\) 的优化。即此时出现了梯度消失的问题。 Wasserstein距离 为了弥补JS散度的局限性，我们需要一种全新的”分布间距离“的度量来进行优化，即使用Wasserstein距离，也被称为“推土机距离”（Earth-Mover），它定义如下： \\(W({P}_d, {P}_g) = inf_{\\gamma \\in \\Pi({P}_d, {P}_g)} {E}[||x - y||] \\\\\\) 这样数学形式的刻画可能会让人看得颇为一头雾水，我们逐步来分析解释它。 其中， \\(\\Pi({P}_d, {P}_g)\\) 代表一个 \\({P}_d, {P}_g\\) 构成的联合分布的集合，且这个集合中的所有联合分布必须满足其边际分布分别为 \\({P}_d, {P}_g\\) 。 \\(||x-y||\\) 是两个分布所在空间 \\(\\mathbb{R}^n\\) 中两点的欧式距离。 我们可以将 \\(\\Pi({P}_d, {P}_g)\\) 中的元素理解为一种“概率的搬运方案”。 而 \\(\\gamma\\) 是上述集合中的一个联合分布，可以使得任意两点的欧式距离期望最小，即将一个分布搬运为另外一个分布的最小开销。 此时，我们再重新观察上面的场景，当概率分布式为两条平行线上的均匀分布时，显然，最佳方案就是直接与x轴平行地进行概率搬运，对应为： \\(W(P_0, P_\\theta) = |\\theta|\\) 。此时，即使两个分布完全没有重叠部分，我们仍然能通过优化Wasserstein距离来实现两个概率分布之间的距离优化。 可以给出证明的是，就像JS散度一样，Wasserstein距离收敛于0时，两个分布也完全一致。 固然，通过Wasserstein距离优化GAN的想法颇为\"美好\"，不过，找到\"最优搬运方案\"的优化问题却是难事，在实现层面上，我们难以直接计算Wasserstein距离。不过，基于对偶理论可以将Wasserstein距离变换为积分概率度量IPM框架下的形式，来方便我们进行优化。 IPM也是用于衡量两个分布之间的距离，它的想法是寻找某种限制下的函数空间 \\(\\mathbb{F}\\) 中的一个函数 \\(f(·)\\) ，使得对任意位置两个分布的差异最大： \\[d_F(p, q) = sup_{f \\in F} \\mathbb{E}_{x \\sim P}[f(x)] - \\mathbb{E}_{x \\sim Q}[f(x)] \\\\\\] 对于Wasserstein距离而言，则变为： \\[W(p, q) = sup_{||f||_L \\le 1} \\mathbb{E}_{x \\sim P}[f(x)] - \\mathbb{E}_{x \\sim Q}[f(x)] \\\\\\] 因而，在函数 $ f(·)$ 满足Lipschitz约束的函数空间中，即 \\(||f(x) - f(y)|| \\le K||x - y||\\) ，找到最佳的函数 \\(f(·)\\) ，该情况下上式的结果则为Wasserstein距离。 这个函数 \\(f(·)\\) 难以求解，但我们可以用神经网络来拟合它。需要注意的是，从此开始，GAN的 \\(D\\) 就不再是先前我们认为的“真假判别器”了，它的意义变成了一个距离的度量。此时，GAN的生成器并不改变仍然生产图片，对生成器的训练则是减小与真实分布的Wasserstein距离，判别器 \\(D\\) 负责给出真实图像和生产图像样本之间的Wasserstein距离，相应的，在固定生成器优化判别器时，化则变为了寻找函数空间 \\(\\mathbb{F}\\) 中最佳的 \\(f(·)\\) 。 下面的图就可以体现传统GAN的判别器梯度和WGAN的判别器梯度的区别 WGAN便有效解决了某些情况下传统GAN的梯度消失的问题","categories":[{"name":"Stanford CS236深度生成模型","slug":"Stanford-CS236深度生成模型","permalink":"https://jia040223.github.io/categories/Stanford-CS236%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}],"tags":[{"name":"生成模型","slug":"生成模型","permalink":"https://jia040223.github.io/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}]},{"title":"Normalizing Flows","slug":"Normalizing Flows","date":"2024-09-17T10:18:14.000Z","updated":"2024-10-08T00:11:51.158Z","comments":true,"path":"2024/09/17/Normalizing Flows/","permalink":"https://jia040223.github.io/2024/09/17/Normalizing%20Flows/","excerpt":"","text":"本学习笔记用于记录我学习Stanford CS236课程的学习笔记，分享记录，也便于自己实时查看。 引入 生成模型模型的目的是让得到的数据分布 \\(P_{\\theta}\\) 与真实的数据分布 \\(P_{data}\\) 相同，也就是需要通过给定的样本来建模对应的分布，使得输入经过该模型后可以生成与给定样本类似的新样本。在这种意义下，评估的最佳方式便是使用极大似然估计，然而VAEs的做法导致计算似然十分复杂，所以我们只能选择计算似然的下界，也就是ELBO。 不妨思考一下，VAEs无法计算似然的原因是什么。不难发现，关键在于需要对所有的潜变量 \\(z\\) 进行积分。所以假如我们有一个可逆映射，使得潜变量 \\(z\\) 和数据 \\(x\\) 之间的是一一对应的，那我们便可以很轻松计算似然了。 Normalizing Flows正是这么做的。但可逆映射意味着潜变量 \\(z\\) 的维度需要和数据 \\(x\\) 的维度一致，所以我们无法利用 \\(z\\) 进行压缩。 简介 正则化流（Normalizing Flow）是一种可逆生成模型，用于将一个原始分布通过学习的变换映射到另一个已知的概率分布。它可以将数据从原始分布转换为目标分布，从而实现数据的生成和采样。 在正则化流中，我们定义一个变换函数，它将输入样本从原始分布映射到目标分布。这个映射是一个可逆函数，确保转换是可逆的，也就是说，在给定目标分布样本的情况下，可以逆向计算出原始分布的样本。这个变换函数通常由一系列的可逆操作组成，每个操作都是可逆的，并且通过组合这些操作可以得到整个变换。常用的可逆操作包括仿射变换、尺度变换、平移变换等。 原理 变量替换 变量替换的形式如下： $ p_{X}(X)=p_{Z}(f(X))|det~J(f(X))|$ \\(Z=f(X)\\) 是一个可逆的变换 \\(J(f(X))\\) 是 \\(f(X)\\) 的雅可比行列式 如何理解呢：即给出一个 \\(X\\) ，使用一个可逆变换 \\(f(\\cdot)\\) 将 \\(X\\) 变为 \\(Z\\) ，那么 \\(p(X)、p(Z)\\) 这两个分布之间相差的就是这样一个雅可比行列式。 1.png 流的组合 基本原理：可导的可逆的函数在进行组合后依然是一个可导且可逆的函数 标准化方向： \\(f=f_{1}\\circ f_{2}\\circ....f_{N}\\) 采样构造概率的方向： \\(g=g_{N} \\circ g_{N-1} \\circ .... \\circ g_{1}\\) 2.png 这种流动的感觉就是标准化流这个名字的由来。 而由 \\(p_{X}(X)=p_{Z}(f(X))|det~J(f(X))|\\) 可知，上面组合出来的 \\(f\\) 的雅可比行列式刚好可以表示为每一个 \\(f_{i}\\) 的雅可比行列式相乘再求行列式。 \\(det~J(f)=det\\prod_{i=1}^{N}J(f_{i})=\\prod_{i=1}^{N}det~J(f_{i})\\) 因为每一个样本都是独立同分布采样出来的，所以它的log likelihood就是把他们的每一个log likelihood加起来。由于做过变量代换，就可以把它变成我们知道的非常简单的分布加上剩下的log 雅可比行列式的和。 3.png 计算 通过最大似然估计，我们便可以训练模型了。但问题在于，如何构建这种可逆映射和如何让雅可比行列式方便计算。因为对于一般的雅可比行列式的计算复杂度是 \\(O(n^3)\\) ，但是我们可以构造半三角的雅可比行矩阵，这样行列式的计算复杂度只有 \\(O(n)\\) 了 NICE: Non-linear Independent Components Estimation NICE的目标是找到一个transformation \\(z=f(x)\\) , 将数据映射到一个新的空间中; 这个空间中的 \\(z\\) 的各个分量 \\(z_d\\) 之间都是独立的, 即 \\(p_\\theta(z)=\\prod_d p_{\\theta_d}(z_d)\\) .在这种\"各分量独立\"的假设下, 模型会自发地学习\"most important factors of variation\"; 否则, 比如 \\(h_1\\) 和 \\(h_2\\) 之间不独立, 那么就浪费了一部分建模能力, 从而无法达到最好的建模效果. 通过 \\(z\\) 的先验分布和 \\(x=f^{-1}(z)\\) , 可以实现 \\(x\\) 的生成(采样)。一般可以假定 \\(z\\) 的分布满足标准高斯分布。 映射构造(Additive coupling layer) 如何构造构造半三角的雅可比行矩阵呢？NICE给出的方法是： \\(z_{1\\sim d} = x_{1\\sim d}\\) \\(z_{ {d\\sim D} } = x_{ {d\\sim D} } + u_{\\theta}(x_{ {1\\sim d} })\\) 这个变换的雅克比矩阵为 \\[ \\frac{\\partial z}{\\partial x}=\\left[ \\begin{array}{cc} I_d &amp; \\bar{0} \\\\ [\\frac{\\partial u_\\theta}{\\partial x_{1\\sim d} }] &amp; I_{n-d} \\\\ \\end{array} \\right] \\] 这个映射的逆变换也很简单，为 \\(x_{1\\sim d} = z_{1\\sim d}\\) \\(x_{ {d\\sim D} } = z_{ {d\\sim D} } - u_{\\theta}(z_{ {1\\sim d} })\\) Combining coupling layers 事实上, 这个 \\(f\\) 是要用很多层叠在一起得到的, 即 \\(f=f_L \\circ ... \\circ f_2 \\circ f_1\\) 。 在堆叠coupling layer的时候, 注意到每个变换有一部分输入是不变的。这样才能让所有部分都能得到变换. 即, 第一层 \\(z_1=x_1\\) , 变 \\(x_2\\) , 那么第二层就 \\(z_2=x_2\\) , 变 \\(z_1\\) . 另外, 堆叠后的雅克比行列式为 \\[ \\left|\\det \\frac{\\partial z}{\\partial x} \\right| = \\left|\\det \\frac{\\partial f_L(x)}{\\partial f_{L-1}(x)}\\right| \\cdot \\left|\\det \\frac{\\partial f_{L-1}(x)}{\\partial f_{L-2}(x)}\\right| \\cdot \\ldots \\cdot \\left|\\det \\frac{\\partial f_2(x)}{\\partial f_1(x)}\\right| \\] 这些行列式的绝对值为1。 Allowing scaling 因为每个行列式的绝对值都是1, 因此 \\(f\\) 是volume preserving（体积不变的）的. 为了消除这个限制, 在 \\(f_L\\) 后又乘了一个diagonal scaling matrix \\(S\\) , 即 \\(z=S \\cdot f_{1, ...,L}(x)\\) . 这样既可以让一些重要特征又更大的变化范围, 又可以让一些不重要的特征减小变化范围(降维). 所以最后目标函数为 \\(\\log p_X(x)=\\sum_{i=1}^D [\\log p_{H_i}(f_i(x)) + \\log |S_{ii}|]\\) Density Estimation Using Real NVP Real NVP将NICE中的每一层的映射改为如下: \\(\\begin{aligned} z_{1:d}&amp;=x_{1:d}\\\\ z_{d+1:D} &amp;=x_{d+1:D} \\odot exp(s(x_{1:d})) +t(x_{1:d}) \\end{aligned}\\) 逆变换为 \\(\\begin{aligned} x_{1:d}&amp;=z_{1:d}\\\\ x_{d+1:D} &amp;=(z_{d+1:D}- t(x_{1:d})) \\odot exp(-s(x_{1:d})) \\end{aligned}\\) 这个变换的雅克比矩阵为 \\[ \\frac{\\partial z}{\\partial x}=\\left[ \\begin{array}{cc} I_d &amp; \\bar{0} \\\\ \\frac{\\partial z_{d+1:D} }{\\partial x_{1:d} } &amp; diag(exp(s(x_{1:d}))) \\\\ \\end{array} \\right] \\] 其中 \\(diag(exp(s(x_{1:d})))\\) 是将 $ exp(s(x_{1:d}))$ 这个向量展开为对角矩阵. 这个雅克比矩阵的log-determinant为 \\[\\prod_{i=1}^d \\log \\exp(s(x_{1:d}))=\\sum_{i=1}^d s(x_{1:d})\\] 其中没有任何 \\(s\\) 和 \\(t\\) 行列式的计算, 因此二者可以任意复杂且hidden layer采用不同于输入的维度. 这样我们便完成了一个更加复杂的构造，同时它的表现也自然比NICE更好。","categories":[{"name":"Stanford CS236深度生成模型","slug":"Stanford-CS236深度生成模型","permalink":"https://jia040223.github.io/categories/Stanford-CS236%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}],"tags":[{"name":"生成模型","slug":"生成模型","permalink":"https://jia040223.github.io/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}]},{"title":"VAEs","slug":"VAEs","date":"2024-09-13T11:31:24.000Z","updated":"2024-10-08T00:28:02.556Z","comments":true,"path":"2024/09/13/VAEs/","permalink":"https://jia040223.github.io/2024/09/13/VAEs/","excerpt":"","text":"本学习笔记用于记录我学习Stanford CS236课程的学习笔记，分享记录，也便于自己实时查看。 潜变量 对于生成模型，我们可以试图寻找一组潜变量z，这个潜变量可以有具体含义，例如对于人脸生成模型的眼睛，鼻子，嘴巴等。通过修改这些潜变量我们可以得到不同风格的生成对象。但是对于图片或者自然语言而言，人为指定这种潜变量极为困难。 所以我们可以并不人为指定潜变量的含义，例如无监督学习的GMM（高斯混合聚类）就并没有指定每个类别具体的含义。但高斯混合聚类人为指定了类别的数量，这对于生成模型也是很难实现定义的。 对于GMM来说，事实上是定义了一组离散的潜变量，在每个潜变量下的数据分布服从高斯分布，它可以给我们一些启示，虽然每个类别的概率只是定义为正态分布，但它组合之后可以形成非常复杂的概率分布。 所以不妨我们可以设定有无穷多个高斯聚类的组合，即设定潜变量 z 是一个连续的随机变量，而每个潜变量 z 的值对应于一个高斯分布，事实上这也正是VAEs做的 核心思想 VAE 的目标是学习一个生成器，将随机向量 $z \\in R^d$ 映射到 $x \\in R^D$ , 使得 $x$ 的分布尽可能接近真实数据的分布。 这里的 $z$ 其实就是上面提到的潜变量，他是一个连续的随机变量，实践中一般定义为服从高斯分布。而对于每个z的值，我们假设x的分布是满足均值为 $\\mu(z)$ ，协方差矩阵为 $\\Sigma(z)$ （可以通过神经网络进行学习）的高斯分布。理论上这样的组合可以逼近任意的概率分布。 当然PPT中的 $z\\sim N(0, I)$ 只是一个例子，也可以有更复杂的定义，但在实践中一般使用标准正态分布。 生成和训练 损失函数 对于一个生成模型来说，生成和评估的难易很大程度上决定了它的实用性和价值。对于上面VAE的假设来说，生成是很简单。即假设我们已经知道了 $p(x|z)$ ，我们只需要先采样 $z$ ，再采样 $x$ 就能得到数据。 但是评估并不容易，这意味着模型的训练可能是一个棘手的问题。对于评估，既然是衡量两个分布的相似度，我们能否直接用各种散度（如 KL 散度）作为损失函数呢？当然可以。在蒙特卡洛抽样（Monte Carlo Sampling）下，最小化KL散度就是最大似然估计。 那么我们的目标是 $θ_∗=argmax \\sum_{i=1}^{n} logp_θ​(x_i) $ ，注意到 $\\sum logP_\\theta(x) =\\sum log(\\sum q(z)P_\\theta(x|z))$ 对于等式右边的计算是非常复杂的，因为 $z$ 的取值理论上具有无穷多个 所以我们需要对这个公式进行简化，注意到 $$ \\begin{align} P_\\theta(x) &amp;= \\sum (q(z) \\frac{p_\\theta(z, x)}{q(z)}) \\nonumber \\ &amp;= E_{z \\sim q(z)}\\left(\\frac{p_\\theta(z,x)}{q(z)}\\right) \\end{align} $$ 通过蒙特卡洛抽样（Monte Carlo Sampling），我们可以从 $q(z)$ 中采样若干数据点，然后进行平均即可估计 $P_\\theta(x)$ 的值。但很可惜，我们无法通过蒙特卡洛抽样来估计 $log(P_\\theta(x))$ , 因为 $log(E_{z \\sim q(z)}(\\frac{p_\\theta(z,x)}{q(z)})) \\ne E_{z \\sim q(z)}(log(\\frac{p_\\theta(z,x)}{q(z)}))$ 但幸运的是，对于对数函数是一个严格的凹函数，所以对于凹函数来说有 $log(px + (1-p)x^{‘}) \\geq plogx +(1-p)logx^{’}$ ，进一步扩展便就是著名的琴生不等式： 琴生不等式 因此 $log(E_{z \\sim q(z)}(\\frac{p_\\theta(z,x)}{q(z)})) \\geq E_{z \\sim q(z)}(log(\\frac{p_\\theta(z,x)}{q(z)}))$ 所以我们可以通过这种方法来估计似然的下限，即上面不等号的右边，叫做ELBO（Evidence Lower Bound） 至于这个界限有多紧，我们对 $logP(x)$ 进行一下推导，就能得到它们之间相差的便是 $D_{KL}(q(z)||p(z|x;\\theta)$ ，也就是说当 $q(z)$ 与我们的后验分布越接近，这个界限越紧。 其实这里的推导就是EM算法里面的推导，最大化ELBO的过程就是对应于EM算法里面的M步（后续有机会可能也会写一写）。非常可惜的是，EM 算法无法直接应用于此，因为 E-step 要求我们能够表达出后验分布 $p_\\theta(z|x)$ ，但没关系，如果我们能够最大化ELBO，也能保证似然的下限被最大化。 问题似乎解决了，但值得注意的是，ELBO 是关于函数 $q$ 的泛函，也就是说 $q$ 可以取任意函数，这并不好直接优化。为了解决这个问题，我们可以将 $q(z)$ 限制为以 $\\phi$ 为参数的某可解分布族 $q_\\phi(z|x)$ ，这样优化变量就从函数 $q$ 变成了参数 $\\phi$ 。不过，由于我们限制了 $q$ 的形式，所以即便能求出最优的参数 $\\phi$ ，也大概率不是 $q$ 的最优解。显然，为了尽可能逼近最优解，我们应该让选取的分布族越复杂越好。 那么这里有一个小问题——为什么 $q(z)$ 参数化后写作 $q_\\phi(z|x)$ 而不是 $q_\\phi(z)$ ? 首先， $q$ 本来就是我们人为引入的，它是否以 $x$ 为条件完全是我们的设计，且并不与之前的推导冲突；其次，ELBO与似然当 $q(z)=p_θ(z|x)$ 时是完全等价的，可见对于不同的 $x$ ，其 $q(z)$ 的最佳形式是不同的，所以这么设定有利于减少ELBO与似然的距离。 在VAE中 的 $p_θ(x|z)$ 和 $q_\\phi(z|x)$ 都由神经网络表示，因此我们用梯度下降来最大化 ELBO 即可。即对ELBO取负数就是最终的损失函数。 注意到这样的形式中并没有 $p_θ(x|z)$ 一项，我们只需要稍微变化一下： $$ \\begin{align} L(x;\\theta, \\phi) &amp;= \\sum q_{\\phi} (z|x)\\left[\\log(p_{\\theta}(z,x;\\theta)) - \\log(q_{\\phi}(z|x))\\right] \\ &amp;= \\sum q_{\\phi} (z|x)\\left[\\log(p_{\\theta}(z,x;\\theta)) - \\log(p(z)) + \\log(p(z)) - \\log(q_{\\phi}(z|x))\\right] \\ &amp;= \\sum q_{\\phi} (z|x)\\left[\\log(p_{\\theta}(x|z)) - \\log\\left(\\frac{q_{\\phi}(z|x)}{p(z)}\\right)\\right] \\ &amp;= E_{z \\sim q_{\\phi}(z|x)}\\left[\\log(p_{\\theta}(x|z))\\right] - D_{KL}(q_{\\phi}(z|x) || p(z)) \\end{align} $$ 这里就把我们的目标分成了两项： 第一项是重构项，要求我们尽可能重构数据本身 第二项是正则项，要求我们的后验与先验接近 所以可以看到，它与自动编码器最大的区别在于有第二项，这保证了隐藏变量 $z$ 的分布，从而我们可以从先验中对 $z$ 取样从而进行生成。换句话来说，VAEs是对潜变量进行了正则化的自动编码器，因为我们知道了潜变量 $z$ 的分布形式，所以它能够用于生成。 按照蒙特卡洛抽样（Monte Carlo Sampling），理论上求这个期望需要对每个样本多次采样进行计算，最后平均。但在具体实践中，往往采样一次进行计算就行。 梯度计算细节：重参数化技巧 有一个细节是现在 $z$ 是从 $q_\\phi(z|x)∼N(μ_ϕ(x)，diag(\\sigma^{2}_ϕ(x)))$ 中采样的，但梯度无法经过采样传播到参数 $\\phi$ 。但其实解决方法很简单，对于高斯函数，只需要先从 $N(0,I)$ 中采样 $\\epsilon$ 再计算 $z=μ_ϕ(x)+\\epsilon⋅σ_ϕ(x)$ 即可。 这种技巧也叫做重参数化技巧，其最开始应该是在强化学习中出现的，后面有时间也可以写一写。","categories":[{"name":"Stanford CS236深度生成模型","slug":"Stanford-CS236深度生成模型","permalink":"https://jia040223.github.io/categories/Stanford-CS236%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}],"tags":[{"name":"生成模型","slug":"生成模型","permalink":"https://jia040223.github.io/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}]}],"categories":[{"name":"Probabilistic Machine Learning","slug":"Probabilistic-Machine-Learning","permalink":"https://jia040223.github.io/categories/Probabilistic-Machine-Learning/"},{"name":"生活blog","slug":"生活blog","permalink":"https://jia040223.github.io/categories/%E7%94%9F%E6%B4%BBblog/"},{"name":"旅行日志","slug":"生活blog/旅行日志","permalink":"https://jia040223.github.io/categories/%E7%94%9F%E6%B4%BBblog/%E6%97%85%E8%A1%8C%E6%97%A5%E5%BF%97/"},{"name":"更新日志","slug":"更新日志","permalink":"https://jia040223.github.io/categories/%E6%9B%B4%E6%96%B0%E6%97%A5%E5%BF%97/"},{"name":"Stanford CS236深度生成模型","slug":"Stanford-CS236深度生成模型","permalink":"https://jia040223.github.io/categories/Stanford-CS236%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"},{"name":"机器学习","slug":"机器学习","permalink":"https://jia040223.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jia040223.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"数学","slug":"数学","permalink":"https://jia040223.github.io/tags/%E6%95%B0%E5%AD%A6/"},{"name":"旅行日志","slug":"旅行日志","permalink":"https://jia040223.github.io/tags/%E6%97%85%E8%A1%8C%E6%97%A5%E5%BF%97/"},{"name":"更新日志","slug":"更新日志","permalink":"https://jia040223.github.io/tags/%E6%9B%B4%E6%96%B0%E6%97%A5%E5%BF%97/"},{"name":"生成模型","slug":"生成模型","permalink":"https://jia040223.github.io/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"},{"name":"概率论与数理统计","slug":"概率论与数理统计","permalink":"https://jia040223.github.io/tags/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/"}]}