{"meta":{"title":"Serendipity's Blog","subtitle":"","description":"","author":"Serendipity","url":"https://jia040223.github.io","root":"/"},"pages":[{"title":"关于我","date":"2024-09-17T10:50:16.194Z","updated":"2024-09-17T10:50:16.194Z","comments":false,"path":"about/index.html","permalink":"https://jia040223.github.io/about/index.html","excerpt":"","text":"欢迎来到我的博客！这个博客是我个人的学习和探索之旅的记录，我希望通过它分享我的想法和见解。无论是技术、理论还是生活中的点滴，我都会在这里进行更新。如果你对我的内容有任何疑问或建议，欢迎通过 GitHub 与我联系。期待与你的交流与互动！ 个人简介 教育经历 高中：石门县第一中学 本科：中国科学院大学（计算机科学与技术专业） 目前感兴趣的方向 大模型 深度学习 计算机视觉 关于博客 学习笔记 我会持续更新我目前正在学习的内容和笔记，包括但不限于深度学习与大模型，计算机视觉，微积分与线性代数等。 项目地址：repository 科研日志 我会记录一些科研上的心得体会，分享一些我在科研中的经验和教训。 生活记录 我会记录一些好玩的生活片段，同样包括但不限于旅游日志、生活琐事和所悟所想，分享是一种很好的生活调味剂，希望你会喜欢。 联系方式 Email：jiachenghao21@mails.ucas.ac.cn QQ：1142747996"}],"posts":[{"title":"GANs","slug":"GANs","date":"2024-09-18T14:10:33.000Z","updated":"2024-09-18T14:26:53.226Z","comments":true,"path":"2024/09/18/GANs/","permalink":"https://jia040223.github.io/2024/09/18/GANs/","excerpt":"","text":"本学习笔记用于记录我学习Stanford CS236课程的学习笔记，分享记录，也便于自己实时查看。 引入 前面我们学习了VAEs和Normalizing Flows，这两种模型都是基于最小化KL散度（对似然进行评估）来进行优化的。我们也可以看到，为了进行生成，我们往往会定义一个潜变量 \\(z\\) ，所以对似然进行评估并不容易。VAEs是通过优化似然的下限ELBO来绕过这个问题，而Normalizing Flows是通过限制映射的形式来计算似然。 直接计算似然来进行评估，要么只能计算其下界，要么需要限制映射的形式。那有没有一种方法能够用间接方法代替这种直接比较，使生成分布变得越来越接近真实分布呢？GANs便是基于一种间接的评估方式进行设计的。 基本思想 GANs的间接方法采用这两个分布的下游任务形式。然后，生成网络的训练是相对于该任务进行的，使生成分布变得越来越接近真实分布。GANs 的下游任务是区分真实样本和生成样本的任务。或者我们可以说是“非区分”任务，因为我们希望区分尽可能失败。 因此，在 GANs 架构中，我们有一个判别器，它接收真实和生成数据样本，并尽可能地对它们进行分类；还有一个生成器，它被训练成尽可能地欺骗判别器。即GANs由2个重要的部分构成： 生成器(Generator)：通过机器生成数据，目的是“骗过”判别器。 判别器(Discriminator)：判断数据是真实的还是生成的，目的是找出生成器做的“假数据”。 训练过程 我们知道GANs的思想后，便能很直观的想到用分类问题的交叉熵作为判别器的损失函数。同时生成器的目的则是最大化这个交叉熵损失函数（混淆判别器），所以我们的训练目标是： \\[\\mathop{\\text{min}}\\limits_{G}\\mathop{\\text{max}}\\limits_{D} \\ V(G,D) = \\mathbb{E}_{x \\sim p_{data}(x)} [\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)} [\\log(1 - D(G(z)))] \\] 其中 \\(G\\) 指的是生成器， \\(D\\) 指的是判别器。 所以我们的训练目标是一个极大极小的优化问题，在实际中，我们只需要从数据集中进行采样，然后用生成器进行采样，然后对上面的目标函数进行近似计算，最后进行梯度上升或者梯度下降即可 ## 与散度的关系 那么为什么这样的设计能够间接地去让生成器生成的样本与真实样本的分布相同呢？ 其实本质上，GANs通过引入判别器来间接地计算了 \\(\\frac{P_\\theta(x)}{P_{data}(x)}\\) ，可以证明，对于一个生成器下的最佳判别器对给定 \\(x\\) 的判定为真实样本的概率是\\(\\frac{P_{data}(x)}{P_{data}(x) + P_\\theta(x)}\\)， 证明如下： *Proof*: 二分类交叉熵损失函数为： \\[\\begin{align} \\mathrm{BCE}(\\mathcal P_1,\\mathcal P_2)&amp;=-\\mathbb E_{x\\sim \\mathcal P_1}[\\log D(x)]-\\mathbb E_{x\\sim \\mathcal P_2}[\\log(1-D(x))]\\\\ &amp;=-\\int \\log D(x)\\cdot p_1(x)\\mathrm d x-\\int\\log(1-D(x))\\cdot p_2(x)\\mathrm d x\\\\ &amp;=-\\int \\left[\\log D(x)\\cdot p_1(x)+\\log(1-D(x))\\cdot p_2(x)\\right]\\mathrm d x\\\\ \\end{align} \\\\\\] 易知 \\(y=a\\log x+b\\log(1-x)\\) 在 \\(x=\\frac{a}{a+b}\\) 处取到唯一极大值（其中 \\(0\\leq a,b\\leq1\\) ），所以欲使上式最小，只需： \\(\\forall x,\\,D(x)=\\frac{p_1(x)}{p_1(x)+p_2(x)} \\\\\\) 这样就证明完成了。 那么，再看我们的训练目标： \\[\\min_G\\max_D V(G, D) \\\\ \\begin{align} V(G, D)&amp;=\\mathbb E_{x\\sim\\mathcal P_{data}}[\\log D(x)]+\\mathbb E_{z\\sim \\mathcal P_z}[\\log(1-D(G(z)))]\\\\ &amp;=\\mathbb E_{x\\sim\\mathcal P_{data}}[\\log D(x)]+\\mathbb E_{x\\sim \\mathcal P_{\\theta}}[\\log(1-D(x))] \\end{align} \\\\\\] 而最优判别器为： \\(D^\\ast(x)=\\frac{p_{data}(x)}{p_{data}(x)+p_\\theta(x)} \\\\\\) 将最优判别器代入 \\(G\\) 的优化目标： \\[\\begin{align} V(G, D^\\ast)&amp;=\\mathbb E_{x\\sim \\mathcal P_{data}}\\left[\\log\\frac{p_{data}(x)}{p_{data}(x)+p_\\theta(x)}\\right]+\\mathbb E_{x\\sim \\mathcal P_\\theta}\\left[\\log\\frac{p_\\theta(x)}{p_{data}(x)+p_\\theta(x)}\\right]\\\\ &amp;=2\\mathrm {JS}(\\mathcal P_{data}\\|\\mathcal P_\\theta)-2\\log2 \\end{align} \\\\ \\] 因此，生成器实际上在最小化 $ P_{data}$ 和 \\(\\mathcal P_\\theta\\) 的 \\(\\mathrm{JS}\\) 散度，从而让生成数据的分布 \\(\\mathcal P_\\theta\\) 接近真实分布 \\(\\mathcal P_{data}\\) 。 注： \\(JS\\) 散度的定义如下： \\[\\begin{align} \\mathrm{JS}(\\mathcal P_1\\|\\mathcal P_2)&amp;=\\frac{1}{2}\\left[\\mathrm{KL}\\left(\\mathcal P_1\\|\\mathcal P_A\\right)+\\mathrm{KL}\\left(\\mathcal P_2\\|\\mathcal P_A\\right)\\right]\\\\ &amp;=\\log 2+\\frac{1}{2}\\mathbb E_{x\\sim\\mathcal P_1}\\left[\\log\\frac{p_1(x)}{p_1(x)+p_2(x)}\\right]+\\frac{1}{2}\\mathbb E_{x\\sim\\mathcal P_2}\\left[\\log\\frac{p_2(x)}{p_1(x)+p_2(x)}\\right] \\end{align} \\\\\\] 其相比 \\(KL\\) 散度最大的特点便是其是对称的。 可以看出GANs是通过判别器来巧妙地规避了计算似然的问题，但正是因为在实践中我们很难得到真正的最佳判别器，所以实际上我们很多时候只是在优化 \\(JS\\) 散度的一个下界，笔者绝对这是GANs不得不直面的一个问题。 fGAN F-散度(F-divergence) 在概率统计中，f散度是一个函数，这个函数用来衡量两个概率密度\\(p\\)和\\(q\\)的区别，也就是衡量这两个分布多么的相同或者不同。像 \\(KL\\) 散度和 \\(JS\\) 散度都是它的一种特例 f散度定义如下： \\[{D_f}(\\mathcal P_1\\|\\mathcal P_2)=\\int f (\\frac{p_2(x)}{p_1(x)})\\cdot p_1(x)\\mathrm d x=\\mathbb E_{x\\sim\\mathcal P_1}\\left[f(\\frac{p_2(x)}{p_1(x)})\\right] \\\\\\] \\(f(·)\\) 就是不同的散度函数， \\(D_f\\) 就是在f散度函数下，两个分布的差异。规定 \\(f\\) 是凸函数(为了用琴生不等式) $f ( 1 ) = 0 $ (如果两个分布一样，刚好公式=0) 这两个规定保证了 \\(D_f\\) 是非负的，而且当两个分布相同时，其值为0，一些常见散度的 \\(f\\) 定义如下： 共轭函数(Fenchel Conjugate) 一个函数 \\(f:\\;\\mathbb{R}^n\\mapsto\\mathbb{R}\\) 的 Frenchel 共轭为： \\[\\begin{align} f^*( t)=\\sup_{ x}\\big(\\langle t, x\\rangle-f( x)) \\end{align}\\] Fenchel 共轭有几何上的解释。当 $ x$ 固定时， \\(\\langle t, x\\rangle-f( x)\\) 是一个仿射函数，因此 Fenchel 共轭就是一组仿射函数的上确界。如果 \\(f\\) 可微，那么仿射函数取得上确界的位置正好是 \\(f\\) 的切线，此处有 \\(\\nabla f( x)= t\\) 。 我们拿 $f ( x ) = x l o g x $ 来说，当 \\(x=10,1, 0.1\\) 时可以看到相应的函数直线，可以看到最大化y的点连起来是个凸函数，很类似$ e^{t-1}$ 公式图像： 用数学来推一下： 将 \\(f ( x ) = x l o g x\\) 代入 $y ( t ) = x t − f ( x ) $ ，得 $y ( x ) = x t − x l o g x $ ,对于每个给定的 \\(t\\) 都可以求出最大值，求导为0即可。 求导后得： \\(t − l o g x − 1 = 0\\) ,即 \\(x=e^{t-1}\\) ，代入$ f(t)$ ，得 $ f^(t)=te{t-1}-e{t-1}(t-1)=e{t-1}$ 读者可以对这个 $ f^*(t)$ 再求一次共轭，可以发现其又变回原函数了。 事实上，可以证明，对于凸函数来说$ f^{**}(x) = f(x)$ 应用于GAN 那这个跟GAN有啥关系呢？ 假如我们用一个 \\(D_f\\) 来评估生成模型，对于 \\(p(x)\\) 和 \\(q(x)\\) 之间的 f-divergence： \\[ \\begin{aligned} D_f(P||Q) &amp;= \\int_{x} q(x) f\\left(\\frac{p(x)}{q(x)}\\right) dx \\\\ &amp;= \\int_{x} q(x) \\left( \\max_{t \\in \\operatorname{dom}(f^*)} \\left\\{\\frac{p(x)}{q(x)}t - f^*(t)\\right\\} \\right) dx \\end{aligned} \\] 记一个函数 D(x)，它输入是 \\(x\\) ，输出是 \\(t\\) ，用该函数代替上式中的 \\(t\\) ，得到 \\[ \\begin{aligned} D_f(P||Q)&amp;\\geq\\int \\limits_{x}q(x)(\\frac{p(x)}{q(x)}D(x)-f^{*}(D(x)))dx\\\\ &amp;= \\int \\limits_{x}p(x)D(x)dx-\\int \\limits_{x}q(x)f^{*}(D(x))dx \\end{aligned} \\] D(x) 其实就是判别器，可以看出，它依然是在解一个求最大值问题，通过这种方法，去逼近 f-divergence。 \\[D_f(P||Q)\\approx\\max \\limits_{D}\\int \\limits_{x}p(x)D(x)dx-\\int \\limits_{x}q(x)f^{*}(D(x))dx\\] p(x) 和 q(x) 本质上是一个概率，于是有 \\[D_f(P||Q)\\approx\\max \\limits_{D}\\{E_{x\\sim P}[D(x)]-E_{x\\sim Q}[f^*(D(x))]\\}\\] 用 \\(P_{data}\\) 和 \\(P_\\theta\\) 来指代 P 和 Q，有 \\[D_f(P_{data}||P_\\theta)\\approx\\max \\limits_{D}\\{E_{x\\sim P_{data}}[D(x)]-E_{x\\sim P_\\theta}[f^*(D(x))]\\}\\] 有没有发现这一套下来很熟悉？其实这还是我们之前训练生成器判别器的那一套流程。也就是 \\[ \\begin{aligned} G^*&amp;=\\mathop{argmin} \\limits_{G}D_f(P_{data}||P_\\theta)\\\\&amp;=\\mathop{argmin} \\limits_{G}\\max \\limits_{D}\\{E_{x\\sim P_{data}}[D(x)]-E_{x\\sim P_\\theta}[f^*(D(x))]\\}\\\\&amp;=\\mathop{argmin} \\limits_{G}\\max \\limits_{D}V(G, D) \\end{aligned} \\] 只不过这次的损失函数更加 general 了。换不同的 \\(f(x)\\) ，就可以量不同的散度（divergence）。 WGAN JS散度 to Wasserstein（Earth-Mover EM）距离 JS散度的问题 考虑两个分布\"完全不相交\"的时候，会发现 \\(JS\\) 散度为常量，梯度为 \\(0\\) 无法优化。 下面一个例子来说明: 假设两个二维空间上的概率分布，记为 \\({P}_d(X_1, Z)\\) 和 \\({P}_g(X_2, Z)\\) 。我们刻画 \\(Z \\sim U(0, 1)\\) 一个 \\([0, 1]\\) 上的均匀分布，而分别令 $ X_1 = 0$ 和 \\(X_2 = \\theta\\) ，因而，它们在二维空间上的概率分布空间就是两条平行线（垂直于 \\(x\\) 的轴，而平行于 \\(z\\) 的轴）。 当 \\(\\theta = 0.5\\) 时，我们考量等价于JS散度的损失函数 \\(V(G, D^*)\\) ，由于两个分布概率大于0的空间范围是完全没有重叠的，因此，对于任意 \\(p_d(x,y) \\ne 0\\) 必然有 \\(p_g(x, y) =0\\) 成立，反之亦然。 因而我们就有，对于任意 \\(x \\in \\mathbb{R}^2\\) ， \\[V(G, D^*)= \\int_x p_d(x) log \\frac{p_{d}(x)}{p_{d}(x) + p_{g}(x)} + p_g(x)log \\frac{p_{g}(x)}{p_{d}(x) + p_{g}(x)} dx \\\\ = \\int_x p_d(x) log (1) + p_g(x)log (1) dx = 0 \\\\ \\] 此时，损失函数恒为常量，无法继续指导生成器 \\(G(x)\\) 的优化。即此时出现了梯度消失的问题。 Wasserstein距离 为了弥补JS散度的局限性，我们需要一种全新的”分布间距离“的度量来进行优化，即使用Wasserstein距离，也被称为“推土机距离”（Earth-Mover），它定义如下： \\(W({P}_d, {P}_g) = inf_{\\gamma \\in \\Pi({P}_d, {P}_g)} {E}[||x - y||] \\\\\\) 这样数学形式的刻画可能会让人看得颇为一头雾水，我们逐步来分析解释它。 其中， \\(\\Pi({P}_d, {P}_g)\\) 代表一个 \\({P}_d, {P}_g\\) 构成的联合分布的集合，且这个集合中的所有联合分布必须满足其边际分布分别为 \\({P}_d, {P}_g\\) 。 \\(||x-y||\\) 是两个分布所在空间 \\(\\mathbb{R}^n\\) 中两点的欧式距离。 我们可以将 \\(\\Pi({P}_d, {P}_g)\\) 中的元素理解为一种“概率的搬运方案”。 而 \\(\\gamma\\) 是上述集合中的一个联合分布，可以使得任意两点的欧式距离期望最小，即将一个分布搬运为另外一个分布的最小开销。 此时，我们再重新观察上面的场景，当概率分布式为两条平行线上的均匀分布时，显然，最佳方案就是直接与x轴平行地进行概率搬运，对应为： \\(W(P_0, P_\\theta) = |\\theta|\\) 。此时，即使两个分布完全没有重叠部分，我们仍然能通过优化Wasserstein距离来实现两个概率分布之间的距离优化。 可以给出证明的是，就像JS散度一样，Wasserstein距离收敛于0时，两个分布也完全一致。 固然，通过Wasserstein距离优化GAN的想法颇为\"美好\"，不过，找到\"最优搬运方案\"的优化问题却是难事，在实现层面上，我们难以直接计算Wasserstein距离。不过，基于对偶理论可以将Wasserstein距离变换为积分概率度量IPM框架下的形式，来方便我们进行优化。 IPM也是用于衡量两个分布之间的距离，它的想法是寻找某种限制下的函数空间 \\(\\mathbb{F}\\) 中的一个函数 \\(f(·)\\) ，使得对任意位置两个分布的差异最大： \\[d_F(p, q) = sup_{f \\in F} \\mathbb{E}_{x \\sim P}[f(x)] - \\mathbb{E}_{x \\sim Q}[f(x)] \\\\\\] 对于Wasserstein距离而言，则变为： $\\(W(p, q) = sup_{||f||_L \\le 1} \\mathbb{E}_{x \\sim P}[f(x)] - \\mathbb{E}_{x \\sim Q}[f(x)] \\\\\\)￥ 因而，在函数 $ f(·)$ 满足Lipschitz约束的函数空间中，即 \\(||f(x) - f(y)|| \\le K||x - y||\\) ，找到最佳的函数 \\(f(·)\\) ，该情况下上式的结果则为Wasserstein距离。 这个函数 \\(f(·)\\) 难以求解，但我们可以用神经网络来拟合它。需要注意的是，从此开始，GAN的 \\(D\\) 就不再是先前我们认为的“真假判别器”了，它的意义变成了一个距离的度量。此时，GAN的生成器并不改变仍然生产图片，对生成器的训练则是减小与真实分布的Wasserstein距离，判别器 \\(D\\) 负责给出真实图像和生产图像样本之间的Wasserstein距离，相应的，在固定生成器优化判别器时，化则变为了寻找函数空间 \\(\\mathbb{F}\\) 中最佳的 \\(f(·)\\) 。 下面的图就可以体现传统GAN的判别器梯度和WGAN的判别器梯度的区别 WGAN便有效解决了某些情况下传统GAN的梯度消失的问题","categories":[{"name":"Stanford CS236深度生成模型","slug":"Stanford-CS236深度生成模型","permalink":"https://jia040223.github.io/categories/Stanford-CS236%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}],"tags":[{"name":"生成模型","slug":"生成模型","permalink":"https://jia040223.github.io/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}]},{"title":"Normalizing Flows","slug":"Normalizing Flows","date":"2024-09-17T10:18:14.000Z","updated":"2024-09-17T10:57:29.014Z","comments":true,"path":"2024/09/17/Normalizing Flows/","permalink":"https://jia040223.github.io/2024/09/17/Normalizing%20Flows/","excerpt":"","text":"本学习笔记用于记录我学习Stanford CS236课程的学习笔记，分享记录，也便于自己实时查看。 引入 生成模型模型的目的是让得到的数据分布 \\(P_{\\theta}\\) 与真实的数据分布 \\(P_{data}\\) 相同，也就是需要通过给定的样本来建模对应的分布，使得输入经过该模型后可以生成与给定样本类似的新样本。在这种意义下，评估的最佳方式便是使用极大似然估计，然而VAEs的做法导致计算似然十分复杂，所以我们只能选择计算似然的下界，也就是ELBO。 不妨思考一下，VAEs无法计算似然的原因是什么。不难发现，关键在于需要对所有的潜变量 \\(z\\) 进行积分。所以假如我们有一个可逆映射，使得潜变量 \\(z\\) 和数据 \\(x\\) 之间的是一一对应的，那我们便可以很轻松计算似然了。 Normalizing Flows正是这么做的。但可逆映射意味着潜变量 \\(z\\) 的维度需要和数据 \\(x\\) 的维度一致，所以我们无法利用 \\(z\\) 进行压缩。 简介 正则化流（Normalizing Flow）是一种可逆生成模型，用于将一个原始分布通过学习的变换映射到另一个已知的概率分布。它可以将数据从原始分布转换为目标分布，从而实现数据的生成和采样。 在正则化流中，我们定义一个变换函数，它将输入样本从原始分布映射到目标分布。这个映射是一个可逆函数，确保转换是可逆的，也就是说，在给定目标分布样本的情况下，可以逆向计算出原始分布的样本。这个变换函数通常由一系列的可逆操作组成，每个操作都是可逆的，并且通过组合这些操作可以得到整个变换。常用的可逆操作包括仿射变换、尺度变换、平移变换等。 原理 变量替换 变量替换的形式如下： $ p_{X}(X)=p_{Z}(f(X))|det~J(f(X))|$ \\(Z=f(X)\\) 是一个可逆的变换 \\(J(f(X))\\) 是 \\(f(X)\\) 的雅可比行列式 如何理解呢：即给出一个 \\(X\\) ，使用一个可逆变换 \\(f(\\cdot)\\) 将 \\(X\\) 变为 \\(Z\\) ，那么 \\(p(X)、p(Z)\\) 这两个分布之间相差的就是这样一个雅可比行列式。 ### 流的组合 基本原理：可导的可逆的函数在进行组合后依然是一个可导且可逆的函数 标准化方向： \\(f=f_{1}\\circ f_{2}\\circ....f_{N}\\) 采样构造概率的方向： \\(g=g_{N} \\circ g_{N-1} \\circ .... \\circ g_{1}\\) 这种流动的感觉就是标准化流这个名字的由来。 而由 \\(p_{X}(X)=p_{Z}(f(X))|det~J(f(X))|\\) 可知，上面组合出来的 \\(f\\) 的雅可比行列式刚好可以表示为每一个 \\(f_{i}\\) 的雅可比行列式相乘再求行列式。 \\(det~J(f)=det\\prod_{i=1}^{N}J(f_{i})=\\prod_{i=1}^{N}det~J(f_{i})\\) 因为每一个样本都是独立同分布采样出来的，所以它的log likelihood就是把他们的每一个log likelihood加起来。由于做过变量代换，就可以把它变成我们知道的非常简单的分布加上剩下的log 雅可比行列式的和。 计算 通过最大似然估计，我们便可以训练模型了。但问题在于，如何构建这种可逆映射和如何让雅可比行列式方便计算。因为对于一般的雅可比行列式的计算复杂度是 \\(O(n^3)\\) ，但是我们可以构造半三角的雅可比行矩阵，这样行列式的计算复杂度只有 \\(O(n)\\) 了 NICE: Non-linear Independent Components Estimation NICE的目标是找到一个transformation \\(z=f(x)\\) , 将数据映射到一个新的空间中; 这个空间中的 \\(z\\) 的各个分量 \\(z_d\\) 之间都是独立的, 即 \\(p_\\theta(z)=\\prod_d p_{\\theta_d}(z_d)\\) .在这种\"各分量独立\"的假设下, 模型会自发地学习\"most important factors of variation\"; 否则, 比如 \\(h_1\\) 和 \\(h_2\\) 之间不独立, 那么就浪费了一部分建模能力, 从而无法达到最好的建模效果. 通过 \\(z\\) 的先验分布和 \\(x=f^{-1}(z)\\) , 可以实现 \\(x\\) 的生成(采样)。一般可以假定 \\(z\\) 的分布满足标准高斯分布。 映射构造(Additive coupling layer) 如何构造构造半三角的雅可比行矩阵呢？NICE给出的方法是： \\(z_{1\\sim d} = x_{1\\sim d}\\) \\(z_{ {d\\sim D} } = x_{ {d\\sim D} } + u_{\\theta}(x_{ {1\\sim d} })\\) 这个变换的雅克比矩阵为 \\[ \\frac{\\partial z}{\\partial x}=\\left[ \\begin{array}{cc} I_d &amp; \\bar{0} \\\\ [\\frac{\\partial u_\\theta}{\\partial x_{1\\sim d} }] &amp; I_{n-d} \\\\ \\end{array} \\right] \\] 这个映射的逆变换也很简单，为 \\(x_{1\\sim d} = z_{1\\sim d}\\) \\(x_{ {d\\sim D} } = z_{ {d\\sim D} } - u_{\\theta}(z_{ {1\\sim d} })\\) Combining coupling layers 事实上, 这个 \\(f\\) 是要用很多层叠在一起得到的, 即 \\(f=f_L \\circ ... \\circ f_2 \\circ f_1\\) 。 在堆叠coupling layer的时候, 注意到每个变换有一部分输入是不变的。这样才能让所有部分都能得到变换. 即, 第一层 \\(z_1=x_1\\) , 变 \\(x_2\\) , 那么第二层就 \\(z_2=x_2\\) , 变 \\(z_1\\) . 另外, 堆叠后的雅克比行列式为 \\[ \\left|\\det \\frac{\\partial z}{\\partial x} \\right| = \\left|\\det \\frac{\\partial f_L(x)}{\\partial f_{L-1}(x)}\\right| \\cdot \\left|\\det \\frac{\\partial f_{L-1}(x)}{\\partial f_{L-2}(x)}\\right| \\cdot \\ldots \\cdot \\left|\\det \\frac{\\partial f_2(x)}{\\partial f_1(x)}\\right| \\] 这些行列式的绝对值为1。 Allowing scaling 因为每个行列式的绝对值都是1, 因此 \\(f\\) 是volume preserving（体积不变的）的. 为了消除这个限制, 在 \\(f_L\\) 后又乘了一个diagonal scaling matrix \\(S\\) , 即 \\(z=S \\cdot f_{1, ...,L}(x)\\) . 这样既可以让一些重要特征又更大的变化范围, 又可以让一些不重要的特征减小变化范围(降维). 所以最后目标函数为 \\(\\log p_X(x)=\\sum_{i=1}^D [\\log p_{H_i}(f_i(x)) + \\log |S_{ii}|]\\) Density Estimation Using Real NVP Real NVP将NICE中的每一层的映射改为如下: \\(\\begin{aligned} z_{1:d}&amp;=x_{1:d}\\\\ z_{d+1:D} &amp;=x_{d+1:D} \\odot exp(s(x_{1:d})) +t(x_{1:d}) \\end{aligned}\\) 逆变换为 \\(\\begin{aligned} x_{1:d}&amp;=z_{1:d}\\\\ x_{d+1:D} &amp;=(z_{d+1:D}- t(x_{1:d})) \\odot exp(-s(x_{1:d})) \\end{aligned}\\) 这个变换的雅克比矩阵为 \\[ \\frac{\\partial z}{\\partial x}=\\left[ \\begin{array}{cc} I_d &amp; \\bar{0} \\\\ \\frac{\\partial z_{d+1:D} }{\\partial x_{1:d} } &amp; diag(exp(s(x_{1:d}))) \\\\ \\end{array} \\right] \\] 其中 \\(diag(exp(s(x_{1:d})))\\) 是将 $ exp(s(x_{1:d}))$ 这个向量展开为对角矩阵. 这个雅克比矩阵的log-determinant为 \\[\\prod_{i=1}^d \\log \\exp(s(x_{1:d}))=\\sum_{i=1}^d s(x_{1:d})\\] 其中没有任何 \\(s\\) 和 \\(t\\) 行列式的计算, 因此二者可以任意复杂且hidden layer采用不同于输入的维度. 这样我们便完成了一个更加复杂的构造，同时它的表现也自然比NICE更好。","categories":[{"name":"Stanford CS236深度生成模型","slug":"Stanford-CS236深度生成模型","permalink":"https://jia040223.github.io/categories/Stanford-CS236%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}],"tags":[{"name":"生成模型","slug":"生成模型","permalink":"https://jia040223.github.io/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}]},{"title":"VAEs","slug":"VAEs","date":"2024-09-13T11:31:24.000Z","updated":"2024-09-17T10:13:51.034Z","comments":true,"path":"2024/09/13/VAEs/","permalink":"https://jia040223.github.io/2024/09/13/VAEs/","excerpt":"","text":"本学习笔记用于记录我学习Stanford CS236课程的学习笔记，分享记录，也便于自己实时查看。 潜变量 对于生成模型，我们可以试图寻找一组潜变量z，这个潜变量可以有具体含义，例如对于人脸生成模型的眼睛，鼻子，嘴巴等。通过修改这些潜变量我们可以得到不同风格的生成对象。但是对于图片或者自然语言而言，人为指定这种潜变量极为困难。 所以我们可以并不人为指定潜变量的含义，例如无监督学习的GMM（高斯混合聚类）就并没有指定每个类别具体的含义。但高斯混合聚类人为指定了类别的数量，这对于生成模型也是很难实现定义的。 对于GMM来说，事实上是定义了一组离散的潜变量，在每个潜变量下的数据分布服从高斯分布，它可以给我们一些启示，虽然每个类别的概率只是定义为正态分布，但它组合之后可以形成非常复杂的概率分布。 所以不妨我们可以设定有无穷多个高斯聚类的组合，即设定潜变量 z 是一个连续的随机变量，而每个潜变量 z 的值对应于一个高斯分布，事实上这也正是VAEs做的 核心思想 VAE 的目标是学习一个生成器，将随机向量 \\(z \\in R^d\\) 映射到 \\(x \\in R^D\\) , 使得 \\(x\\) 的分布尽可能接近真实数据的分布。 这里的 \\(z\\) 其实就是上面提到的潜变量，他是一个连续的随机变量，实践中一般定义为服从高斯分布。而对于每个z的值，我们假设x的分布是满足均值为 \\(\\mu(z)\\) ，协方差矩阵为 \\(\\Sigma(z)\\) （可以通过神经网络进行学习）的高斯分布。理论上这样的组合可以逼近任意的概率分布。 当然PPT中的 \\(z\\sim N(0, I)\\) 只是一个例子，也可以有更复杂的定义，但在实践中一般使用标准正态分布。 生成和训练 损失函数 对于一个生成模型来说，生成和评估的难易很大程度上决定了它的实用性和价值。对于上面VAE的假设来说，生成是很简单。即假设我们已经知道了 \\(p(x|z)\\) ，我们只需要先采样 \\(z\\) ，再采样 \\(x\\) 就能得到数据。 但是评估并不容易，这意味着模型的训练可能是一个棘手的问题。对于评估，既然是衡量两个分布的相似度，我们能否直接用各种散度（如 KL 散度）作为损失函数呢？当然可以。在蒙特卡洛抽样（Monte Carlo Sampling）下，最小化KL散度就是最大似然估计。 那么我们的目标是 \\(θ _∗=arg max\\sum_{i=1}^{n}{ logp_ θ​ (x _i})\\) ，注意到 \\(\\sum logP_\\theta(x) =\\sum log(\\sum q(z)P_\\theta(x|z))\\) 对于等式右边的计算是非常复杂的，因为 \\(z\\) 的取值理论上具有无穷多个 所以我们需要对这个公式进行简化，注意到 \\[ \\begin{align} P_\\theta(x) &amp;= \\sum (q(z) \\frac{p_\\theta(z, x)}{q(z)}) \\nonumber \\\\ &amp;= E_{z \\sim q(z)}\\left(\\frac{p_\\theta(z,x)}{q(z)}\\right) \\end{align} \\] 通过蒙特卡洛抽样（Monte Carlo Sampling），我们可以从 \\(q(z)\\) 中采样若干数据点，然后进行平均即可估计 \\(P_\\theta(x)\\) 的值。但很可惜，我们无法通过蒙特卡洛抽样来估计 \\(log(P_\\theta(x))\\) , 因为 \\(log(E_{z \\sim q(z)}(\\frac{p_\\theta(z,x)}{q(z)})) \\ne E_{z \\sim q(z)}(log(\\frac{p_\\theta(z,x)}{q(z)}))\\) 但幸运的是，对于对数函数是一个严格的凹函数，所以对于凹函数来说有 \\(log(px + (1-p)x^{&#39;}) \\geq plogx +(1-p)logx^{&#39;}\\) ，进一步扩展便就是著名的琴生不等式： 琴生不等式 因此 \\(log(E_{z \\sim q(z)}(\\frac{p_\\theta(z,x)}{q(z)})) \\geq E_{z \\sim q(z)}(log(\\frac{p_\\theta(z,x)}{q(z)}))\\) 所以我们可以通过这种方法来估计似然的下限，即上面不等号的右边，叫做ELBO（Evidence Lower Bound） 至于这个界限有多紧，我们对 \\(logP(x)\\) 进行一下推导，就能得到它们之间相差的便是 \\(D_{KL}(q(z)||p(z|x;\\theta)\\) ，也就是说当 \\(q(z)\\) 与我们的后验分布越接近，这个界限越紧。 其实这里的推导就是EM算法里面的推导，最大化ELBO的过程就是对应于EM算法里面的M步（后续有机会可能也会写一写）。非常可惜的是，EM 算法无法直接应用于此，因为 E-step 要求我们能够表达出后验分布 \\(p_\\theta(z|x)\\) ，但没关系，如果我们能够最大化ELBO，也能保证似然的下限被最大化。 问题似乎解决了，但值得注意的是，ELBO 是关于函数 \\(q\\) 的泛函，也就是说 \\(q\\) 可以取任意函数，这并不好直接优化。为了解决这个问题，我们可以将 \\(q(z)\\) 限制为以 \\(\\phi\\) 为参数的某可解分布族 \\(q_\\phi(z|x)\\) ，这样优化变量就从函数 \\(q\\) 变成了参数 \\(\\phi\\) 。不过，由于我们限制了 \\(q\\) 的形式，所以即便能求出最优的参数 \\(\\phi\\) ，也大概率不是 \\(q\\) 的最优解。显然，为了尽可能逼近最优解，我们应该让选取的分布族越复杂越好。 那么这里有一个小问题——为什么 \\(q(z)\\) 参数化后写作 \\(q_\\phi(z|x)\\) 而不是 \\(q_\\phi(z)\\) ? 首先， \\(q\\) 本来就是我们人为引入的，它是否以 \\(x\\) 为条件完全是我们的设计，且并不与之前的推导冲突；其次，ELBO与似然当 \\(q(z)=p_θ(z|x)\\) 时是完全等价的，可见对于不同的 \\(x\\) ，其 \\(q(z)\\) 的最佳形式是不同的，所以这么设定有利于减少ELBO与似然的距离。 在VAE中 的 \\(p_θ(x|z)\\) 和 \\(q_\\phi(z|x)\\) 都由神经网络表示，因此我们用梯度下降来最大化 ELBO 即可。即对ELBO取负数就是最终的损失函数。 注意到这样的形式中并没有 \\(p_θ(x|z)\\) 一项，我们只需要稍微变化一下： \\[ \\begin{align} L(x;\\theta, \\phi) &amp;= \\sum q_{\\phi} (z|x)\\left[\\log(p_{\\theta}(z,x;\\theta)) - \\log(q_{\\phi}(z|x))\\right] \\\\ &amp;= \\sum q_{\\phi} (z|x)\\left[\\log(p_{\\theta}(z,x;\\theta)) - \\log(p(z)) + \\log(p(z)) - \\log(q_{\\phi}(z|x))\\right] \\\\ &amp;= \\sum q_{\\phi} (z|x)\\left[\\log(p_{\\theta}(x|z)) - \\log\\left(\\frac{q_{\\phi}(z|x)}{p(z)}\\right)\\right] \\\\ &amp;= E_{z \\sim q_{\\phi}(z|x)}\\left[\\log(p_{\\theta}(x|z))\\right] - D_{KL}(q_{\\phi}(z|x) || p(z)) \\end{align} \\] 这里就把我们的目标分成了两项： 第一项是重构项，要求我们尽可能重构数据本身 第二项是正则项，要求我们的后验与先验接近 所以可以看到，它与自动编码器最大的区别在于有第二项，这保证了隐藏变量 \\(z\\) 的分布，从而我们可以从先验中对 \\(z\\) 取样从而进行生成。换句话来说，VAEs是对潜变量进行了正则化的自动编码器，因为我们知道了潜变量 \\(z\\) 的分布形式，所以它能够用于生成。 按照蒙特卡洛抽样（Monte Carlo Sampling），理论上求这个期望需要对每个样本多次采样进行计算，最后平均。但在具体实践中，往往采样一次进行计算就行。 梯度计算细节：重参数化技巧 有一个细节是现在 \\(z\\) 是从 \\(q_\\phi(z|x)∼N(μ_ϕ(x)，diag(\\sigma^{2}_ϕ(x)))\\) 中采样的，但梯度无法经过采样传播到参数 \\(\\phi\\) 。但其实解决方法很简单，对于高斯函数，只需要先从 \\(N(0,I)\\) 中采样 \\(\\epsilon\\) 再计算 \\(z=μ_ϕ(x)+\\epsilon⋅σ_ϕ(x)\\) 即可。 这种技巧也叫做重参数化技巧，其最开始应该是在强化学习中出现的，后面有时间也可以写一写。","categories":[{"name":"Stanford CS236深度生成模型","slug":"Stanford-CS236深度生成模型","permalink":"https://jia040223.github.io/categories/Stanford-CS236%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}],"tags":[{"name":"生成模型","slug":"生成模型","permalink":"https://jia040223.github.io/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}]}],"categories":[{"name":"Stanford CS236深度生成模型","slug":"Stanford-CS236深度生成模型","permalink":"https://jia040223.github.io/categories/Stanford-CS236%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}],"tags":[{"name":"生成模型","slug":"生成模型","permalink":"https://jia040223.github.io/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}]}