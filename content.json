{"meta":{"title":"Serendipity's Blog","subtitle":"","description":"","author":"Serendipity","url":"https://jia040223.github.io","root":"/"},"pages":[{"title":"关于我","date":"2024-10-08T00:34:06.588Z","updated":"2024-10-08T00:34:06.588Z","comments":false,"path":"about/index.html","permalink":"https://jia040223.github.io/about/index.html","excerpt":"","text":"欢迎来到我的博客！这个博客是我个人的学习和探索之旅的记录，我希望通过它分享我的想法和见解。无论是技术、理论还是生活中的点滴，我都会在这里进行更新。如果你对我的内容有任何疑问或建议，欢迎通过 GitHub 与我联系。期待与你的交流与互动！ 个人简介 教育经历 高中：石门县第一中学 本科：中国科学院大学（计算机科学与技术专业） 目前感兴趣的方向 大模型 深度学习 计算机视觉 关于博客 学习笔记 我会持续更新我目前正在学习的内容和笔记，包括但不限于深度学习与大模型，计算机视觉，微积分与线性代数等。 项目地址：repository 科研日志 我会记录一些科研上的心得体会，分享一些我在科研中的经验和教训。 生活记录 我会记录一些好玩的生活片段，同样包括但不限于旅游日志、生活琐事和所悟所想，分享是一种很好的生活调味剂，希望你会喜欢。 联系方式 Email：jiachenghao21@mails.ucas.ac.cn"}],"posts":[{"title":"[Probabilistic Machine Learning]: Fundamentals-Optimization","slug":"Fundamentals-Optimization","date":"2024-10-29T01:41:24.000Z","updated":"2024-10-29T03:11:24.119Z","comments":true,"path":"2024/10/29/Fundamentals-Optimization/","permalink":"https://jia040223.github.io/2024/10/29/Fundamentals-Optimization/","excerpt":"","text":"优化问题通常可以表示为： \\[\\theta^* \\in \\arg\\min_{\\theta \\in \\Theta} L(\\theta)\\] 其中，\\(\\theta^*\\)是优化得到的参数，\\(\\Theta\\)是参数空间，可以是任意维度的实数集合，而\\(L(\\theta)\\)是目标函数或损失函数，它衡量了参数与某种目标之间的距离。 凸优化问题的局部最优解即是全局最优解，而非凸问题可能有多个局部最优解。 一、自动微分（Automatic Differentiation） 自动微分（AD）是计算复杂函数的导数的有效方法，尤其是在深度学习和优化中具有重要应用。AD与数值微分和符号微分不同，它通过跟踪计算图来有效计算导数。 1. 函数微分的基础知识 在介绍自动微分之前，首先需要了解导数的数学基础。 符号表示 给定函数\\(f : \\mathbb{R}^2 \\rightarrow \\mathbb{R}\\)，我们可以写其关于第一个参数的偏导数为\\(\\frac{\\partial f}{\\partial x_1}\\bigg|_{x=a}\\)或者\\(\\frac{\\partial}{\\partial a_1} f(a_1, a_2)\\) 这种命名变量表示法在函数嵌套较多时会变得复杂，因此可以考虑采用函数操作符的表示法。 线性和多线性函数： 定义线性函数\\(F : \\mathbb{R}^n \\overset{\\ell}{\\rightarrow} \\mathbb{R}^m\\)。每个线性映射对应一个矩阵，其列由\\(F[e_1], \\ldots, F[e_n]\\)组成（具体线性代数课已经证明并详细学习了）。 多线性映射定义为： \\[T : \\mathbb{R}^n \\times \\cdots \\times \\mathbb{R}^n \\rightarrow \\mathbb{R}^m \\ \\] 它对应于\\(\\mathbb{R}^{m \\times n \\times \\cdots \\times n}\\)中的一个张量。用\\(T[x_1, \\ldots, x_k] \\in \\mathbb{R}^m\\)表示这样的\\(k\\)-线性映射对向量\\(x_1, \\ldots, x_k \\in \\mathbb{R}^n\\)的作用。 导数算子 对可微分函数\\(f : U \\rightarrow \\mathbb{R}^m\\)，其导数可以表示为： \\[\\partial f : U \\rightarrow (\\mathbb{R}^n \\overset{\\ell}{\\rightarrow} \\mathbb{R}^m)\\] 或者等价地： \\[\\partial f : U \\rightarrow \\mathbb{R}^{m \\times n}\\] 雅可比矩阵表示为所有偏导数组成的矩阵，给定\\(x \\in U\\)，可以表示为： \\[J(x) = \\partial f(x)\\] 雅可比-向量积 (JVP)定义为： \\[(x, v) \\mapsto \\partial f(x)[v]\\] 它表示在点\\(x\\)处，输入扰动\\(v\\)对输出的影响。 向量-雅可比积 (VJP)定义为： \\[(x, u) \\mapsto \\partial f(x)^T[u]\\] 它表示在点\\(x\\)处，输出扰动\\(u\\)对应输入的变化。 高阶导数 如果函数\\(f\\)在其域\\(U\\)内任意可微，则其二阶导数写作： \\[\\partial^2 f : U \\rightarrow (\\mathbb{R}^n \\overset{\\ell}{\\rightarrow} \\mathbb{R}^n \\overset{\\ell}{\\rightarrow} \\mathbb{R}^m)\\] 对于任意高阶导数，可以表示为： \\[\\partial^k f : U \\rightarrow (\\mathbb{R}^n \\overset{\\ell}{\\rightarrow} \\cdots \\overset{\\ell}{\\rightarrow} \\mathbb{R}^m)\\] \\(\\partial^2 f\\)在\\(m=1\\)时对应于海森矩阵。 泰勒级数近似： \\[f(x + v) \\approx f(x) + \\partial f(x)[v] + \\frac{1}{2!} \\partial^2 f(x)[v, v] + \\cdots + \\frac{1}{k!} \\partial^k f(x)[v, \\ldots, v]\\] 多个输入 考虑一个两个参数的函数： \\[g : U \\times V \\rightarrow \\mathbb{R}^m\\] 其中\\(U \\subset \\mathbb{R}^{n_1}​\\)和\\(V \\subset \\mathbb{R}^{n_2}\\)​。 导数函数可以分别表示为： \\[\\partial_1 g : \\mathbb{R}^{n_1} \\times \\mathbb{R}^{n_2} \\rightarrow (\\mathbb{R}^{n_1} \\overset{\\ell}{\\rightarrow} \\mathbb{R}^m)\\] \\[\\partial_2 g : \\mathbb{R}^{n_1} \\times \\mathbb{R}^{n_2} \\rightarrow (\\mathbb{R}^{n_2} \\overset{\\ell}{\\rightarrow} \\mathbb{R}^m)\\] 总导数表示为： \\[\\partial g : \\mathbb{R}^{n_1} \\times \\mathbb{R}^{n_2} \\rightarrow (\\mathbb{R}^{n_1} \\times \\mathbb{R}^{n_2} \\overset{\\ell}{\\rightarrow} \\mathbb{R}^m)\\] 对于点\\((x, y) \\in U \\times V\\)和扰动\\(\\dot{x} \\in \\mathbb{R}^{n_1}，\\dot{y} \\in \\mathbb{R}^{n_2}\\)​，有： \\[\\partial g(x, y)[\\dot{x}, \\dot{y}] = \\partial_1 g(x, y)[\\dot{x}] + \\partial_2 g(x, y)[\\dot{y}]\\] 或者，从矩阵的角度来看： \\[\\partial g(x, y) = \\begin{pmatrix} \\partial_1 g(x, y) &amp; \\partial_2 g(x, y) \\end{pmatrix}\\] 组合和分支 当我们有复合函数\\(f = g \\circ h\\)时，链式法则表明： \\[\\partial f(x) = \\partial g(h(x)) \\circ \\partial h(x)\\] 当存在多个输入时，比如： \\[f(x) = g(a(x), b(x))\\] 我们可以使用链式法则来计算： \\[\\begin{align*} \\partial f(x) &amp;= \\partial g(h(x)) \\circ \\partial h(x) \\\\ &amp;= \\partial_1 g(a(x), b(x)) \\circ \\partial a(x) + \\partial_2 g(a(x), b(x)) \\circ \\partial b(x) \\end{align*}\\] 2. 链法则到计算图 自动微分的目标是计算任意输入函数的导数。对于给定的函数\\(f: U \\subset \\mathbb{R}^n \\to \\mathbb{R}^m\\)和线性化点\\(x \\in U\\)，AD 可以计算： 输入扰动\\(v \\in \\mathbb{R}^n\\)的 JVP：\\(\\partial f(x)[v]\\) 输出扰动\\(u \\in \\mathbb{R}^m\\)的 VJP：\\(\\partial f(x)^T[u]\\) 决定使用哪些函数作为输入，以及如何表示这些函数，是AD设置中最重要的方面之一。 2.1 链式组合和链式法则 这里考虑的函数是基本操作的链式组合。链式组合的导数按照链式法则分解，从而提供了一种便利的函数表示形式。例如，考虑由三个操作依次组成的函数： \\(f = c \\circ b \\circ a\\) 根据链式法则，其导数为： \\[\\partial f(x) = \\partial c(b(a(x))) \\circ \\partial b(a(x)) \\circ \\partial a(x)\\] 针对输入扰动\\(v \\in \\mathbb{R}^n\\)的 JVP 可以表示为： \\[\\partial f(x)[v] = \\partial c(b(a(x))) [\\partial b(a(x)) [\\partial a(x)[v]]]\\] 这个表达式的括号突出了从右到左的评估顺序，这对应于前向模式自动微分。为了计算这个 JVP，我们需要首先计算原始链的前缀： \\[x, \\quad a(x), \\quad b(a(x))\\] 以及相应的偏导数： \\[\\partial a(x), \\quad \\partial b(a(x)), \\quad \\partial c(b(a(x)))\\] 所以，具体的算法如下： 对于输出扰动\\(u\\in \\mathbb{R}^m\\)，VJP 的表达式为： \\[\\partial f(x)^T[u] = \\partial a(x)^T \\partial b(a(x))^T \\partial c(b(a(x)))^T[u]\\] 这里对应于反向模式自动微分。 为了进行 VJP，我们可以首先计算原始链的前缀 \\(x、a(x)、 b(a(x))\\) 然后反向读取它们，具体为： \\[\\partial c(b(a(x)))^T, \\quad \\partial b(a(x))^T, \\quad \\partial a(x)^T\\] 反向模式自动微分在输出为标量时（如深度学习中的损失函数）比前向模式更快。然而，反向模式在向后遍历之前存储所有链的前缀，因此它消耗的内存比前向模式多。 在某些特殊情况下（如每个链操作都是可逆的），可以通过一些方法减少内存需求。还可以通过丢弃某些前缀并在需要时重新计算来在内存与计算之间进行权衡。 2.2 从链到计算图 当原语可以接受多个输入时，我们可以自然地将链扩展为电路（circuits）——即基于原语操作的有向无环图（DAG），也称为计算图。电路会有若干个输入节点，表示函数的参数；也有若干个原语节点，每个节点都标记为某种原语操作。输入节点没有入边（即没有其他节点指向它们），并且每个节点恰好有一条出边，同时图中只有一个汇点（输出节点）。电路的整体功能是从输入节点到汇点的操作组合，每个操作的输出作为其他操作的输入。 对于链的自动微分之所以有效，是因为导数沿链可以分解，这得益于著名的链式法则。当我们从链转向有向无环图时，我们是否需要某种“图法则”来沿电路结构分解计算呢？电路引入了两个新特性：入度（fan-in）和出度（fan-out）。 入度（fan-in）：当一个原语操作接受多个参数时，会出现入度。在第1小节中，我们知道多个参数可以视为一个参数，因此链式法则适用。 出度（fan-out）：出度在反向模式微分中需要稍微更多的关注。 示例说明 考虑上图的电路。操作\\(a\\)在拓扑上位于\\(b\\)和\\(c\\)之前，并且有一条出边指向它们。我们可以将\\(a\\)从\\(\\{b, c\\}\\)中切割出来，生成两个新电路，如上图(b)所示。第一个电路对应于\\(a\\)，第二个电路的计算为： \\[f_{\\{b,c\\}}(x_1, x_2) = c(x_1, b(x_2))\\] 我们可以通过一个名为\\(dup\\)的函数来恢复完整的函数\\(f\\)： \\[dup(x) = (x, x) \\equiv \\begin{pmatrix} I \\\\ I \\end{pmatrix} x\\] 因此，函数\\(f\\)可以写为链式组合： \\[f = f_{\\{b,c\\}} \\circ dup \\circ a\\] 电路\\(f_{\\{b,c\\}}\\)不包含出度，其导数与\\(b\\)、\\(c\\)及其导数的关系，都是通过链式法则实现的。同时，根据链式法则： \\[\\begin{align*} \\partial f(x) &amp;= \\partial f_{\\{b,c\\}}(dup(a(x))) \\circ \\partial dup(a(x)) \\circ \\partial a(x) \\\\&amp;= \\partial f_{\\{b,c\\}}(a(x), a(x)) \\circ \\begin{pmatrix} I \\\\ I \\end{pmatrix} \\circ \\partial a(x) \\end{align*}\\] 以上表达式表明，可以通过从右到左的评估来计算\\(f\\)的 JVP。这类似于链情况中建议的 JVP 计算，但中间有一个复制的操作\\(\\begin{pmatrix} I \\\\ I \\end{pmatrix}\\)，即\\(dup\\)的雅可比矩阵。 对\\(f\\)在\\(x\\)处的导数进行转置： \\[\\partial f(x)^T = \\partial a(x)^T \\circ (I \\ \\ I) \\circ \\partial f_{\\{b,c\\}}(a(x), a(x))^T\\] 考虑从右到左的评估，这同样类似于链情况中建议的 VJP 计算，但中间有一个求和操作\\((I\\ \\ I)\\)，即\\(dup\\)的转置雅可比矩阵。 二、 随机优化 这节我们主要考虑形式为\\(L(\\theta) = \\mathbb{E}_{q_\\theta(z)} \\left[ \\tilde{L}(\\theta, z) \\right]\\)的随机目标优化，其中\\(\\theta\\)是我们要优化的参数，\\(z\\)是随机变量，例如外部噪声，采样数据或者隐变量等。 1. 随机梯度下降 (SGD) 假设我们能够计算目标函数梯度的无偏估计\\(g_t\\)​ ，即： \\[\\mathbb{E}[g_t] = \\nabla_\\theta L(\\theta) |_{\\theta_t}\\] 则我们可以在梯度下降过程中使用它： \\[\\theta_{t+1} = \\theta_t - \\eta_t g_t\\] 其中\\(\\eta_t\\)是学习率或步长。这被称为随机梯度下降（SGD）。SGD 收敛速度可能较慢，因为它依赖于梯度的随机估计。目前已经提出多种方法以减少每一步生成的参数估计的方差，从而加速收敛。 在使用 SGD 时，我们需要选择学习率来更好地收敛。我们可以使用学习率调度，而不是选择单一的常数学习率，逐步调整步长。理论上，SGD 收敛的充分条件是学习率调度满足 Robbins-Monro 条件： \\[\\eta_t \\to 0, \\quad \\sum_{t=1}^\\infty \\eta_t^2 &lt; \\infty, \\quad \\sum_{t=1}^\\infty \\eta_t = \\infty\\] 一些常见的学习率调度示例如下： 分段常数：\\(\\eta_t = \\eta_i \\quad \\text{if } t_i \\leq t \\leq t_{i+1}\\) 指数衰减：\\(\\eta_t = \\eta_0 e^{-\\lambda t}\\) 多项式衰减：\\(\\eta_t = \\eta_0 (\\beta t + 1)^{-\\alpha}\\) 在许多情况下，梯度的大小在各个维度之间可能差异很大，导致损失面在某些方向上陡峭而在其他方向上较为平坦，类似于一个山谷的底部。在这种情况下，可以用条件矩阵\\(C_t\\)来缩放梯度向量，从而实现更快的收敛： \\[\\theta_{t+1} = \\theta_t - \\eta_t C_t g_t\\] 这被称为预条件 SGD。 2. SGD 用于优化有限和目标 在最简单的情况下，计算期望的分布\\(q_\\theta(z)\\)不依赖于正在优化的参数\\(\\theta\\)（即噪声或者数据与参数无关）。在这种情况下，我们可以将梯度推入期望算子内部，然后通过 Monte Carlo 采样来近似梯度： \\[\\begin{align*} \\nabla_\\theta L(\\theta) &amp;= \\nabla_\\theta \\mathbb{E}_{q(z)} \\left[ \\tilde{L}(\\theta, z) \\right] \\\\ &amp;= \\mathbb{E}_{q(z)} \\left[ \\nabla_\\theta \\tilde{L}(\\theta, z) \\right] \\\\ &amp;\\approx \\frac{1}{S} \\sum_{s=1}^{S} \\nabla_\\theta \\tilde{L}(\\theta, z_s) \\end{align*}\\] 其中\\(S\\)是采样数量。例如，考虑经验风险最小化（ERM）的问题，要求最小化： \\[L(\\theta) = \\frac{1}{N} \\sum_{n=1}^{N} \\tilde{L}(\\theta, z_n) = \\frac{1}{N} \\sum_{n=1}^{N} \\ell(y_n, f(x_n; \\theta))\\] 其中\\(z_n = (x_n, y_n)\\)是第\\(n\\)个标记示例，\\(f\\)是预测函数。这种目标称为有限和目标。我们可以将其写为关于经验分布\\(p_D(x, y)\\)的期望损失： \\[L(\\theta) = \\mathbb{E}_{p_D(z)} \\left[ \\tilde{L}(\\theta, z) \\right]\\] 由于期望依赖于数据，而不是参数，我们可以通过在每次迭代中使用来自完整数据集\\(D\\)的\\(B = |B|\\)个数据点的小批量来近似梯度： \\[g_t = \\nabla L(\\theta_t) = \\frac{1}{B} \\sum_{n \\in B} \\nabla \\ell(y_n, f(x_n; \\theta))\\] 这些带噪声的梯度可以传递给 SGD。当数据集很大时，这种方法比全批梯度下降快得多，因为它不需要在更新模型之前评估所有示例的损失 。 3. SGD 用于优化分布的参数 现在假设随机性依赖于我们正在优化的参数。例如，在强化学习中，\\(z\\)可能是从随机策略\\(q_\\theta\\)​ 中采样的动作，或者在随机变分推理中，\\(z\\)可能是从推理网络\\(q_\\theta\\)​ 中采样的潜变量。 在这种情况下，梯度为： \\[\\begin{align*} \\nabla_\\theta \\mathbb{E}_{q_\\theta(z)}\\left[\\tilde{L}(\\theta, z)\\right] &amp;= \\nabla_\\theta \\int \\tilde{L}(\\theta, z) q_\\theta(z) \\, dz \\\\ &amp;= \\int \\nabla_\\theta \\tilde{L}(\\theta, z) q_\\theta(z) \\, dz \\\\ &amp;= \\int \\left[\\nabla_\\theta \\tilde{L}(\\theta, z)\\right] q_\\theta(z) \\, dz + \\int \\tilde{L}(\\theta, z) \\left[\\nabla_\\theta q_\\theta(z)\\right] \\, dz \\end{align*}\\] 第一个项可以通过 Monte Carlo 采样近似： \\[\\int \\left[ \\nabla_\\theta \\tilde{L}(\\theta, z) \\right] q_\\theta(z) dz \\approx \\frac{1}{S} \\sum_{s=1}^{S} \\nabla_\\theta \\tilde{L}(\\theta, z_s)\\] 其中\\(z_s \\sim q_\\theta\\)​。 现在考虑第二项，注意，如果\\(\\tilde{L}\\)与\\(\\theta\\)无关，则此项消失，但如果相关，则该项涉及分布本身的梯度： \\[I \\equiv \\int \\tilde{L}(\\theta, z) [\\nabla_\\theta q_\\theta(z)] dz\\] 我们不能再使用简单的 Monte Carlo 采样来近似这个积分。然而，还有其他各种方法可以近似这个积分。将在后面简要描述两种主要方法。 4 分数函数估计器（REINFORCE） 分数函数是对数概率分布的梯度，其表达式为： \\[\\nabla_\\theta q_\\theta(z) = q_\\theta(z) \\nabla_\\theta \\log q_\\theta(z)\\] 利用上述分数函数定义，可以将积分式的目标函数梯度表达式重新写为： \\[\\begin{align*} I &amp;= \\int \\tilde{L}(\\theta, z) [q_\\theta(z) \\nabla_\\theta \\log q_\\theta(z)] \\, dz \\\\ &amp;= \\mathbb{E}_{q_\\theta(z)}\\left[\\tilde{L}(\\theta, z) \\nabla_\\theta \\log q_\\theta(z)\\right] \\end{align*}\\] 这里，\\(I\\)表示我们要计算的梯度期望。 通过蒙特卡洛方法，可以对上述积分进行近似： \\[I \\approx \\frac{1}{S} \\sum_{s=1}^{S} \\tilde{L}(\\theta, z_s) \\nabla_\\theta \\log q_\\theta(z_s)\\] 其中\\(z_s \\sim q_\\theta\\)​。 为了降低分数函数估计的方差，可以使用控制变量的方法，将\\(\\tilde{L}(\\theta, z)\\)替换为： \\[\\hat{\\tilde{L}}(\\theta, z) = \\tilde{L}(\\theta, z) - c \\left(b(\\theta, z) - \\mathbb{E}[b(\\theta, z)]\\right)\\] 这里，\\(b(\\theta, z)\\)是与\\(\\tilde{L}(\\theta, z)\\)相关的基线函数，\\(c &gt; 0\\)是一个系数。使用此新的估计\\(\\hat{\\tilde{L}}\\)计算的梯度估计仍然是无偏的，但具有较低的方差。 当\\(q_\\theta(z)\\)是离散分布时，目标函数为： \\[L(\\theta) = \\sum_{z} \\tilde{L}(\\theta, z) q_\\theta(z)\\] 梯度计算为： \\[\\nabla_\\theta L(\\theta) = \\sum_{z} \\tilde{L}(\\theta, z) \\nabla_\\theta q_\\theta(z)\\] 如果\\(z\\)可以取指数级多的值，直接计算可能不可行。可以将和分为两部分：一部分\\(S_1\\)​ 是高概率值的小集合，另一部分\\(S_2\\)​ 是所有其他值的大集合。梯度的计算可以表达为： \\[\\nabla_\\theta L(\\theta) = \\sum_{z \\in S_1} \\tilde{L}(\\theta, z) \\nabla_\\theta q_\\theta(z) + \\mathbb{E}_{q_\\theta(z|z \\in S_2)}\\left[\\tilde{L}(\\theta, z) \\nabla_\\theta \\log q_\\theta(z)\\right]\\] 对于第二项的计算，可以使用拒绝采样的方法，用来自\\(q_\\theta(z)\\)的样本进行近似。这种方法就是拉奥-布莱克威尔化(Rao-Blackwellization)，能够有效减少方差。 5. 重参数化技巧 重参数化技巧用于降低得分函数估计器的方差，前提是\\(\\tilde{L}(\\theta, z)\\)对\\(z\\)可微，并且可以通过先从一个与\\(\\theta\\)无关的噪声分布\\(q_0\\)中采样\\(\\epsilon\\)，然后通过确定性和可微的函数\\(z = g(\\theta, \\epsilon)\\)转换得到\\(z\\)。 例如，可以从标准正态分布中采样： \\[z = g(\\theta, \\epsilon) = \\mu + \\sigma \\epsilon\\] 其中\\(\\theta = (\\mu, \\sigma)\\)。 我们可以将目标函数重写为： \\[L(\\theta) = \\mathbb{E}_{q_\\theta(z)}\\left[\\tilde{L}(\\theta, z)\\right] = \\mathbb{E}_{q_0(\\epsilon)}\\left[\\tilde{L}(\\theta, g(\\theta, \\epsilon))\\right]\\] 由于\\(q_0(\\epsilon)\\)与\\(\\theta\\)无关，可以将梯度算子推入期望中，从而获得： \\[\\nabla_\\theta L(\\theta) = \\mathbb{E}_{q_0(\\epsilon)}\\left[\\nabla_\\theta \\tilde{L}(\\theta, g(\\theta, \\epsilon))\\right]\\] 通过蒙特卡洛方法进行近似： \\[\\nabla_\\theta L(\\theta) \\approx \\frac{1}{S} \\sum_{s=1}^{S} \\nabla_\\theta \\tilde{L}(\\theta, g(\\theta, \\epsilon_s))\\] 当\\(\\tilde{L}\\)依赖于\\(\\theta\\)和噪声样本\\(z\\)时，需要使用总导数计算梯度。对于形式为\\(\\tilde{L}(\\theta_1, \\ldots, \\theta_d, z_1(\\theta), \\ldots, z_d(\\theta))\\)的函数，其对\\(\\theta_i\\)的总导数为： \\[\\frac{\\partial \\tilde{L}}{\\partial \\theta_i}\\bigg|_{TD} = \\frac{\\partial \\tilde{L}}{\\partial \\theta_i} + \\sum_j \\frac{\\partial \\tilde{L}}{\\partial z_j} \\frac{\\partial z_j}{\\partial \\theta_i}\\] 进而得到： \\[\\nabla_\\theta \\tilde{L}(\\theta, z)\\bigg|_{TD} = \\nabla_z \\tilde{L}(\\theta, z) J + \\nabla_\\theta \\tilde{L}(\\theta, z)\\] 其中，\\(J\\)为噪声变换的雅可比矩阵：\\(J = \\frac{\\partial z}{\\partial \\theta}\\)，这也是一种计算方法。 特别的，在变分推断中，ELBO（证据下界）目标形式为： \\[\\tilde{L}(\\theta, z) = \\log p(z, x) - \\log q(z|\\theta)\\] 梯度为： \\[\\nabla_\\theta \\tilde{L}(\\theta, z) = \\nabla_z \\left[\\log p(z, x) - \\log q(z|\\theta)\\right] J - \\nabla_\\theta \\log q(z|\\theta)\\] 这里第一项是通过生成样本\\(z\\)对目标的间接影响，第二项是\\(\\theta\\)对目标的直接影响。第二项在期望上为零，但在有限样本中可能非零。为了减少方差，可以使用“断开”的\\(\\theta&#39;\\)替代\\(\\theta\\)来计算： \\[g = \\nabla_\\theta \\left[\\log p(z, x) - \\log q(z|\\theta&#39;)\\right]\\] 其它项如下： \\[\\epsilon \\sim q_0(\\epsilon) \\\\ z = g(\\epsilon, \\theta) \\\\θ′=stop-gradient(θ)\\] 这被叫做sticking the landing or STL estimator。需要指出的，这不一定比不忽略第二项的效果更好，实际中可以使用这两者的加权平均。 6. Gumbel Softmax技巧 在处理离散变量时，传统的重参数化技巧不可用。但是，我们可以将离散变量放松为连续变量，从而实现类似的效果。 考虑一个包含\\(K\\)位的 one-hot 向量\\(d\\)，其中\\(d_k \\in \\{0, 1\\}\\)且\\(\\sum_{k=1}^{K} d_k = 1\\)。这可以用于表示一个\\(K\\)-元的分类变量。设\\(P(d) = \\text{Cat}(d|\\pi)\\)，其中\\(\\pi_k = P(d_k = 1)\\)，满足\\(0 \\leq \\pi_k \\leq 1\\)。我们也可以将分布参数化为\\((\\alpha_1, \\ldots, \\alpha_K)\\)，其中\\(\\pi_k = \\frac{\\alpha_k}{\\sum_{k&#39; = 1}^{K} \\alpha_{k&#39;}}\\)​​。我们记作\\(d \\sim \\text{Cat}(d|\\alpha)\\)。 我们可以通过以下公式从该分布计算 one-hot 向量\\(d\\)： \\[d = \\text{onehot}\\left(\\arg\\max_k\\left[\\epsilon_k + \\log \\alpha_k\\right]\\right)\\] 其中\\(\\epsilon_k \\sim \\text{Gumbel}(0, 1)\\)，从 Gumbel 分布中采样。可以通过先采样\\(u_k \\sim \\text{Uniform}(0, 1)\\)，然后计算\\(\\epsilon_k = -\\log(-\\log(u_k))\\)来得到。Gumbel-Max Trick 提供了分类分布的重参数化表示。遗憾的是，\\(argmax\\)的导数在边界处未定义，这使得梯度计算变得复杂。 为了克服这个问题，我们可以将\\(argmax\\)替换为\\(softmax\\)，并将离散的 one-hot 向量\\(d\\)替换为连续放松\\(x \\in \\Delta^{K-1}\\)，其中 \\[\\Delta^{K-1} = \\{ x \\in \\mathbb{R}^K : x_k \\in [0, 1], \\sum_{k=1}^{K} x_k = 1 \\}\\] 这样我们可以写出： \\[x_k = \\frac{\\exp\\left(\\frac{\\log \\alpha_k + \\epsilon_k}{\\tau}\\right)}{\\sum_{k&#39; = 1}^{K} \\exp\\left(\\frac{\\log \\alpha_{k&#39;} + \\epsilon_{k&#39;}}{\\tau}\\right)}\\] 其中\\(\\tau &gt; 0\\)是温度参数。随着\\(\\tau \\to 0\\)，这个分布平滑地接近离散分布。通过将\\(f(d)\\)替换为\\(f(x)\\)，我们可以对\\(x\\)进行重参数化梯度计算。这允许我们在进行优化时有效地处理离散变量。 7. 直通估计器 (Straight-Through Estimator) 直通估计器主要用于近似量化信号的梯度。考虑一个阈值函数： \\[f(x) = \\begin{cases} 1 &amp; \\text{if } x &gt; 0 \\\\ 0 &amp; \\text{if } x \\leq 0 \\end{cases}\\] 这个函数没有定义良好的梯度，因为它是一个分段常数函数。 直通估计器的基本思想是，在反向传播过程中将\\(g(x) = f&#39;(x)\\)的计算替换为\\(g(x) = x\\)。这样做的好处是可以在没有梯度的情况下为网络的训练提供一个近似值。 在实际应用中，我们有时会用硬双曲正切函数替代\\(g(x) = x\\)，其定义为： \\[\\text{HardTanh}(x) = \\begin{cases} x &amp; \\text{if } -1 \\leq x \\leq 1 \\\\ 1 &amp; \\text{if } x &gt; 1 \\\\ -1 &amp; \\text{if } x &lt; -1 \\end{cases}\\] 这种替代确保了反向传播的梯度不会变得过大，有助于稳定训练过程。 三、自然梯度下降 自然梯度下降（Natural Gradient Descent，NGD）是一种优化方法，主要用于优化条件概率分布\\(p_\\theta(y|x)\\)的参数。与常规的梯度下降方法不同，NGD 通过测量引发的分布之间的距离来计算参数更新，而不是直接使用参数值的距离。这种方法特别适用于处理参数间相互作用较强的情况，比如在高维空间中的概率分布。 以两个高斯分布为例，\\(p_\\theta = p(y|\\mu, \\sigma)\\)和\\(p_{\\theta&#39;} = p(y|\\mu&#39;, \\sigma&#39;)\\)。如果直接计算参数向量之间的欧几里得距离： \\[||\\theta - \\theta&#39; ||^2 = (\\mu - \\mu&#39;)^2 + (\\sigma - \\sigma&#39;)^2\\] 并不能反映出分布的真实变化。实际上，预测分布的形式为： \\[\\exp\\left(-\\frac{1}{2\\sigma^2}(y - \\mu)^2\\right)\\] 这意味着，均值\\(\\mu\\)对于不同的方差\\(\\sigma\\)的影响大小会有区别。下图 (a-b) 说明了当方差较小和较大时，均值变化对分布的影响是不同的。明显方差小的 时候均值的影响会更大 1. 自然梯度的定义 NGD 的关键在于用 KL 散度来度量两个概率分布之间的距离。根据之前的推导，我们有： \\[D_{KL}(p_\\theta(y|x) \\parallel p_{\\theta+\\delta}(y|x)) \\approx \\frac{1}{2} \\delta^T F_x \\delta\\] 其中\\(F_x\\)​ 是 Fisher 信息矩阵 (FIM)，定义为： \\[F_x(\\theta) = -\\mathbb{E}_{p_\\theta(y|x)}\\left[\\nabla^2 \\log p_\\theta(y|x)\\right] = \\mathbb{E}_{p_\\theta(y|x)}\\left[(\\nabla \\log p_\\theta(y|x))(\\nabla \\log p_\\theta(y|x))^T\\right]\\] 我们可以使用平均 FIM 来计算当前和更新分布之间的平均 KL 散度： \\[F(\\theta) = \\mathbb{E}_{p_D(x)}\\left[F_x(\\theta)\\right]\\] 自然梯度下降通过使用 FIM 的逆作为预条件矩阵来进行参数更新： \\[\\theta_{t+1} = \\theta_t - \\eta_t F(\\theta_t)^{-1} g_t\\] 其中\\(g_t\\)​ 是常规梯度。 因此，定义自然梯度为： \\[F^{-1} g_t = F^{-1} \\nabla L(\\theta_t) \\equiv \\tilde{\\nabla} L(\\theta_t)\\] 2. 自然梯度下降的解释 标准的梯度下降可以被理解为在目标函数的线性近似下进行优化，同时对参数变化的\\(\\ell_2\\)​ 范数施加惩罚。如果设定\\(\\theta_{t+1} = \\theta_t + \\delta\\)，我们优化的目标为： \\[M_t(\\delta) = L(\\theta_t) + g_t^T \\delta + \\eta \\frac{||\\delta||^2}{2}\\] 其中\\(g_t\\)​ 是在点\\(\\theta_t\\)处的梯度，\\(\\eta\\)是步长。 现在我们用基于 Fisher 信息矩阵 (FIM) 的距离替换平方距离，即： \\[||\\delta||^2_F = \\delta^T F \\delta\\] 这在whitened coordinate system中是等价的，即使用\\(\\phi = F^{1/2} \\theta\\)进行变换。我们可以得到： \\[||\\phi_{t+1} - \\phi_t||^2 = ||F^{1/2} (\\theta_t + \\delta) - F^{1/2} \\theta_t||^2 = ||F^{1/2} \\delta||^2 = ||\\delta||^2_F\\] 则新的优化目标变为： \\[M_t(\\delta) = L(\\theta_t) + g_t^T \\delta + \\eta \\delta^T F \\delta\\] 通过求解\\(\\nabla_\\delta M_t(\\delta) = 0\\)，我们可以得到更新公式： \\[\\delta_t = -\\eta F^{-1} g_t\\] 这与自然梯度的方向一致。因此，我们可以将 NGD 视为一种trust region method，其中使用了目标函数的一阶近似，并在约束中使用 FIM 距离。 在上述推导中，我们假设 F 是常数矩阵。在大多数问题中，FIM 会在空间中的每个点上变化，这意味着我们是在一个黎曼流形的曲面上进行优化。对于某些模型，尽管我们还是使用目标函数的一阶近似，我们仍然可以高效地计算 FIM，捕捉曲率信息。 如果\\(p(y|x, \\theta)\\)是一个指数族分布，其自然参数由\\(\\eta = f(x, \\theta)\\)计算，那么可以证明 NGD 实际上与广义高斯-牛顿（GGN）方法是相同的 。这意味着 NGD 在某些情况下可以看作是高斯-牛顿优化方法的应用。 3. 自然梯度下降 (NGD) 的优点 使用 FIM 作为预处理矩阵的一个主要优点是 FIM 总是正定的，而海森矩阵（Hessian）在鞍点处可能具有负特征值。这在高维空间中很常见，可能导致优化过程的不稳定。 同时FIM 可以通过小批量数据简单地在线近似，因为它是关于梯度向量外积的期望： \\[F_x(\\theta) = \\mathbb{E}_{p_\\theta(y|x)} \\left[ (\\nabla \\log p_\\theta(y|x))(\\nabla \\log p_\\theta(y|x))^T \\right]\\] 与之相比，研究显示，基于海森矩阵的方法对小批量数据引入的噪声非常敏感。 NGD 更新参数的方式侧重于对预测最重要的部分，这使得在不确定区域中可以采取更大的步骤，从而帮助避免陷入平坦区域或鞍点。 以一个二维高斯为例子： 在梯度下降过程中，NGD能够更快地收敛到全局最优解，相较于普通的梯度下降方法，NGD 在参数空间中的运动更加直接和高效。同时 NGD 对概率分布的参数化方式不敏感，因此即使在复杂的概率模型中（例如深度神经网络），也能保持同样的收敛效果。 一个具体的对比如下： 4. 近似自然梯度 NGD 的主要缺点是计算 FIM （及其逆）的计算成本较高。为了加速计算，一些方法对 FIM 的形式做出假设，以便高效地反转。例如： 有学者提出了 KFAC 方法（Kronecker Factored Approximate Curvature），该方法将深度神经网络的 FIM 近似为一个块对角矩阵，每个块是两个小矩阵的 Kronecker 积。这种方法在监督学习和强化学习中表现良好，并且已被证明在过参数化情况下能够收敛到 DNN 的全局最优解。 另一种简化方法是使用经验分布来近似 FIM。具体定义如下： \\[\\begin{align*} F &amp;= \\mathbb{E}_{p_\\theta(x,y)} \\left[ \\nabla \\log p(y|x, \\theta) \\nabla \\log p(y|x, \\theta)^T \\right] \\\\ &amp;\\approx \\mathbb{E}_{p_D(x,y)} \\left[ \\nabla \\log p(y|x, \\theta) \\nabla \\log p(y|x, \\theta)^T \\right] \\\\ &amp;= \\frac{1}{|D|} \\sum_{(x,y) \\in D} \\nabla \\log p(y|x, \\theta) \\nabla \\log p(y|x, \\theta)^T \\end{align*}\\] 这种近似方法易于计算，但在平坦区域，经验 Fisher 可能变得奇异，从而导致算法陷入停滞。 另一种策略是精确计算 F，但使用截断共轭梯度（CG）方法近似计算\\(F^{-1}g\\)，其中每一步 CG 使用高效的海森-向量乘法方法。但这种方法可能较慢，因为进行单次参数更新需要许多 CG 迭代。 5. 自然梯度对于指数族的应用 假设损失函数\\(L\\)具有以下形式的期望损失： \\[L(\\mu) = \\mathbb{E}_{q_\\mu(z)}\\left[\\tilde{L}(z)\\right]\\] 其中\\(q_\\mu(z)\\)是具有矩参数\\(\\mu\\)的指数族分布。 事实证明，相对于矩参数的梯度与自然参数\\(\\lambda\\)的自然梯度是相同的。这是通过链式法则得出的： \\[\\frac{d}{d\\lambda} L(\\lambda) = \\frac{d\\mu}{d\\lambda} \\frac{d}{d\\mu} L(\\mu) = F(\\lambda) \\nabla_\\mu L(\\mu)\\] 其中\\(L(\\mu) = L(\\lambda(\\mu))\\)，并且： \\[F(\\lambda) = \\nabla_\\lambda \\mu(\\lambda) = \\nabla^2_\\lambda A(\\lambda)\\] 因此， \\[\\tilde{\\nabla}_{\\lambda} L(\\lambda) = F(\\lambda)^{-1} \\nabla_\\lambda L(\\lambda) = \\nabla_\\mu L(\\mu)\\] 接下来需要计算相对于矩参数的标注梯度。具体如何进行计算将取决于\\(q\\)的形式以及\\(L(\\lambda)\\)的形式。 5.1 高斯情况下的解析计算 假设分布\\(q(z) = \\mathcal{N}(z|m, V)\\)，我们推导如何计算与矩参数（moment parameters）相关的自然梯度。 根据Probability中对高斯分布的介绍，我们有 自然参数：\\(\\lambda^{(1)} = V^{-1}m, \\quad \\lambda^{(2)} = -\\frac{1}{2} V^{-1}\\) 矩参数：\\(\\mu^{(1)} = m, \\quad \\mu^{(2)} = V + mm^T\\) 通过链式法则计算梯度： 对于矩参数的梯度，我们有： \\[\\begin{align*} &amp;\\frac{\\partial L}{\\partial \\mu^{(1)}} = \\frac{\\partial L}{\\partial m} \\frac{\\partial m}{\\partial \\mu^{(1)}} + \\frac{\\partial L}{\\partial v} \\frac{\\partial v}{\\partial \\mu^{(1)}} = \\frac{\\partial L}{\\partial m} - 2 \\frac{\\partial L}{\\partial v} m \\\\ &amp;\\frac{\\partial L}{\\partial \\mu^{(2)}} = \\frac{\\partial L}{\\partial m} \\frac{\\partial m}{\\partial \\mu^{(2)}} + \\frac{\\partial L}{\\partial v} \\frac{\\partial v}{\\partial \\mu^{(2)}} = \\frac{\\partial L}{\\partial v} \\end{align*}\\] 通过 Bonnet 定理和 Price 定理，我们可以得到： \\[\\frac{\\partial}{\\partial m_i} \\mathbb{E}\\left[ \\tilde{L}(z) \\right] = \\mathbb{E}\\left[ \\frac{\\partial}{\\partial \\theta_i} \\tilde{L}(z) \\right] \\\\\\frac{\\partial}{\\partial V_{ij}} \\mathbb{E}\\left[ \\tilde{L}(z) \\right] = c_{ij} \\mathbb{E}\\left[ \\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} \\tilde{L}(z) \\right]\\] 多变量情况下的结果： \\[\\begin{align*} \\nabla_{\\mu^{(1)}} E_q(z) \\left[ \\tilde{L}(z) \\right] &amp;= \\nabla_m E_q(z) \\left[ \\tilde{L}(z) \\right] - 2 \\nabla_V E_q(z) \\left[ \\tilde{L}(z) \\right] m \\\\ &amp;= E_q(z) \\left[ \\nabla_z \\tilde{L}(z) \\right] - E_q(z) \\left[ \\nabla^2_z \\tilde{L}(z) \\right] m \\\\ \\nabla_{\\mu^{(2)}} E_q(z) \\left[ \\tilde{L}(z) \\right] &amp;= \\nabla_V E_q(z) \\left[ \\tilde{L}(z) \\right] \\\\ &amp;= \\frac{1}{2} E_q(z) \\left[ \\nabla^2_z \\tilde{L}(z) \\right] \\end{align*}\\] 5.2 一般情况下的随机近似 在一般情况下，解析计算自然梯度可能很困难。可以使用蒙特卡洛近似。 期望损失的定义： \\[L(\\mu) = \\mathbb{E}_{q_\\mu(z)}[\\tilde{L}(z)]\\] 自然梯度的表达： \\[\\nabla_\\mu L(\\mu) = F(\\lambda)^{-1} \\nabla_\\lambda L(\\lambda)\\] 期望的近似： \\[\\begin{align*} F(\\lambda) &amp;= \\nabla_\\lambda \\mu(\\lambda) = \\nabla_\\lambda E_{q_\\lambda}(z) \\left[ T(z) \\right] \\\\ \\nabla_\\lambda L(\\lambda) &amp;= \\nabla_\\lambda E_{q_\\lambda}(z) \\left[ \\tilde{L}(z) \\right] \\end{align*}\\] 如果\\(q\\)是可重参数化的，可以使用重参数化技巧将梯度推入期望算子内，从而对样本\\(z\\)进行采样并计算梯度。 5.3 熵的自然梯度 自然梯度\\(\\tilde{\\nabla}_{\\lambda} H(\\lambda)\\)表示指数族分布的熵\\(H(\\lambda)\\)相对于自然参数\\(\\lambda\\)的梯度，其公式为： \\[\\tilde{\\nabla}_{\\lambda} H(\\lambda) = -\\nabla_\\mu \\mathbb{E}_{q_\\mu(z)}[\\log q(z)]\\] 我们可以将\\(q(z)\\)的对数表示为： \\[\\log q(z) = \\log h(z) + T(z)^T \\lambda - A(\\lambda)\\] 期望的梯度 由于\\(E[T(z)] = \\mu\\)，我们可以对\\(\\log q(z)\\)求\\(\\mu\\)的梯度： \\[\\nabla_\\mu E_{q_\\mu(z)}[\\log q(z)] = \\nabla_\\mu E_{q(z)}[\\log h(z)] + \\nabla_\\mu \\mu^T \\lambda(\\mu) - \\nabla_\\mu A(\\lambda)\\] 因为\\(\\lambda\\)是\\(\\mu\\)的函数，我们有： \\[\\nabla_\\mu \\mu^T \\lambda = \\lambda + (\\nabla_\\mu \\lambda)^T \\mu = \\lambda + (F^{-1}_\\lambda \\nabla_\\lambda \\lambda)^T \\mu = \\lambda + F^{-1}_\\lambda \\mu\\] 这里\\(F(\\lambda)\\)是Fisher信息矩阵。由\\(\\mu = \\nabla_\\lambda A(\\lambda)\\)可得： \\[\\nabla_\\mu A(\\lambda) = F^{-1}_\\lambda \\nabla_\\lambda A(\\lambda) = F^{-1}_\\lambda \\mu\\] 将以上结果代入，我们得到： \\[-\\nabla_\\mu \\mathbb{E}_{q_\\mu(z)}[\\log q(z)] = -\\nabla_\\mu \\mathbb{E}_q[\\log h(z)] - \\lambda\\] 如果我们假设\\(h(z)\\)是常数（这种情况在许多实际应用中成立），则得到： \\[\\tilde{\\nabla}_{\\lambda} H(\\lambda) = -\\lambda\\] 四、边界优化（MM）算法 边界优化（Bound optimization）或者称为 MM（Majorize-Minimize）算法该方法主要通过构造一个次级函数\\(Q(\\theta, \\theta^t)\\)，使其成为目标函数\\(\\ell(\\theta)\\)的下界，来最大化目标函数\\(\\ell(\\theta)\\)。这种方法在许多应用中都非常有效，例如期望最大化（EM）算法和一些聚类算法。 1. 一般算法 我们的目标是最大化某个函数\\(\\ell(\\theta)\\)，MM算法的基本思路是构造一个代理函数\\(Q(\\theta, \\theta^t)\\)，使其满足以下条件 \\(Q(\\theta, \\theta^t) \\leq \\ell(\\theta)\\) \\(Q(\\theta^t, \\theta^t) = \\ell(\\theta^t)\\) 如果满足这些条件，我们称\\(Q\\)小于等于\\(\\ell\\)。然后在每一步进行如下更新： \\[\\theta^{t+1} = \\arg\\max_\\theta Q(\\theta, \\theta^t)\\] 这一过程保证了原始目标函数的单调增加： \\[\\ell(\\theta^{t+1}) \\geq Q(\\theta^{t+1}, \\theta^t) \\geq Q(\\theta^t, \\theta^t) = \\ell(\\theta^t)\\] 这保证了每一步都会提升目标函数的值。 2 示例：逻辑回归 在逻辑回归中，如果我们希望最大化的函数\\(\\ell(\\theta)\\)是一个凹函数，可以通过对其海森矩阵（Hessian）进行界定来构造有效的下界。即寻找一个负定矩阵\\(B\\)使得\\(H(\\theta) \\succ B\\) 通过泰勒展开，可以证明： \\[\\ell(\\theta) \\geq \\ell(\\theta^t) + (\\theta - \\theta^t)^T g(\\theta^t) + \\frac{1}{2} (\\theta - \\theta^t)^T B (\\theta - \\theta^t)\\] 其中\\(g(\\theta^t) = \\nabla \\ell(\\theta^t)\\)。 因此，可以构造以下有效的下界（这里的等于是泰勒展开前两项意义下的相等）： \\[Q(\\theta, \\theta^t) = \\theta^T (g(\\theta^t) - B\\theta^t) + \\frac{1}{2} \\theta^T B \\theta\\] 对应的更新规则变为： \\[\\theta^{t+1} = \\theta^t - B^{-1} g(\\theta^t)\\] 这类似于牛顿法的更新，但使用的是固定的矩阵\\(B\\)，而不是每次迭代时变化的海森矩阵\\(H(\\theta^t)\\)。 2.1 应用示例：多类逻辑回归 以多类逻辑回归为例，假设\\(p(y_n = c | x_n, w)\\)表示样本\\(n\\)属于类\\(c\\)的概率，公式为： \\[p(y_n = c|x_n, w) = \\frac{e^{w_c^T x_n}}{\\sum_{i=1}^C e^{w_i^T x_n}}\\] 由于归一化条件，我们可以只学习\\(C-1\\)个\\(w_i\\)。参数\\(\\theta\\)对应权重矩阵\\(w\\)。 逻辑回归的对数似然可以表示为： \\[\\ell(w) = \\sum_{n=1}^N \\left( \\sum_{c=1}^{C-1} y_{nc} w_c^T x_n - \\log \\sum_{c=1}^C e^{w_c^T x_n} \\right)\\] 其梯度和海森矩阵分别为： \\[g(w) = \\sum_{n=1}^N (y_n - p_n(w)) \\otimes x_n \\\\ H(w) = -\\sum_{n=1}^N \\left( \\text{diag}(p_n(w)) - p_n(w) p_n(w)^T \\right) \\otimes (x_n x_n^T)\\] 通过构造海森矩阵的下界，可以得到： \\[H(w) \\succ -\\frac{1}{2} \\left( I - \\frac{1}{C} \\mathbf{1} \\mathbf{1}^T \\right) \\otimes \\left( \\sum_{n=1}^N x_n x_n^T \\right) \\equiv B\\] 这里的\\(I\\)是一个\\((C−1)\\)维的单位矩阵，而\\(1\\)是一个全为 1 的\\((C - 1)\\)维向量。在二分类的情况下，这可以变为： \\[H(w) \\succ -\\frac{1}{2} \\left(1 - \\frac{1}{2}\\right) \\left(\\sum_{n=1}^N x_n^T x_n\\right) = -\\frac{1}{4} X^T X\\] 这是因为\\(p_n \\leq 0.5\\)，所以有\\(- (p_n - p_n^2) \\geq -0.25\\)。 因此更新规则为： \\[w^{t+1} = w^t - B^{-1} g(w^t)\\] 在二分类的情况下，\\(g_t = \\nabla \\ell(w_t) = X^T (y - \\mu_t)\\)，则更新规则为： \\[w^{t+1} = w^t - 4(X^TX)^{-1} g^t\\] 与牛顿法（IRLS）的标准更新比较： \\[w^{t+1} = w^t - H^{-1} g(w^t) = = w^t − (X^TS^tX)^{−1}g^t\\] 可以看出，使用下界的MM算法在每一步的计算上会更快，因为\\((X^T X)^{-1}\\)可以预先计算。 3. EM算法 EM算法通过在两个步骤之间交替进行来估计模型参数： E步骤（期望步骤）：估计隐藏变量或缺失值。 M步骤（最大化步骤）：使用完全观察到的数据计算MLE。 这个过程需要迭代，因为期望值依赖于参数，而参数又依赖于期望值。 3.1 下界 EM算法的目标是最大化观察数据的对数似然函数： \\[\\ell(\\theta) = \\sum_{n=1}^{N} \\log p(y_n | \\theta) = \\sum_{n=1}^{N} \\log \\left( \\sum_{z_n} p(y_n, z_n | \\theta) \\right)\\] 其中\\(y_n\\)​ 是可观察变量，\\(z_n\\)​ 是隐藏变量。直接优化这个对数似然函数是困难的，因为对数函数不能被推入求和符号内部。 EM算法通过引入任意分布\\(q_n(z_n)\\)来解决这个问题。观察数据的对数似然函数可以写为： \\[\\ell(\\theta) = \\sum_{n=1}^{N} \\log \\left( \\sum_{z_n} q_n(z_n) \\frac{p(y_n, z_n | \\theta)}{q_n(z_n)} \\right)\\] 利用詹森不等式（Jensen’s inequality），可以将对数函数推入期望之内，得到对数似然函数的下界： \\[\\ell(\\theta) \\geq \\sum_n \\sum_{z_n} q_n(z_n) \\log \\frac{p(y_n, z_n | \\theta)}{q_n(z_n)}\\] 这可以进一步写为： \\[\\ell(\\theta) \\geq \\sum_n \\mathbb{E}_{q_n}[\\log p(y_n, z_n | \\theta)] + H(q_n)\\] 其中\\(H(q)\\)是概率分布\\(q\\)的熵。定义下界为ELBO： \\[\\mathcal{L}(\\theta, \\{q_n\\} | D) = \\sum_n \\mathcal{L}(\\theta, q_n | y_n)\\] 优化这个下界是变分推断的基础。 3.2 E步骤 下界的每一项可以写为： \\[\\mathcal{L}(\\theta, q_n | y_n) = \\sum_{z_n} q_n(z_n) \\log \\frac{p(y_n, z_n | \\theta)}{q_n(z_n)}\\] 这可以分解为： \\[\\begin{align*} \\mathcal{L}(\\theta, q_n | y_n) &amp;= \\sum_{z_n} q_n(z_n) \\log \\frac{p(y_n, z_n | \\theta)}{q_n(z_n)} \\\\ &amp;= \\sum_{z_n} q_n(z_n) \\log \\frac{p(z_n | y_n, \\theta) p(y_n | \\theta)}{q_n(z_n)} \\\\ &amp;= \\sum_{z_n} q_n(z_n) \\log \\frac{p(z_n | y_n, \\theta)}{q_n(z_n)} + \\sum_{z_n} q_n(z_n) \\log p(y_n | \\theta) \\\\ &amp;= -D_{KL}(q_n(z_n) \\parallel p(z_n | y_n, \\theta)) + \\log p(y_n | \\theta) \\end{align*}\\] 因此，我们可以通过将每个\\(q_n\\)设置为： \\[q_n^* = p(z_n | y_n, \\theta)\\] 来最大化下界\\(\\mathcal{L}(\\theta, \\{q_n\\} | D)\\)。这就是E步骤。此时，ELBO就等于最大似然： \\[\\mathcal{L}(\\theta, \\{q_n^*\\} | D) = \\sum_n \\log p(y_n | \\theta) = \\ell(\\theta | D)\\] 3.3 M步骤 在M步骤中，我们需要最大化： \\[\\mathcal{L}(\\theta, \\{q_n^t\\})\\] 由于熵项\\(H(q_n)\\)在\\(\\theta\\)方面是常数，因此在M步骤中可以忽略这些项。我们剩下的部分是： \\[\\ell_t(\\theta) = \\sum_n \\mathbb{E}_{q_n^t(z_n)} [\\log p(y_n, z_n | \\theta)]\\] 如果联合概率属于指数族，我们可以将其重写为： \\[\\ell_t(\\theta) = \\sum_n \\mathbb{E}[T(y_n, z_n)]^T \\theta - A(\\theta)\\] 在M步骤中，我们最大化期望的完整数据对数似然函数，得到： \\[\\theta^{t+1} = \\arg \\max_{\\theta} \\sum_n \\mathbb{E}_{q_n^t} [\\log p(y_n, z_n | \\theta)]\\] 对于指数族的情况，最大化可以通过匹配期望充分统计量的矩来闭合地求解。 3.4 例子 书中以缺失数据的MLE为例子： 3.5 扩展EM算法 Variational EM （变分 EM） 变分 EM 是 EM 算法的一种变体。在 E 步中，我们使用一种称为变分推断的方法来近似后验分布。具体来说，在 E 步中，我们选择一个分布\\(q^*_n\\)​ 来最小化\\(q_n\\)​ 与真实后验分布\\(p(z_n | x_n, \\theta)\\)之间的 KL 散度： \\[q^*_n = \\arg \\min_{q_n \\in Q} D_{KL}(q_n \\| p(z_n | x_n, \\theta))\\] 这实际上是一个变分推断过程。在这种情况下，\\(Q\\)是我们定义的一个分布族。如果分布族\\(Q\\)足够灵活，能够包含真实的后验分布，那么\\(q_n\\)可以接近\\(p(z_n | x_n, \\theta)\\)，使得 KL 散度趋于 0。 为了计算简便，通常选择较为简单的分布族。例如，即使真实后验分布是相关的，我们可能也会假设\\(q_n(z_n) = N(z_n | \\mu_n, \\text{diag}(\\sigma_n))\\)（即独立的多元正态分布）。 在这种情况下，使用有限的分布族\\(Q\\)代替真实的后验分布，称为 变分 EM。变分 EM 不一定能保证增加实际的对数似然值，但可以单调增加变分下界。 Hard EM 在变分 EM 的框架下，假设我们使用的是一个退化的后验近似，即仅考虑后验分布的一个点估计： \\[q(z | x_n) = \\delta_{z_{\\hat{n}}}(z)\\] 这样\\(z_{\\hat{n}} = \\arg \\max_z p(z | x_n)\\)，即后验分布的最大值点。这样的方式称为 Hard EM，即在 E 步中忽略了\\(z_n\\)​ 的不确定性。 这种方法的缺点是，它很容易导致过拟合，特别是在隐变量的数量与数据量成正比的情况下。 Monte Carlo EM 当 E 步难以求解时，可以使用 Monte Carlo 方法来近似期望的充分统计量。具体地，我们从后验分布中采样： \\[z^s_n \\sim p(z_n | x_n, \\theta^{(t)})\\] 然后计算每个样本的充分统计量，最后将结果取平均。这种方法称为 Monte Carlo EM（MCEM）。 每个 E 步中都要等待 MCMC 收敛会非常耗时。另一种选择是使用随机近似法（stochastic approximation），即在 E 步中只进行简短的采样，之后进行部分参数更新。这种方法称为 随机近似 EM（Stochastic Approximation EM），通常比 MCEM 更快。 Generalized EM （广义 EM） 在某些情况下，我们可以精确地执行 E 步，但无法精确地执行 M 步。然而，即使不精确执行 M 步，也可以通过增加期望的完整数据对数似然来单调增加对数似然。这种情况下，我们可以只进行“部分” M 步，例如执行几个梯度下降步。这种方法称为 广义 EM（GEM）。 具体地，我们可以使用一个牛顿-拉弗森（Newton-Raphson）步骤来更新参数： \\[\\theta^{(t+1)} = \\theta^{(t)} - \\eta_t H_t^{-1} g_t\\] 其中，\\(0 &lt; \\eta_t \\leq 1\\)是步长，\\(g_t\\)和\\(H_t\\)​ 分别表示梯度和 Hessian 矩阵： \\[g_t = \\frac{\\partial}{\\partial \\theta} Q(\\theta, \\theta^{(t)}) \\big|_{\\theta = \\theta^{(t)}}​ \\\\H_t = \\frac{\\partial^2}{\\partial \\theta \\partial \\theta^T} Q(\\theta, \\theta^{(t)}) \\big|_{\\theta = \\theta^{(t)}}\\] 当\\(\\eta_t = 1\\)时，这被称为梯度 EM 算法。为了加速算法，也可以采用较大的步长，如在准牛顿 EM 中使用 BFGS 近似替代 Hessian。 五、贝叶斯优化 贝叶斯优化（Bayesian Optimization）简称 BayesOpt，它是一种基于模型的黑盒优化方法，旨在优化难以评估的目标函数\\(f : X \\to R\\)的情形。（例如模拟成本比较高、训练复杂模型等）。由于计算代价高，我们希望尽可能少地调用目标函数，即减少查询\\(x\\)的次数。因此，贝叶斯优化通过构建一个代理函数来近似目标函数\\(f\\)并在每次查询进行选择，避免重复昂贵的函数评估。 贝叶斯优化的核心思想是：在构建代理函数的基础上权衡探索（exploration）和利用（exploitation）。 探索指的是在不确定的区域进行查询，以改善对代理模型的整体了解； 利用指的是在目标函数\\(f(x)\\)预期较高的区域查询，以更快地找到最优点。这种权衡称为探索-利用困境。 假设目标函数的查询数据集为\\(D_n = \\{(x_i, y_i) : i = 1, ..., n\\}\\)，其中\\(y_i = f(x_i) + \\epsilon_i\\)，其中\\(\\epsilon_i\\)​ 是一个可能的噪声项。基于此数据集构建的代理模型可以估计\\(f\\)的概率分布\\(p(f | D_n)\\)。每一步优化中，我们通过采集函数（acquisition function）来选择下一个查询点\\(x_{n+1}\\)​，采集函数根据模型不确定性和目标值的期望收益计算出最优的查询点。 1. 顺序模型优化（Sequential Model-Based Optimization, SMBO） 贝叶斯优化是一种顺序模型优化（SMBO）的方法。在每一次迭代中，贝叶斯优化通过一个已标记的数据集\\(D_n = \\{(x_i, y_i) : i = 1, ..., n\\}\\)来更新对代理模型的估计。该数据集记录了每个查询点\\(x_i\\)​ 及其对应的函数值\\(y_i = f(x_i) + \\epsilon_i\\)。基于此数据集，我们可以估计目标函数的概率分布\\(p(f | D_n)\\)，然后使用采集函数\\(\\alpha(x; D_n)\\)来决定下一个查询点\\(x_{n+1}\\)​。查询\\(x_{n+1}\\)​ 获得\\(y_{n+1} = f(x_{n+1}) + \\epsilon_{n+1}\\)后，我们更新对目标函数的估计，然后重复上述步骤。 贝叶斯优化的主要步骤包括： 代理模型的构建和更新：代理模型用于近似目标函数\\(f\\)的后验分布\\(p(f | D_n)\\)。 采集函数的定义和优化：采集函数\\(\\alpha(x; D_n)\\)用于评估查询点的潜在收益，从而决定下一个查询点的位置。 下图展示了贝叶斯优化的运行过程： 在初始时，通过两个已查询点\\(x_1\\)和\\(x_2\\)​ 确定了代理模型在这些位置上的函数值，模型在这两点附近的不确定性趋近于0。 采集函数（绿色曲线）在已查询点的位置值为 0，并在代理模型不确定性较高的区域达到最大值。 之后的迭代中，随着新的点被查询和观测，模型逐渐减少了对真实函数的估计不确定性，逐步逼近目标函数的全局最优解。 2. 代理函数 对于代理函数，主要有高斯过程（Gaussian Processes, GPs）、贝叶斯神经网络（Bayesian Neural Networks, BNNs）等。 高斯过程（GPs） 高斯过程是一种常用的代理函数。在 GP 中，\\(p(f(x)|D_n)\\)被建模为正态分布\\(N(f|\\mu_n(x), \\sigma_n^2(x))\\)，其中均值\\(\\mu_n(x)\\)和方差\\(\\sigma_n(x)\\)可以通过训练数据集\\(D_n\\)​ 推导得出。 核函数\\(K_\\theta(x, x&#39;)\\)用于度量输入点\\(x\\)和\\(x′\\)之间的相似度。相似度高的点对应该有相似的函数值，因此可以正相关。高斯过程据此在训练点之间内插值（有时甚至可以外推）。核函数的选择至关重要。通常可以通过最大化边际似然来估计核参数\\(\\theta\\)，或者使用贝叶斯推断来近似边际化\\(\\theta\\)。 高斯过程在数据较少时效果很好，支持闭式的贝叶斯更新，但计算代价为\\(O(N^3)\\)（其中\\(N\\)为样本数），对于大量函数评估来说会很慢。有一些方法可以将计算复杂度降低至\\(O(NM^2)\\)，其中\\(M\\)是可选参数，但会牺牲精度。 贝叶斯神经网络（BNNs） 贝叶斯神经网络是高斯过程的一种自然替代。使用线性回归时，可以高效地进行精确的贝叶斯推断。但若使用非线性模型（如深度神经网络，DNN），则需使用近似推断。 BNNs 提供了一个在贝叶斯优化中替代 GPs 的方式，因为它们能够建模非线性关系，且在大数据量下通常更高效。关于贝叶斯网络的具体内容会后面进行学习。 还可以使用其他形式的回归模型。例如，随机森林集成模型在贝叶斯优化中表现良好，因为它们可以处理条件参数空间，但需要bootstrapping来获得不确定性估计，计算较慢。 3. 采集函数 采集函数用来评估每个可能的查询点的预期效用。一般表示为： \\[\\alpha(x|D_n) = \\mathbb{E}_{p(y|x,D_n)}[U(x, y; D_n)]\\] 其中： \\(y = f(x)\\)是在点\\(x\\)处未知函数的值， \\(U\\)是效用函数，它决定不同采集函数的形式。 下面介绍一些常见的采集函数。 3.1 改进概率（Probability of Improvement, PI） 改进概率用于评估某一点是否可能带来比当前最优观测值更好的结果。定义如下： 首先，设定目前的最优值（incumbent）为： \\[M_n = \\max_{i=1}^n y_i\\] 如果观测是有噪声的，可以选择最高均值代替最大观测值。定义效用函数： \\[U(x, y; D_n) = I(y &gt; M_n)\\] 其中\\(I(\\cdot)\\)是指示函数，即当\\(y\\)超过当前最优值\\(M_n\\)​ 时，产生效用 1，否则为 0。 因此，PI 采集函数可以表示为： \\[\\alpha_{\\text{PI}}(x; D_n) = p(f(x) &gt; M_n | D_n)\\] 若代理模型是高斯过程，可以得到 PI 的闭式解（就是简单的高斯分布求概率）： \\[\\alpha_{\\text{PI}}(x; D_n) = \\Phi\\left(\\gamma_n(x, M_n)\\right)\\] \\[\\gamma_n(x, \\tau) = \\frac{\\mu_n(x) - \\tau}{\\sigma_n(x)}\\] 其中： \\(\\Phi\\)是标准正态分布的累积分布函数（CDF）， \\(\\mu_n(x)\\)和\\(\\sigma_n(x)\\)分别是高斯过程在\\(x\\)处的均值和标准差。 3.2 期望改进（Expected Improvement, EI） 期望改进考虑的是改进的量，而不仅仅是是否产生改进。定义效用函数为 ： \\[U(x, y; D_n) = (y - M_n)I(y &gt; M_n)\\] 这样只有\\(y &gt; M_n\\)​ 时才会有正值的效用。 因此，EI 采集函数表示为： \\[\\alpha_{\\text{EI}}(x; D_n) = \\mathbb{E}_{D_n}[(f(x) - M_n)I(f(x) &gt; M_n)]\\] 在高斯过程模型下，EI 有以下闭式解： \\[\\alpha_{\\text{EI}}(x; D_n) = (\\mu_n(x) - M_n)\\Phi(\\gamma) + \\sigma_n(x)\\phi(\\gamma)\\] 其中： \\(\\phi\\)是标准正态分布的概率密度函数 \\(\\gamma = \\gamma_n(x, M_n) = \\frac{\\mu_n(x) - M_n}{\\sigma_n(x)}\\) 我们可以注意到EI 的两个部分： 第一项\\((\\mu_n(x) - M_n)\\Phi(\\gamma)\\)促进利用（选择均值高的点） 第二项\\(\\sigma_n(x)\\phi(\\gamma)\\)则促进探索（选择方差大的点）。 如果无法计算预测方差，可以使用蒙特卡洛方法近似： \\[\\alpha_{\\text{EI}}(x; D_n) \\approx \\frac{1}{S} \\sum_{s=1}^S \\max(\\mu_n^s(x) - M_n, 0)\\] 3.3 上置信界（Upper Confidence Bound, UCB） 上置信界通过计算函数的上置信区间来决定评估点。其采集函数定义为： \\[\\alpha_{\\text{UCB}}(x; D_n) = \\mu_n(x) + \\beta_n \\sigma_n(x)\\] 其中\\(\\beta_n\\)​ 控制置信区间的宽度。在高斯过程代理模型下，这种方法称为 GP-UCB。 3.4 汤普森采样（Thompson Sampling） 汤普森采样尝试直接从后验分布中采样，以找到潜在的最优点。采集函数定义为： \\[\\alpha(x; D_n) = \\mathbb{E}_{p(\\theta|D_n)} \\left[ I\\left( x = \\arg\\max_{x&#39;} f_\\theta(x&#39;) \\right) \\right]\\] 通过从\\(p(\\theta|D_n)\\)中采样一个\\(\\tilde{\\theta}\\)，我们选择最大化\\(f_{\\tilde{\\theta}}(x)\\)的点。对于高斯过程，采样会比较复杂，因此应用起来需要特殊的技术。 3.5 熵搜索（Entropy Search） 熵搜索直接最小化对最优点位置的不确定性。熵搜索的效用函数定义为： \\[U(x, y; D_n) = H(x^*|D_n) - H(x^*|D_n \\cup \\{(x, y)\\})\\] 其中\\(H(x^*|D_n)\\)是后验分布\\(p^*(x|D_n)\\)的熵。熵搜索的采集函数为： \\[\\alpha_{\\text{ES}}(x; D_n) = \\mathbb{E}_{p(y|x, D_n)}[U(x, y; D_n)] = H(x^*|D_n) - \\mathbb{E}_{p(y|x, D_n)}[H(x^*|D_n \\cup \\{(x, y)\\})]\\] 通过预测熵搜索（Predictive Entropy Search, PES），可以使用互信息的对称性来简化该采集函数： \\[\\alpha_{\\text{PES}}(x; D_n) = H(y|D_n, x) - \\mathbb{E}_{x^*|D_n}[H(y|D_n, x, x^*)]\\] 在这里，可以通过汤普森采样来近似积分。 3.6 知识梯度 (Knowledge Gradient) 知识梯度获取函数的核心思想是：通过考虑在查询一个新点后，更新我们的后验分布，然后基于新的分布进一步寻找最大值。这种方法不只是关注当前步骤的最大化，而是展望两步，从而达到长远的优化效果。 假设我们在点\\(x\\)查询后，得到了新的观测值\\(y\\)，可以根据新的信念找到下一个可能的最佳值： \\[V_{n+1}(x, y) = \\max_{x&#39;} \\mathbb{E}_{p(f|x, y, D_n)} [f(x&#39;)]\\] 其中\\(V_{n+1}(x, y)\\)表示在位置\\(x\\)查询并得到结果\\(y\\)后，下一步所能达到的最大值。 由于我们在进行查询时并不知道真实的\\(y\\)值，因此需要对所有可能的\\(y\\)取期望，以得出在查询\\(x\\)位置后的期望的增益： \\[V_{n+1}(x) = \\mathbb{E}_{p(y|x, D_n)} [V_{n+1}(x, y)]\\] 知识梯度获取函数的定义如下： \\[\\alpha_{KG}(x; D_n) = \\mathbb{E}_{D_n} \\left[(V_{n+1}(x) - M_n) \\cdot I(V_{n+1}(x) &gt; M_n)\\right]\\] 这里\\(M_n = \\max_{i=1}^n y_i\\)，​ 是当前已观察到的最佳值（即，迄今为止观察到的最大值）。该函数的定义类似于期望提升（Expected Improvement, EI），区别在于知识梯度考虑的是观察新点后可以利用的新知识，而不仅仅是找到一个新的最大值。 下面是上面方法之间在一个实践中的对比： 六、无梯度优化 无梯度优化（DFO，Derivative-free Optimization）指的是在不知道函数的导数信息时如何进行优化。根据目标函数的评估成本，无梯度优化可以分为贝叶斯优化（适用于高评估成本的情况）和随机局部搜索或进化搜索（适用于低评估成本的情况）。贝叶斯优化已经讲过了，所以下面的内容主要集中在随机局部搜索或进化搜索。 1. 局部搜索（Local Search） 局部搜索是一类启发式优化算法，旨在寻找离散、非结构化搜索空间中的全局最大值。传统的梯度更新形式为： \\[\\theta_{t+1} = \\theta_t + \\eta_t d_t\\] 但在无梯度优化中无法使用此更新形式，因此使用一个离散版本的更新方式。具体来说，更新的公式为： \\[L(x)x_{t+1} = \\arg \\max_{x \\in \\text{nbr}(x_t)} L(x)\\] 其中\\(\\text{nbr}(x_t) \\subset X\\)表示\\(x_t\\)的邻域，这一方法被称为爬山算法（hill climbing）、最陡上升（steepest ascent）或贪心搜索。 若一个点的邻域包含整个搜索空间，则上面公式可在一步内返回全局最优解，但通常这种邻域过大，难以完全搜索。因此，通常会定义局部邻域。例如，在8皇后问题中，目标是将8个皇后放置在 8×8 棋盘上，使它们互不攻击。8皇后的状态空间\\(X = 64^8\\)，但由于约束，只有大约 17M 个可行状态。定义每个状态的邻域为所有通过将一个皇后移动到同列其他位置而生成的状态，因此每个状态有 56 个邻居。然而，随机生成的初始状态通过最陡上升法只能成功解决14%的问题，并且在失败时会停留在局部最优解处。 1.1 随机局部搜索（Stochastic Local Search） 爬山算法是一种贪心算法，因为它在邻域内选择最优点。这种方法容易卡在局部最优解。为了减少这种情况，一个解决方法是将目标在每一步的最大化从确定性转变为随机化。具体而言，可以定义邻居的概率分布，概率与邻居改进的程度成正比，从而随机采样一个邻居点。这被称为随机爬山法（stochastic hill climbing）。 如果逐渐减小该概率分布的熵，使算法逐渐趋于贪心，则得到模拟退火算法（simulated annealing）。另一种方法是使用贪心爬山算法，但在遇到局部最优解时，从一个新的随机起点重新开始。这种方法称为随机重启爬山法（random restart hill climbing）。例如在8皇后问题中，如果每次爬山搜索的成功概率为\\(p \\approx 0.14\\)，期望需要\\(R = 1/p \\approx 7\\)次重启才能找到有效解。 1.2 禁忌搜索（Tabu Search） 爬山法在达到局部最优或平台时会停止。虽然可以通过随机重启来避免这种情况，但会丢弃已有的信息。禁忌搜索（Tabu Search）是一种更智能的替代方法。禁忌搜索允许移动到得分下降的状态，只要该状态之前没有访问过。为此，我们维护一个禁忌表（tabu list），记录最近访问的\\(\\tau\\)个状态，从而强制算法探索新的状态，增加逃离局部最优解的机会。 禁忌搜索达到局部最高点\\(x_t\\)​ 时，会移动到邻居\\(x_{t+1} \\in \\text{nbr}(x_t)\\)，该点得分较低。接着它会移动到之前状态的邻居\\(x_{t+2} \\in \\text{nbr}(x_{t+1})\\)，禁忌表阻止它返回到峰顶，从而使它继续探索，直到找到新的峰顶，或达到最多非改进步数\\(c_{\\text{max}}\\)​。 在8皇后问题中，禁忌搜索能将解决成功率从14%提高到94%，平均每个成功实例需要21步，失败实例需要64步。 2. 模拟退火 (Simulated Annealing) 模拟退火是一种随机局部搜索算法，通常用于寻找黑箱函数\\(E(x)\\)的全局最小值，这里的\\(E\\)被称为能量函数。模拟退火的灵感来源于物理学中的退火过程，通过逐步降低“温度”来达到目标。 将能量函数\\(E(x)\\)转换为状态概率分布： \\[p(x) = \\exp(-E(x))\\] 这里的\\(p(x)\\)是未归一化的概率分布，目的是构建一个与能量成反比的概率分布。 模拟退火每次会使用一种变体的 Metropolis-Hastings 算法（采样算法）来从概率分布中采样。每次迭代中，算法会选择一个新的候选状态\\(x′\\)，然后按照概率接受或拒绝该状态。接受概率由能量差值和当前“温度”决定： \\[\\text{接受概率} = \\min \\left( 1, \\exp \\left( \\frac{E(x) - E(x&#39;)}{T} \\right) \\right)\\] 其中\\(T\\)是“温度”参数，随着迭代次数增加逐渐减小。在高温阶段，算法倾向于接受更差的解，以避免陷入局部极小值；在低温阶段，逐渐收敛于全局最优解。 3. 进化算法 (Evolutionary Algorithms) 进化算法是一类仿生算法，模拟自然选择和遗传进化的过程，以优化复杂目标函数。这类算法通常使用一个种群\\(S_t\\)来表示候选解的集合。 进化算法的主要组成部分 适应度 (Fitness)： 每个候选解的“适应度”是其目标函数的值，即评估该解的好坏。 选择函数 (Selection Function)： 从当前种群中选择适应度较高的成员作为“父代”生成“子代”。常见的选择策略包括： 截断选择 (Truncation Selection)： 从适应度最高的前\\(K\\)个成员中随机选择父代。 锦标赛选择 (Tournament Selection)： 每次从种群中随机选择\\(K\\)个成员，选择适应度最高的个体作为父代。 适应度比例选择 (Fitness Proportionate Selection)： 按照个体适应度的相对比例选择父代（也叫轮盘选择）。 重组 (Recombination) 和变异 (Mutation)： 生成新候选解的关键操作： 交叉 (Crossover)： 对两个父代进行基因交换，产生子代。例如在遗传算法中，每个个体可以表示为二进制或整数向量，随机选择切割点并交换父代的子串。 变异 (Mutation)： 对单个个体进行小范围随机改变，用于增加种群多样性。 常见的进化算法如下： 遗传算法 (Genetic Algorithm, GA)： 使用二进制或整数表示个体，通过交叉和变异产生新的个体，如上图。 遗传编程 (Genetic Programming, GP)： 使用树状结构表示个体，适合生成结构化对象或程序，如下图。 代理辅助进化算法 (Surrogate-assisted EA)： 使用近似模型替代真实的目标函数，以减少评估开销。 记忆算法 (Memetic Algorithm)： 结合进化算法和局部搜索，提升求解效率。 4. 分布估计算法 分布估计算法（Estimation of Distribution Algorithms, EDA）是一种进化算法（EA）的变体。EDA 的主要思想是用概率模型来表示高适应度解的分布，而不是通过基因操作（如交叉、变异）来生成新解。 传统的进化算法依赖遗传操作来保持和生成候选解的种群，而 EDA 则采用显式的概率模型来估计解的分布。在每次迭代中，EDA 从当前的概率模型中采样生成候选解，选出最优的部分解，然后基于这些解更新概率模型，从而使其逐步向最优解靠近。 具体的 EDA 操作步骤： 候选解采样：从当前模型\\(p(x|\\theta_t)\\)中采样\\(K&#39; &gt; K\\)个候选解，构成种群\\(S_t = \\{x_k \\sim p(x|\\theta_t)\\}\\)。 选择操作：用适应度函数对这些候选解排序，选择出最优的\\(K\\)个解，记为\\(S^*_t\\)（截断选择）。 更新概率模型：利用最大似然估计（MLE）将新模型\\(p(x|\\theta_{t+1})\\)拟合到\\(S^*_t\\)​ 上，得到下一代的解分布。 EDA 本质上是通过最小化\\(S^*_t\\)​ 的经验分布与新模型分布\\(p(x|\\theta_{t+1})\\)的交叉熵来进行更新，因此与交叉熵法（CEM）有一定联系。对比来看，交叉熵法通常假设模型为多元高斯分布\\(N(x|\\mu, \\Sigma)\\)，而 EDA 更加通用。 在实际应用中，为了表示解空间中的变量依赖性，可以选择更加复杂的概率模型。 对于连续变量，可以使用多元高斯模型\\(p(x) = N(x|\\mu, \\Sigma)\\)，此方法被称为多元正态估计算法（EMNA）。 对于离散变量：可以用概率图模型（如树结构、贝叶斯网络）来捕捉变量之间的依赖关系。Chow-Liu 算法可以用于树结构的学习，而更复杂的图模型结构也可以通过 BOA 等算法学习。 随着深度学习的应用，可以使用深度生成模型来表示解分布。例如，可以使用去噪自编码器、NADE、DNN 回归模型、RBM（受限玻尔兹曼机）和 VAE（变分自编码器）等。 4.1 示例：UMDA 考虑一个简单的例子，目标空间是长度为\\(D\\)的二进制字符串，适应度函数为 \\[f(x) = \\sum_{d=1}^D x_d\\] 其中\\(x_d \\in \\{0, 1\\}\\)（称为 one-max 函数）。一个简单的概率模型是完全分解的模型： \\[p(x|\\theta) = \\prod_{d=1}^D \\text{Ber}(x_d|\\theta_d)\\] 这称为单变量边际分布算法（UMDA）。对于这种伯努利分布，可以通过统计\\(S^*_t\\)​ 中各位置的 1 的比例来估计参数： \\[\\hat{\\theta}_{d, t+1} = \\frac{1}{N_t} \\sum_{k=1}^K I(x_{k, d} = 1)\\] 其中\\(K = |S^*_t|\\)，表示选择的候选解数量。为了平滑参数更新，可以使用如下增量更新公式： \\[\\hat{\\theta}_{d, t+1} = (1 - \\eta_t) \\hat{\\theta}_{d, t} + \\eta_t \\theta_{d, t}\\] 其中，\\(\\eta_t\\)是学习率，\\(\\theta_{d, t}\\)是第\\(t\\)代选择子集中第\\(d\\)位为 1 的比例。这个更新方法称为基于种群的增量学习（PBIL）。 4.2 交叉熵法（CEM） 交叉熵法是一种特殊的分布估计算法（EDA），其中种群由多元高斯分布表示。算法的主要步骤如下： 设置均值和协方差：\\(\\mu_{t+1}\\)和\\(\\Sigma_{t+1}\\)被设置为\\(S^*_{t+1}\\)的经验均值和协方差，\\(S^*_{t+1}\\)是前 K 个样本。 优化问题： 在 CEM 中，我们希望最大化以下目标： \\[\\theta_{t+1} = \\arg\\max_{\\theta} \\sum_{i \\in S_t} p_t(i) \\log p(x_{t,i} | \\theta)\\] 其中\\(p_t(i)\\)是与选择的样本\\(S^*_t\\)相关的概率分布。 5. 进化策略（Evolutionary Strategies, ES） 进化策略是一种基于分布的优化方法，其中种群的分布用高斯分布表示。与 CEM 不同，ES 通过对期望目标的梯度上升来更新参数，而不是使用 MLE。主要步骤如下： 平滑目标函数： \\[L(\\theta) = \\mathbb{E}_{p(x|\\theta)}[f(x)]\\] 计算梯度： 使用 REINFORCE 估计器计算梯度： \\[\\nabla_\\theta L(\\theta) = \\mathbb{E}_{p(x|\\theta)}[f(x) \\nabla_\\theta \\log p(x|\\theta)]\\] 这个期望可以通过蒙特卡罗采样进行近似。 当概率模型属于指数族时，可以计算自然梯度，通常收敛速度更快。自然进化策略（Natural Evolution Strategies, NES）使用自然梯度替代“普通”梯度，以加速优化。 CMA-ES 是一种 NES，主要特征在于它以加权的方式更新均值和协方差。具体步骤如下： 计算加权均值： 新均值设为当前样本的加权 MLE。 更新协方差： 使用“进化路径”来累积搜索方向，更新协方差，更新公式较为复杂。 CMA-ES 通过这些更新近似目标函数\\(L(\\theta)\\)的自然梯度，而无需显式建模费舍尔信息矩阵。","categories":[{"name":"Probabilistic Machine Learning","slug":"Probabilistic-Machine-Learning","permalink":"https://jia040223.github.io/categories/Probabilistic-Machine-Learning/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jia040223.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"数学","slug":"数学","permalink":"https://jia040223.github.io/tags/%E6%95%B0%E5%AD%A6/"}]},{"title":"[Probabilistic Machine Learning]: Fundamentals-Information theory","slug":"Fundamentals-Information theory","date":"2024-10-24T07:02:22.000Z","updated":"2024-10-24T14:59:19.329Z","comments":true,"path":"2024/10/24/Fundamentals-Information theory/","permalink":"https://jia040223.github.io/2024/10/24/Fundamentals-Information%20theory/","excerpt":"","text":"一、KL散度 机器学习本质上是对信息的处理，其中一个关键就在于如何衡量从一个信息的分布到另一个信息分布的差别与变化。KL 散度（Kullback-Leibler Divergence）作为衡量分布差异的最经典函数，其定义为： 离散情况： \\[D_{KL}(p \\| q) \\equiv \\sum_{k=1}^{K} p_k \\log \\frac{p_k}{q_k}\\] 连续情况： \\[D_{KL}(p \\| q) \\equiv \\int dx \\, p(x) \\log \\frac{p(x)}{q(x)}\\] 1. KL 散度满足的性质 1.1 连续性 KL 散度在其参数上是连续的，除非\\(p_k\\)​ 或\\(q_k\\)​ 为零。如果\\(p\\)趋近于零，满足： \\[\\lim_{p \\to 0} \\frac{p \\log p}{q} = 0\\] 这保证了当\\(p = 0\\)时，函数仍然连续。然而，当\\(q = 0\\)且\\(p &gt; 0\\)时，更新的幅度将趋向于无限。 1.2 非负性 KL 散度总是非负的： \\[D_{KL}(p \\| q) \\geq 0\\] 当且仅当\\(p = q\\)时，KL 散度等于零。证明使用 Jensen 不等式： \\[f\\left(\\sum_{i=1}^{n} \\lambda_i x_i\\right) \\leq \\sum_{i=1}^{n} \\lambda_i f(x_i)\\] 通过推导，可以得到： \\[\\begin{align*} -D_{KL}(p \\| q) &amp;= -\\sum_{x \\in A} p(x) \\log \\frac{p(x)}{q(x)} \\\\&amp;\\leq \\log \\left(\\sum_{x \\in A} \\frac{q(x)}{p(x)}p(x)\\right)\\\\ &amp;\\leq \\log 1 = 0 \\end{align*}\\] 非负性方便我们明确优化的目标。 1.3 重参数化不变性 KL 散度在随机变量的任意可逆变换下保持不变。如果将随机变量从\\(x\\)变换为\\(y = f(x)\\)，则有： \\[p(x) \\, dx = p(y) dy\\] \\[q(x) \\, dx = q(y)\\] 那么 KL 散度可以写成： \\[D_{KL}(p(x) \\| q(x)) = \\int dx \\, p(x) \\log \\frac{p(x)}{q(x)}\\] 通过变换得到： \\[= \\int dy \\, p(y) \\log \\left( \\frac{p(y)}{q(y)} \\cdot \\frac{dy}{dx} \\cdot \\frac{dx}{dy} \\right)\\] 由于\\(p(y) dy = p(x) dx\\)和\\(q(y) dy = q(x) dx\\)，可以得到： \\[D_{KL}(p(x) \\| q(x)) = D_{KL}(p(y) \\| q(y))\\] 因此，KL 散度在重参数化下是保持不变的。这表明 KL 散度是关于分布本身的，而不是表示空间的方式。 1.4 对均匀分布的单调性 从\\(N\\)个元素的均匀分布更新到\\(N&#39;\\)个元素的均匀分布，KL 散度为： \\[D_{KL}(p \\| q) = \\sum_{k} \\frac{1}{N&#39;} \\log \\frac{1/N&#39;}{1/N} = \\log \\frac{N}{N&#39;}\\] 这表明更新的幅度与元素数量的比例相关。 1.5 KL 散度的链式法则 KL 散度满足链式法则： \\[D_{KL}(p(x, y) \\| q(x, y)) = \\int dx \\, dy \\, p(x, y) \\log \\frac{p(x, y)}{q(x, y)}\\] 可以分解为： \\[D_{KL}(p(x) \\| q(x)) + E_{p(x)}\\left[D_{KL}(p(y|x) \\| q(y|x))\\right]\\] 条件 KL 散度 定义为： \\[D_{KL}(p(y|x) \\| q(y|x)) \\equiv \\int dx \\, p(x) \\int dy \\, p(y|x) \\log \\frac{p(y|x)}{q(y|x)}\\] 此外我们注意到： \\[D_{KL} (p(x,y) \\| q(x,y)) \\geq D_{KL} (p(y) \\| q(y))\\] 对这一结果的一种直观解释是，如果只部分观察随机变量，那么区分两个候选分布比观察全部随机变量更难。 2. Thinking about KL 在这一节中，我们主要讨论 KL 散度的一些特性。 2.1 KL 散度的单位 KL 散度的单位与我们选择的对数的底数有关。对数的不同底数之间的差异只是一个乘法常数的差异，因此 KL 散度的计算方式类似于选择测量信息的单位： 当使用底为 2 的对数时，KL 散度的单位为比特（bits），即“二进制数字”。 当使用自然对数（通常用于数学方便）时，单位为纳特（nats），即“自然单位”。 为了在这两种单位之间进行转换，我们有\\(\\log_2 y = \\frac{\\log y}{\\log 2}\\)，因此： \\[1 \\text{ bit} = \\log_2 \\text{ nats} \\sim 0.693 \\text{ nats}\\] \\[1 \\text{ nat} = \\frac{1}{\\log_2} \\text{ bits} \\sim 1.44 \\text{ bits}\\] 2.2 KL 散度的非对称性 KL 散度在其两个参数中不是对称的。尽管看起来非常不合理，但实际上非对称性源于我们要求自然链式法则。在分解分布时，我们需要对被条件化的变量取期望。在 KL 散度中，我们是对第一个参数\\(p(x)\\)取期望，这打破了两个分布之间的对称性。 从更直观的角度来看，从分布\\(q\\)更新到\\(p\\)所需的信息量通常与从\\(p\\)更新到\\(q\\)所需的信息量不同。例如，考虑两个 Bernoulli 分布： 第一个分布成功的概率为 0.443； 第二个分布成功的概率为 0.975。 计算这两个分布之间的 KL 散度： \\[\\begin{align*} D_{KL}(p \\| q) &amp;= 0.975 \\log \\frac{0.975}{0.443} + 0.025 \\log \\frac{0.025}{0.557} \\\\&amp;= 0.692 \\text{ nats} \\sim 1.0 \\text{ bits} \\end{align*}\\] 这表明，从分布\\([0.443, 0.557]\\)更新到\\([0.975, 0.025]\\)需要 1 比特的信息。 反过来： \\[\\begin{align*} D_{KL}(q \\| p) &amp;= 0.443 \\log \\frac{0.443}{0.975} + 0.557 \\log \\frac{0.557}{0.025} \\\\&amp;= 1.38 \\text{ nats} \\sim 2.0 \\text{ bits} \\end{align*}\\] 这表明，从分布\\([0.975, 0.025]\\)更新到\\([0.443, 0.557]\\)需要 2 比特的信息。由此可见，从一个接近均匀的分布到几乎确定的分布需要大约 1 比特的信息，而从近乎确定的结果转变到类似于抛硬币的结果则需要更多的信息。 2.3 KL 散度作为evidence的期望权重 假设你有两个不同的假设\\(P\\)和\\(Q\\)，并希望在它们之间选择。你收集了一些数据\\(D\\)。贝叶斯定理告诉我们： \\[Pr(P|D) = \\frac{Pr(D|P) \\cdot Pr(P)}{Pr(D)}\\] 通常，这需要评估边际似然\\(Pr(D)\\)，这在计算上是困难的。如果我们考虑两个假设的概率比率： \\[\\frac{Pr(P|D)}{Pr(Q|D)} = \\frac{Pr(D|P)}{Pr(D|Q)} \\cdot \\frac{Pr(P)}{Pr(Q)}\\] 边际似然\\(Pr(D)\\)被消去。取对数后结果如下： \\[\\log \\frac{Pr(P|D)}{Pr(Q|D)} = \\log \\frac{p(D)}{q(D)} + \\log \\frac{Pr(P)}{Pr(Q)}\\] 后验对数概率比率只是先验对数概率比率加上 I. J. Good 所称的evidence权重\\(D\\)： \\[w[P/Q; D] \\equiv \\log \\frac{p(D)}{q(D)}\\] 在这种解释下，KL 散度就是在假设\\(P\\)为真时，每次观察提供的\\(P\\)相对于\\(Q\\)的evidence的期望。 3. 最小化KL散度 在具体问题时，我们往往需要对KL散度进行最小化。 3.1 正向与反向 KL KL 散度的非对称性意味着，通过最小化\\(D_{KL}(p \\| q)\\)（也称为正向 KL ）来寻找一个接近于\\(p\\)的分布\\(q\\)，与通过最小化\\(D_{KL}(q \\| p)\\)（也称为反向 KL ）来寻找\\(q\\)会有一定区别。 示例： 考虑一个双峰分布\\(p\\)，我们用一个单峰高斯分布\\(q\\)来近似。为了防止\\(D_{KL}(p \\| q)\\)变为无穷大，我们必须当\\(p &gt; 0\\)时有\\(q &gt; 0\\)（即\\(q\\)必须在\\(p\\)的每个非零点上都有支持），因此\\(q\\)会覆盖两个峰，这称为模式覆盖（mode-covering）或零避免行为（orange curve）。相反，为了防止\\(D_{KL}(q \\| p)\\)变为无穷大，我们必须有\\(q = 0\\)当\\(p = 0\\)时，这会导致模式追寻（mode-seeking）或零强制行为（green curve）。 3.2 矩（Moment）投影（模式覆盖） 当我们通过最小化正向 KL 来计算\\(q\\)时： \\[q = \\arg \\min_q D_{KL}(p \\| q)\\] 我们不妨假设\\(q\\)是一种指数族分布： \\[q(x) = h(x) \\exp\\left[\\eta^T T(x) - \\log Z(\\eta)\\right]\\] 最优性的一阶条件如下： \\[\\frac{\\partial \\eta_i D_{KL}(p \\| q)}{\\partial \\eta_i} = -\\frac{\\partial \\eta_i}{\\partial \\eta_i} \\int p(x) \\log q(x) dx\\] 继续推导可得： \\[\\begin{align*} \\partial_{\\eta_i} D_{KL}(p \\| q) &amp;= -\\partial_{\\eta_i} \\int p(x) \\log q(x) \\, dx \\\\ &amp;= -\\partial_{\\eta_i} \\int p(x) \\left( \\eta^T T(x) - \\log Z(\\eta) \\right) \\, dx \\\\ &amp;= -\\partial_{\\eta_i} \\int \\left( p(x) T_i(x) \\, dx - E_{q(x)}[T_i(x)] \\right)dx \\\\ &amp;= -E_{p(x)}[T_i(x)] + E_{q(x)}[T_i(x)] = 0 \\end{align*}\\] 可以看到，当梯度为0时候，最优的\\(q\\)匹配\\(p\\)的矩，因此该过程称为矩匹配（moment matching）。这种优化被称矩投影（M-projection）。 示例： 假设真实目标分布\\(p\\)是一个相关的二维高斯分布： \\[p(x) = N(x|\\mu, \\Sigma) = N(x|\\mu, \\Lambda^{-1})\\] 我们将其近似为一个由两个一维高斯分布组成的分布\\(q\\)： \\[q(x|m, V) = N(x_1|m_1, v_1) N(x_2|m_2, v_2)\\] 进行矩匹配后，最优的\\(q\\)形状为： \\[q(x) = N(x_1|\\mu_1, \\Sigma_{11}) N(x_2|\\mu_2, \\Sigma_{22})\\] 如下图所示，结果分布\\(q\\)覆盖了\\(p\\)，即模式覆盖。 3.3 信息（Information）投影（模式追寻） 现在假设我们通过最小化反向 KL 来计算\\(q\\)： \\[q = \\arg \\min_q D_{KL}(q \\| p)\\] 这称为信息投影（I-projection）。这个优化问题通常更容易计算，因为目标需要对\\(q\\)进行期望计算，而我们可以选择一个可处理的分布族。 示例： 考虑真实分布是一个全协方差高斯分布： \\[p(x) = N(x|\\mu, \\Lambda^{-1})\\] 并让\\(q\\)为对角高斯分布： \\[q(x) = N(x|m, \\text{diag}(v))\\] 可以证明，最优的变分参数为： \\[m = \\mu\\] \\[v_i = \\Lambda^{-1}_{ii}\\] 证明过程如下： 如下图所示，我们看到后验方差过于狭窄，即近似后验过于自信。然而，值得注意的是，最小化反向 KL 并不总是导致过于紧凑的近似。 4. KL 散度的重要性质 4.1 压缩引理（Compression lemma） 一个KL散度的重要的通用结论是压缩引理，其指出，对于任何分布\\(P\\)和\\(Q\\)，以及在这些分布的定义域上定义的标量函数\\(ϕ\\)，都有以下不等式： \\[E_P[ϕ] \\leq \\log E_Q\\left[e^{ϕ}\\right] + D_{KL}(P \\| Q)\\] 证明过程： 考虑分布\\(g(x)\\)： \\[g(x) = \\frac{q(x)}{Z} e^{ϕ(x)}\\] 其中\\(Z\\)是分区函数： \\[Z = \\int dx \\, q(x) e^{ϕ(x)}\\] 计算\\(D_{KL}(P \\| G)\\)： \\[D_{KL}(P \\| G) = D_{KL}(P \\| Q) - E_P[ϕ(x)] + \\log(Z) \\geq 0\\] 压缩引理的另一种形式是 Donsker-Varadhan 变分表示： \\[D_{KL}(P \\| Q) = \\sup_{ϕ} \\left( E_P[ϕ(x)] - \\log E_Q\\left[e^{ϕ(x)}\\right] \\right)\\] 这样我们便为KL散度提供了一个下界。 4.2 KL 散度的数据处理不等式（Data processing inequality for KL） 数据处理不等式表明，任何对来自两个不同分布的样本进行处理的操作都会使这两个样本趋向彼此。具体来说，考虑两个不同的分布\\(p(x)\\)和\\(q(x)\\)，通过概率通道\\(t(y|x)\\)处理后，得到的分布满足： \\[D_{KL}(p(x) \\| q(x)) \\geq D_{KL}(p(y) \\| q(y))\\] 证明过程： \\[\\begin{align*} D_{KL} (p(x) \\| q(x)) &amp;= \\int dx \\, p(x) \\log \\frac{p(x)}{q(x)} \\\\ &amp;= \\int dx \\, \\int dy \\, p(x) t(y|x) \\log \\frac{p(x) t(y|x)}{q(x) t(y|x)} \\\\ &amp;= \\int dx \\, \\int dy \\, p(x, y) \\log \\frac{p(x, y)}{q(x, y)} \\\\ &amp;= -\\int dy \\, p(y) \\int dx \\, p(x|y) \\log \\frac{q(x, y)}{p(x, y)} \\\\ &amp;\\geq - \\int dy \\, p(y) \\log \\left( \\int dx \\, p(x|y) \\frac{q(x, y)}{p(x, y)} \\right) \\\\ &amp;= - \\int dy \\, p(y) \\log \\left( \\frac{q(y)}{p(y)} \\int dx \\, q(x|y) \\right) \\\\ &amp;= \\int dy \\, p(y) \\log \\frac{p(y)}{q(y)} \\\\&amp;= D_{KL} (p(y) \\| q(y)) \\end{align*}\\] 数据处理不等式表明，对随机样本的任何处理都会使这两个分布更难以区分，即这样导致了信息的损失。 5. KL散度与最大似然估计 目标：我们希望找到一个分布\\(q\\)，使其与真实分布\\(p\\)的KL散度最小化： \\[q^* = \\arg \\min_q D_{KL}(p \\| q) =\\arg \\min_q (\\int p(x) \\log p(x) dx - \\int p(x) \\log q(x) dx)\\] 经验分布：\\(p\\)是经验分布\\(p_D\\)​指的是它只在观察到的训练数据上有概率，其余地方为零： \\[p_D(x) = \\frac{1}{N} \\sum_{n=1}^N \\delta(x - x_n)\\] 其中\\(\\delta\\)是狄拉克函数。 利用狄拉克函数的特性，可以推导出： \\[D_{KL}(p_D \\| q) = -\\int p_D(x) \\log q(x) dx + C = -\\frac{1}{N} \\sum_{n} \\log q(x_n) + C\\] 这里\\(C\\)是与\\(q\\)无关的常数。 可以将上述推导重写为： \\[D_{KL}(p_D \\| q) = H_{ce}(p_D, q) - H(p_D)\\] 其中\\(H_{ce}\\)​ 是交叉熵，定义为： \\[H_{ce}(p, q) = -\\sum_k p_k \\log q_k\\] ​交叉熵\\(H_{ce}(p_D, q)\\)是在训练集上评估\\(q\\)的平均负对数似然。 所以最小化KL散度与经验分布的过程等价于最大化似然，这意味着我们在训练过程中希望找到一个能很好地拟合训练数据的分布。 基于似然的训练方法过于依赖训练集。经验分布仅在有限的数据点上有概率质量，而在其它地方为零，这可能导致对真实分布的不合理假设。即使数据集很大，实际数据来源的空间通常更大，因此需要通过在“相似”输入之间共享概率质量来平滑经验分布。 6. KL散度与贝叶斯推断 贝叶斯推断可以被看作是一个特定的最小化问题，目标是使得新的联合分布\\(p(\\theta, D)\\)尽可能接近先验分布\\(q(\\theta, D)\\)，同时满足已知数据\\(D_0\\)​ 的约束条件： \\[p(\\theta, D) = \\arg\\min D_{KL}(p(\\theta, D) \\| q(\\theta, D)) \\quad \\text{subject to} \\quad p(D) = \\delta(D - D_0)\\] 这里\\(\\delta(D - D_0)\\)是一个将所有质量集中在数据集\\(D_0\\)​ 上的狄拉克分布。 将KL散度写成链式法则的形式，可以得出： \\[D_{KL}(p(\\theta, D) \\| q(\\theta, D)) = D_{KL}(p(D) \\| q(D)) + D_{KL}(p(\\theta|D) \\| q(\\theta|D))\\] 根据贝叶斯公式，我们有： \\[q(\\theta, D) = q(D)q(\\theta|D) = q(\\theta)q(D|\\theta) \\Rightarrow q(\\theta|D) = \\frac{q(D|\\theta) q(\\theta)}{q(D)}\\] 这里注意到\\(q(\\theta|D)\\)与\\(q(D|\\theta), q(\\theta), q(D)\\)之间的关系，但它们都是同一分布的不同表示。 通过KL散度的链式法则，更新后的条件分布保持不变： \\[p(\\theta|D) = q(\\theta|D)\\] 然而，关于参数的边际信念则会发生变化： \\[p(\\theta) = \\int dD \\, p(D)q(\\theta|D)= ∫dDδ(D−D_0​)q(θ∣D)=q(θ∣D=D_0​)\\] 这正是我们在观察到数据时，从先验信念更新得到的后验分布 这一更新过程的一个自然扩展是，如果我们有额外的测量误差，而这些误差是可理解的，那么我们不应将更新的信念完全依赖于观察数据的狄拉克函数，而应使用一个我们理解的分布\\(p(D)\\)。例如，我们可能不确切知道数据的精确值，但相信经过测量后，它呈现某种均值和标准差的高斯分布。 通过这种方式，参数的条件分布保持不变，但参数的边际概率则更新为： \\[p(\\theta) = \\int dD \\, p(D)q(\\theta|D)\\] 这个贝叶斯规则的广义形式有时被称为Jeffrey的条件化规则。 7. KL散度与指数族 对于一个具有自然参数\\(\\eta\\)、基础测度\\(h(x)\\)和充分统计量\\(T(x)\\)的指数族分布\\(p(x)\\)表示为： \\[p(x) = h(x) \\exp\\left[\\eta^T T(x) - A(\\eta)\\right]\\] 其中，\\(A(\\eta) = \\log Z\\)是对数分区函数，定义为： \\[A(\\eta) = \\log \\int h(x) \\exp(\\eta^T T(x)) dx\\] 对于同一指数族中的两个分布\\(p(x|\\eta_1)\\)和\\(p(x|\\eta_2)\\)，KL散度的闭式解为： \\[D_{KL}(p(x|\\eta_1) \\| p(x|\\eta_2)) = E_{\\eta_1}\\left[(\\eta_1 - \\eta_2)^T T(x) - A(\\eta_1) + A(\\eta_2)\\right]\\] 或者用期望表示为： \\[= (\\eta_1 - \\eta_2)^T \\mu_1 - A(\\eta_1) + A(\\eta_2)\\] 其中\\(\\mu_j = E_{\\eta_j}[T(x)]\\)。 两个高斯分布的KL散度 对于两个多元高斯分布\\(N(x|\\mu_1, \\Sigma_1)\\)和\\(N(x|\\mu_2, \\Sigma_2)\\)，KL散度为： \\[D_{KL}(N(x|\\mu_1, \\Sigma_1) \\| N(x|\\mu_2, \\Sigma_2)) = \\frac{1}{2}\\left[\\text{tr}(\\Sigma_2^{-1} \\Sigma_1) + (\\mu_2 - \\mu_1)^T \\Sigma_2^{-1} (\\mu_2 - \\mu_1) - D + \\log\\left(\\frac{\\det(\\Sigma_2)}{\\det(\\Sigma_1)}\\right)\\right]\\] 在标量情况下： \\[D_{KL}(N(x|\\mu_1, \\sigma_1) \\| N(x|\\mu_2, \\sigma_2)) = \\log\\left(\\frac{\\sigma_2}{\\sigma_1}\\right) + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}\\] 8. 使用Fisher信息矩阵近似KL散度 设\\(p_\\theta(x)\\)和\\(p_{\\theta&#39;}(x)\\)是两个分布，其中\\(\\theta&#39; = \\theta + \\delta\\)。KL散度可以表示为： \\[D_{KL}(p_\\theta \\| p_{\\theta&#39;}) = E_{p_\\theta(x)}\\left[\\log p_\\theta(x) - \\log p_{\\theta&#39;}(x)\\right]\\] 使用二阶泰勒展开进行近似： \\[D_{KL}(p_\\theta \\| p_{\\theta&#39;}) \\approx -\\delta^T E\\left[\\nabla \\log p_\\theta(x)\\right] - \\frac{1}{2} \\delta^T E\\left[\\nabla^2 \\log p_\\theta(x)\\right] \\delta\\] 由于期望评分函数为零（第一项为0），我们得到： \\[D_{KL}(p_\\theta \\| p_{\\theta&#39;}) \\approx \\frac{1}{2} \\delta^T F(\\theta) \\delta\\] 其中，Fisher信息矩阵定义为： \\[F = -E\\left[\\nabla^2 \\log p_\\theta(x)\\right] = E\\left[(\\nabla \\log p_\\theta(x))(\\nabla \\log p_\\theta(x))^T\\right]\\] 9. 与Bregman散度的关系 设\\(f : \\Omega \\to \\mathbb{R}\\)是一个在闭凸集\\(\\Omega\\)上连续可微且严格凸的函数，Bregman散度定义为： \\[B_f(w \\| v) = f(w) - f(v) - (w - v)^T \\nabla f(v)\\] 这个定义可以理解为一个非线性度量，度量了点\\(w\\)与点\\(v\\)之间的距离。 引入函数的线性近似： \\[\\hat{f}_v(w) = f(v) + (w - v)^T \\nabla f(v)\\] Bregman散度实际上是实际值与线性近似值之间的差： \\[B_f(w \\| v) = f(w) - \\hat{f}_v(w)\\] 由于\\(f\\)是凸函数，因此有\\(B_f(w \\| v) \\geq 0\\)。 如果\\(f(w) = \\|w\\|^2\\)，则\\(B_f(w \\| v) = \\|w - v\\|^2\\)表示平方欧几里得距离。 如果\\(f(w) = w^T Q w\\)，则Bregman散度表示为平方Mahalanobis距离。 如果\\(w\\)是指数族分布的自然参数，且\\(f(w) = \\log Z(w) = A(w)\\)，则Bregman散度等同于KL散度。 \\[\\begin{align*} B_f(\\eta_q \\| \\eta_p) &amp;= A(\\eta_q) - A(\\eta_p) - (\\eta_q - \\eta_p)^T \\nabla_{\\eta_p} A(\\eta_p) \\\\ &amp;= A(\\eta_q) - A(\\eta_p) - (\\eta_q - \\eta_p)^T E_p[T(x)] \\\\ &amp;= D_{KL}(p \\| q) \\end{align*}\\] 二、熵 1. 离散随机变量的熵 离散随机变量\\(X\\)的熵\\(H(X)\\)定义为： \\[H(X) \\equiv -\\sum_{k=1}^{K} p(X = k) \\log p(X = k) = -E_X[\\log p(X)]\\] 熵也可以用不同的对数底数表示，常用的是底数为 2（单位为 bits）或底数为\\(e\\)（单位为 nats）。熵可以表示为与均匀分布之间的 KL 散度的关系： \\[H(X) = \\log K - D_{KL}(p(X) \\| u(X))\\] 其中与均匀分布\\(u(X)\\)的 KL 散度为： \\[D_{KL}(p(X) \\| u(X)) = \\sum_{k=1}^{K} p(X = k) \\log p(X = k) - \\frac{1}{K}\\] 因此，如果\\(p\\)是均匀分布，KL 散度为零，熵达到最大值\\(\\log K\\)。 特别的，对于二元随机变量\\(X \\in \\{0, 1\\}\\)，我们可以表示为： \\[H(X) = -[p(X = 1) \\log p(X = 1) + p(X = 0) \\log p(X = 0)]\\] 即： \\[H(X) = -[\\theta \\log \\theta + (1 - \\theta) \\log(1 - \\theta)]\\] 这被称为二元熵函数\\(H(\\theta)\\)。 2. 连续随机变量的微分熵 对于连续随机变量\\(X\\)及其概率密度函数\\(p(x)\\)，微分熵定义为： \\[h(X) \\equiv -\\int_X dx \\, p(x) \\log p(x)\\] 例如，\\(d\\)维高斯分布的熵为： \\[h(N(\\mu, \\Sigma)) = \\frac{1}{2} \\log |2\\pi e \\Sigma| = \\frac{d}{2} + \\frac{d}{2} \\log(2\\pi) + \\frac{1}{2} \\log |\\Sigma|\\] 在一维情况下： \\[h(N(\\mu, \\sigma^2)) = \\frac{1}{2} \\log(2\\pi e \\sigma^2)\\] 需要注意的是，与离散情况不同，微分熵可以为负值，因为概率密度函数可以大于1。 微分熵可以通过量化的有限精度来理解。可以证明，对于连续随机变量\\(X\\)的\\(n\\)位量化，熵近似为： \\[h(X) + n\\] 以均匀分布为例子： 此外，微分熵缺乏重参数不变性。例如，如果我们变换随机变量\\(y = f(x)\\)，熵将会变换： \\[p(y) dy = p(x) dx \\Rightarrow p(y) = p(x) \\left|\\frac{dy}{dx}\\right|^{-1}\\] 因此，微分熵变换为： \\[h(X) = -\\int dx \\, p(x) \\log p(x) = h(Y) - \\int dy \\, p(y) \\log \\left|\\frac{dy}{dx}\\right|\\] 这意味着，即使是简单的单位转换也会改变微分熵的值。 3 典型集 概率分布的典型集是信息内容接近于从该分布随机样本的期望信息内容的元素集合。更具体地，对于支持\\(x \\in X\\)的分布\\(p(x)\\)，定义\\(\\epsilon\\)-典型集\\(A^{N}_{ϵ} \\subseteq X^N\\)为： \\[H(p(x)) - ϵ \\leq -\\frac{1}{N} \\log p(x_1, \\ldots, x_N) \\leq H(p(x)) + ϵ\\] 如果我们假设\\(p(x_1, \\ldots, x_N) = \\prod_{n=1}^{N} p(x_n)\\)，则中间项可以解释为 N-sample 的经验熵估计。渐近均分性质（AEP）表明，随着\\(N \\to \\infty\\)，这个值（在概率上）会收敛到真实的熵。因此，典型集的概率接近于 1，从而成为从\\(p(x)\\)生成的结果的compact summary。 4. 交叉熵和困惑度 模型分布\\(q\\)与真实分布\\(p\\)之间距离的标准方法是 KL 散度： \\[D_{KL}(p \\| q) = \\sum_{x} p(x) \\log \\frac{p(x)}{q(x)} = H_{ce}(p, q) - H(p)\\] 其中，交叉熵\\(H_{ce}(p, q)\\)定义为： \\[H_{ce}(p, q) = -\\sum_{x} p(x) \\log q(x)\\] 而\\(H(p) = H_{ce}(p, p)\\)是熵，它是与模型无关的常数。 在语言建模中，通常报告的替代性能度量称为困惑度，定义为： \\[\\text{perplexity}(p, q) \\equiv 2^{H_{ce}(p, q)}\\] 可以通过以下方式计算交叉熵的经验近似： 假设我们用基于从\\(p\\)中采样的数据的经验分布来近似真实分布： \\[p_D(x|D) = \\frac{1}{N} \\sum_{n=1}^{N} I(x = x_n)\\] 在这种情况下，交叉熵为： \\[H = -\\frac{1}{N} \\sum_{n=1}^{N} \\log p(x_n) = -\\frac{1}{N} \\log \\prod_{n=1}^{N} p(x_n)\\] 相应的困惑度为： \\[\\text{perplexity}(p_D, p) = 2^{-\\frac{1}{N} \\log\\left(\\prod_{n=1}^{N} p(x_n)\\right)} = \\left(\\prod_{n=1}^{N} p(x_n)\\right)^{-\\frac{1}{N}} = \\sqrt[N]{\\prod_{n=1}^{N} \\frac{1}{p(x_n)}}\\] 在语言模型中，我们通常在预测下一个单词时考虑前面的单词。例如，在二元模型中，我们使用二阶马尔可夫模型的形式\\(p(x_n|x_{n-1})\\)。假设模型预测每个单词是同样可能的，而与上下文无关，即\\(p(x_n|x_{n-1}) = \\frac{1}{K}\\)，其中\\(K\\)是词汇表中的单词数量。此时，困惑度为： \\[\\left(\\left(\\frac{1}{K}\\right)^{N}\\right)^{-\\frac{1}{N}} = K\\] 如果某些符号比其他符号更可能，且模型正确反映了这一点，则其困惑度将低于\\(K\\)。然而，我们有\\(H(p^*) \\leq H_{ce}(p^*, p)\\), 因此我们无法将困惑度降低到\\(2^{-H(p^*)}\\)之下。 三、 互信息 KL散度告诉了我们如何衡量两个分布之间的区别，而两个分布之间的相关性则需要通过互信息来进行衡量。 1. 定义 互信息\\(I(X; Y)\\)衡量了随机变量\\(X\\)和\\(Y\\)之间的信息增益，定义为： \\[I(X; Y) \\equiv D_{KL}(p(x, y) \\| p(x)p(y)) = \\sum_{y \\in Y} \\sum_{x \\in X} p(x, y) \\log \\frac{p(x, y)}{p(x)p(y)}\\] 很容易看出，互信息是非负的： \\[I(X; Y) = D_{KL}(p(x, y) \\| p(x)p(y)) \\geq 0\\] 当且仅当\\(p(x, y) = p(x)p(y)\\)时，等号成立。 2. 解释 互信息可以用联合熵和条件熵重新表示： \\[I(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)\\] 这表明观察\\(Y\\)后，关于\\(X\\)的不确定性减少的值，反之亦然。此外，互信息还可以表示为： \\[I(X; Y) = H(X, Y) - H(X|Y) - H(Y|X)\\] 或者： \\[I(X; Y) = H(X) + H(Y) - H(X, Y)\\] 具体关系可以通过下面的图很直观的展示出来： 3 数据处理不等式 数据处理不等式表明，对于未知变量\\(X\\)，如果我们观察到它的噪声函数\\(Y\\)，并对噪声观测进行处理以生成新变量\\(Z\\)，则对未知量\\(X\\)的信息量不会增加。这可以形式化为： \\[I(X; Y) \\geq I(X; Z)\\] 证明如下： \\[I(X;Y,Z)=I(X;Z)+I(X;Y∣Z)=I(X;Y)+I(X;Z∣Y)\\] 由于\\(X \\perp Z | Y\\)，我们有\\(I(X; Z | Y) = 0\\)，因此： \\[I(X; Z) + I(X; Y | Z) = I(X; Y)\\] 因为\\(I(X; Y | Z) \\geq 0\\)，所以： \\[I(X; Y) \\geq I(X; Z)\\] 数据处理不等式（Data Processing Inequality, DPI）表明，在一个马尔可夫链\\(X \\to Y \\to Z\\)中，关于\\(X\\)的信息在通过\\(Y\\)传递到\\(Z\\)的过程中不会增加。 4. 充分统计量 设有链关系\\(\\theta \\to X \\to s(X)\\)，其中\\(\\theta\\)是待推断的参数，\\(X\\)是观测数据，\\(s(X)\\)是从数据中提取的统计量。通过数据处理不等式，得出\\(I(\\theta; s(X)) \\leq I(\\theta; X)\\)。这意味着，通过\\(s(X)\\)获取的信息不可能超过通过\\(X\\)获取的信息。 当不等式成立为等式时，称\\(s(X)\\)是\\(X\\)的充分统计量。也就是说，知道\\(s(X)\\)就足够推断\\(\\theta\\)，并且从\\(s(X)\\)可以重建\\(X\\)。 如果统计量\\(s(X)\\)包含关于\\(\\theta\\)的所有相关信息，并且不包含冗余信息，则称\\(s(X)\\)是最小充分统计量。它最大限度地压缩数据，而不丢失与推断\\(\\theta\\)相关的信息。 例如，对于\\(N\\)次伯努利试验，最小充分统计量是\\(N\\)和成功次数\\(N_1 = n\\)（即\\(I(X_n = 1)\\)）。对于已知方差的高斯分布，推断均值只需知道经验均值和样本数量。 指数族分布是最小充分统计量的代表，因为它们包含的信息仅限于某些统计量的约束。根据 Pitman-Koopman-Darmois 定理，只有在样本数量增加时，才存在具有有限维充分统计量的指数族分布。 5. 多元互信息 多元互信息主要用来衡量一组随机变量之间的相关性。 5.1. 总相关性（Total Correlation） 多元互信息最简单的定义之一是使用总相关性（Total Correlation）或多信息（Multi-Information），定义为： \\[TC(\\{X_1, \\dots, X_D\\}) ≜ D_{KL} \\left( p(x) \\middle\\| \\prod_{d} p(x_d) \\right)\\] 其等价表达式为： \\[TC(\\{X_1, \\dots, X_D\\}) = \\sum_{d} H(x_d) - H(x)\\] 其中，\\(H(x)\\)是联合熵，\\(H(x_d)\\)是边际熵。对三个变量\\(X, Y, Z\\)，总相关性为： \\[TC(X, Y, Z) = H(X) + H(Y) + H(Z) - H(X, Y, Z)\\] 联合熵\\(H(X,Y,Z)\\)的定义是： \\[H(X, Y, Z) = - \\sum_{x, y, z} p(x, y, z) \\log p(x, y, z)\\] 总相关性描述了所有变量之间的整体依赖关系。如果\\(p(x) = \\prod_d p(x_d)\\)，即变量是独立的，那么总相关性为零。即使只有一对变量是相互依赖的，总相关性也会为正。 5.2. 相互作用信息（Interaction Information, Co-Information） 为了克服总相关性只要任意两个变量相互作用就为正的缺点，可以引入多元互信息（multivariate mutual information (MMI)），也叫做相互作用信息（Interaction Information）或者共信息（Co-Information）。这个定义基于条件互信息的递归定义： \\[I(X_1; \\dots ; X_D) = I(X_1; \\dots ; X_{D-1}) - I(X_1; \\dots ; X_{D-1} | X_D)\\] 对于三个变量\\(X, Y, Z\\)，其定义为： \\[I(X; Y; Z) = I(X; Y) - I(X; Y | Z)\\] 这可以解释为：当条件变量\\(Z\\)已知时，互信息\\(I(X;Y)\\)的变化。同样，可以写成： \\[I(X; Y; Z) = I(X; Z) - I(X; Z | Y)\\] \\[I(X; Y; Z) =I(Y; Z) - I(Y; Z | X)\\] 这意味着：相互作用信息是两个变量之间的互信息在条件第三个变量已知时的变化。 根据条件互信息的定义，可以推导出： \\[I(X; Z | Y) = I(Z; X, Y) - I(Y; Z)\\] 因此，式子可以重新写成： \\[I(X; Y; Z) = I(X; Z) + I(Y; Z) - I(X, Y; Z)\\] 这说明，相互作用信息是变量\\(X\\)和\\(Y\\)单独提供的关于\\(Z\\)的信息与它们联合提供的信息之间的差异。 上面的关系可以通过下面的图来直观展示： 5.3. 协同与冗余（Synergy and Redundancy） 相互作用信息\\(I(X; Y; Z)\\)可以为正、零或负，取决于变量之间的关系： 如果\\(I(X; Z) + I(Y; Z) &gt; I(X, Y; Z)\\)，说明\\(X\\)和\\(Y\\)提供了关于\\(Z\\)的冗余信息，此时\\(I(X; Y; Z) &gt; 0\\)。 如果\\(I(X; Z) + I(Y; Z) &lt; I(X, Y; Z)\\)，说明联合观察\\(X\\)和\\(Y\\)提供了额外的关于\\(Z\\)的信息，存在协同作用，此时\\(I(X; Y; Z) &lt; 0\\)。 5.4. 相互作用信息与因果关系（MMI and Causality） MMI 的符号可以用于区分不同的有向图模型，这些模型有时可以用来解释因果关系。例如： 共同原因模型（Common Cause Model）：例如\\(X \\leftarrow Z \\rightarrow Y\\)，其中\\(Z\\)是\\(X\\)和\\(Y\\)的共同原因。此时，条件化在\\(Z\\)上会使\\(X\\)和\\(Y\\)独立，因此\\(I(X; Y | Z) \\leq I(X; Y)\\)，所以\\(I(X; Y; Z) \\geq 0\\)。 共同结果模型（Common Effect Model）：例如\\(X \\rightarrow Z \\leftarrow Y\\)，其中\\(Z\\)是\\(X\\)和\\(Y\\)的共同结果。此时，条件化在\\(Z\\)上会使\\(X\\)和\\(Y\\)相关，因此\\(I(X; Y | Z) \\geq I(X; Y)\\)，所以\\(I(X; Y; Z) \\leq 0\\)。 同样，对于\\(X \\rightarrow Z \\rightarrow Y\\)，此时，条件化在\\(Z\\)上会使\\(X\\)和\\(Y\\)独立，因此\\(I(X; Y | Z) \\leq I(X; Y)\\)，所以\\(I(X; Y; Z) \\geq 0\\)。 5.5. 相互作用信息与熵的关系（MMI and Entropy） MMI 还可以用熵来表达。我们知道： \\[I(X; Y) = H(X) + H(Y) - H(X, Y)\\] 和 \\[I(X; Y | Z) = H(X, Z) + H(Y, Z) - H(Z) - H(X, Y, Z)\\] 因此，可以将\\(I(X; Y; Z)\\)重新写为： \\[I(X; Y; Z) = [H(X) + H(Y) + H(Z)] - [H(X, Y) + H(X, Z) + H(Y, Z)] + H(X, Y, Z)\\] 跟集合比较相似。对于多个变量，MMI 可以推广为： \\[I(X_1, \\dots, X_D) = - \\sum_{T \\subseteq \\{1, \\dots, D\\}} (-1)^{|T|} H(T)\\] 对于大小为 1, 2 和 3 的变量集，其展开为： \\[I_1 = H_1\\] \\[I_{12} = H_1 + H_2 - H_{12}\\] \\[I_{123} = H_1 + H_2 + H_3 - H_{12} - H_{13} - H_{23} + H_{123}\\] 通过 Möbius 反演公式，可以得到熵和互信息的对偶关系： \\[H(S) = - \\sum_{T \\subseteq S} (-1)^{|T|} I(T)\\] 使用链式规则，我们还可以得到 3 变量互信息的另一种形式： \\[I(X; Y; Z) = H(Z) - H(Z | X) - H(Z | Y) + H(Z | X, Y)\\] 6. 互信息的变分界限 在直接计算互信息不可行的情况下，我们有一些估计其上界和下界的方法。 6.1 上界（Upper Bound） 假设联合分布\\(p(x, y)\\)难以直接计算，但我们可以从\\(p(x)\\)采样，并且能够计算条件分布\\(p(y|x)\\)。同时我们用\\(q(y)\\)来近似\\(p(y)\\)，此时互信息可以表示为： \\[I(x; y) = \\mathbb{E}_{p(x,y)}\\left[ \\log \\frac{p(y|x)}{q(y)} \\right] - D_{KL}(p(y) \\parallel q(y))\\] 通过去掉KL散度项，我们得到一个互信息的上界： \\[I(x; y) \\leq \\mathbb{E}_{p(x)}\\left[ D_{KL}(p(y|x) \\parallel q(y)) \\right]\\] 该上界是当\\(q(y) = p(y)\\)时等号成立。 可以这么理解，互信息\\(I(Y; X) = H(Y) - H(Y | X)\\)，我们假设已知\\(p(y|x)\\)，因此可以很好地估计条件熵\\(H(Y | X)\\)。然而，关于边缘熵\\(H(Y)\\)，我们无法直接知道，所以我们用某个分布\\(q(y)\\)来对其进行上界估计。由于KL散度是非负的，因此我们的模型\\(q(y)\\)无法比真实的\\(p(y)\\)表现得更好，这意味着我们对\\(H(Y)\\)的估计总是偏大的，从而得到了互信息的估计的一个上界。 6.2 BA下界（BA Lower Bound） 假设我们可以计算\\(p(x)\\)，并且用\\(q(x|y)\\)来近似\\(p(x|y)\\)。那么互信息的变分下界为： \\[\\begin{align*} I(x; y) &amp;= \\mathbb{E}_{p(x,y)} \\left[ \\log \\frac{p(x|y)}{p(x)} \\right] \\\\ &amp;= \\mathbb{E}_{p(x,y)} \\left[ \\log \\frac{q(x|y)}{p(x)} \\right] + \\mathbb{E}_{p(y)} \\left[ D_{\\mathrm{KL}} (p(x|y) \\parallel q(x|y)) \\right] \\\\ &amp;\\geq \\mathbb{E}_{p(x,y)} \\left[ \\log \\frac{q(x|y)}{p(x)} \\right] \\\\ &amp;= \\mathbb{E}_{p(x,y)} \\left[ \\log q(x|y) \\right] + h(x) \\end{align*}\\] 这首先被Barber和Agakov提出，所以叫做BA下界。它的关键思想是使用容易计算的\\(q(x|y)\\)来近似\\(p(x|y)\\)，从而给出互信息的下界。 6.3 NWJ下界（NWJ Lower Bound） NWJ下界是通过重新参数化\\(q(x|y)\\)得到的。我们可以设： \\[q(x|y) = \\frac{p(x)e^{f(x,y)}}{Z(y)}\\] 其中\\(Z(y) = \\mathbb{E}_{p(x)} \\left[ e^{f(x,y)} \\right]\\)是归一化常数。这样我们可以得到新的变分下界： \\[\\begin{align*} \\mathbb{E}_{p(x,y)} \\left[ \\log \\frac{p(x) e^{f(x,y)}}{p(x) Z(y)} \\right] &amp;= \\mathbb{E}_{p(x,y)} \\left[ f(x, y) \\right] - \\mathbb{E}_{p(y)} \\left[ \\log Z(y) \\right] \\\\ &amp;= \\mathbb{E}_{p(x,y)} \\left[ f(x, y) \\right] - \\mathbb{E}_{p(y)} \\left[ \\log \\mathbb{E}_{p(x)} \\left[ e^{f(x,y)} \\right] \\right] \\\\ &amp;\\equiv I_{\\text{DV}}(X; Y) \\end{align*}\\] 这被叫做Donsker-Varadhan下界，为了进一步简化，可以用线性上界来逼近log函数 \\[\\log x \\leq \\frac{x}{a} + \\log a - 1\\] 最终得到NWJ下界的形式如下： \\[I(X; Y) \\geq \\mathbb{E}_{p(x,y)} \\left[ f(x, y) \\right] - e^{-1} \\mathbb{E}_{p(y)} Z(y) \\equiv I_{\\text{NWJ}}(X; Y)\\] 这个下界不需要归一化的分布（实践中我们使用蒙特卡洛采样估计就行），因此更易于计算。 6.4 InfoNCE下界（InfoNCE Lower Bound） 如果我们对DV下界进行多样本扩展，则可以得到InfoNCE下界。即对于每个样本\\((x_i, y_i)\\)，我们不只考虑它本身的联合分布，而是通过和其他样本进行对比来估计 mutual information。 假设我们有\\(K\\)对样本\\(\\{(x_i, y_i)\\}_{i=1}^K\\)​ ，其中\\(x_i\\)​ 和\\(y_i\\)来自联合分布\\(p(x, y)\\)，而\\(y_j\\)（对于\\(i \\neq j\\)）来自边缘分布\\(p(y)\\)。为每一对\\((x_i, y_i)\\)，我们希望通过对比它与其他负样本（\\((x_i, y_j)\\)其中\\(j \\neq i\\)）来计算 mutual information。最终其形式为： \\[I_{NCE} = \\mathbb{E} \\left[ \\frac{1}{K} \\sum_{i=1}^{K} \\log \\frac{e^{f(x_i, y_i)}}{\\frac{1}{K} \\sum_{j=1}^{K} e^{f(x_i, y_j)}} \\right]\\] 其中，\\(f(x_i, y_i)\\)是可学习的判别函数，表示\\(x_i\\)​ 和\\(y_i\\)​ 的关联性。这样便通过对比联合分布和边缘分布的采样来估计互信息。InfoNCE下界的一个缺点是，如果互信息较大，则需要较大的样本数\\(K\\)来得到准确的估计。 7. 相关网络（Relevance Networks） 相关网络是通过一组相关变量来构建的图网络。在这个网络中，如果两个变量\\(X_i\\)和\\(X_j\\)之间的互信息高于某个阈值，就会在它们之间添加一条边。这种方法可以应用于高斯分布情况下的变量，也可以扩展到离散随机变量。 在高斯分布情况下，两个变量之间的互信息可以通过它们的相关系数（Correlation Coefficient）计算，公式为： \\[I(X_i ; X_j ) = -\\frac{1}{2} \\log(1 - \\rho_{ij}^2)\\] 其中\\(\\rho_{ij}\\)是\\(X_i\\)和\\(X_j\\)的相关系数。这种情况下的图称为协方差图（Covariance Graph）。 相关网络有一个主要问题：它通常会产生非常稠密的图。因为大多数变量在一定程度上都会与其他大多数变量相互依赖，即使我们对互信息进行了阈值处理，图中仍可能存在大量的边。例如： 假设\\(X_1\\)​ 直接影响\\(X_2\\)​，而\\(X_2\\)​ 又直接影响\\(X_3\\)​，此时它们组成了一个信号传导，\\(X_1 \\to X_2 \\to X_3\\)。在这种情况下，\\(X_1\\)和\\(X_3\\)​ 之间也会有非零的互信息，因此会有一条\\(1-3\\)的边，虽然它们之间没有直接的依赖关系。这样，图可能会在很多情况下甚至会变得完全连接。 四. 数据压缩（源编码） 数据压缩是信息理论的核心内容之一，也与概率机器学习密切相关。其基本思想是，我们对数据样本的不同种类建模，并且能够为出现频率最高的那些种类分配短的编码字，为出现频率较低的种类保留更长的编码。因此，数据压缩的能力需要发现数据中的潜在模式及其相对频率。 1. 无损压缩（Lossless Compression） 对于离散数据（如自然语言），总是可以以一种方式进行压缩，使我们能够唯一地恢复原始数据。这称为无损压缩。克劳德·香农证明了，从分布\\(p\\)中获取的数据所需的平均比特数至少为\\(H(p)\\)，这被称为源编码定理。 根据源编码定理，期望所需的比特数\\(R\\)满足以下不等式： \\[R \\geq H(p)\\] 其中\\(H(p)\\)是分布\\(p\\)的熵。使用非真实模型\\(q\\)进行压缩会导致多出的比特数，这正是 KL 散度的非负性所表明的。具体地， \\[H_{ce}(p, q) \\geq H(p)\\] 其中\\(H_{ce}(p, q)\\)是使用模型\\(q\\)对数据进行压缩时的交叉熵。 常见的无损编码技术包括 Huffman 编码、算术编码和非对称数字符号系统（Asymmetric Numeral Systems）等。 2. 有损压缩与率失真权衡（Lossy Compression and the Rate-Distortion Tradeoff） 对于实值信号（如图像和声音），首先需要对信号进行量化，得到一系列符号。然后可以使用无损编码方法来压缩这些离散符号序列。但是，在解压缩时会丢失一些信息，因此这种方法称为有损压缩。 有损压缩中存在一个重要的权衡关系，即压缩表示的大小（使用的符号数量）和由此产生的误差之间的权衡。我们使用了变分信息瓶颈（Variational Information Bottleneck）的术语来量化这种权衡，尤其是在无监督设置下。 Distortion\\(D\\)的定义： 我们可以通过编码解码模型，假设有一个随机编码器\\(e(z|x)\\)、一个随机解码器\\(d(x|z)\\)和一个先验边际\\(m(z)\\),我们可以得到最终损失信息，即对失真的量化\\(D\\)如下： \\[D = - \\int p(x) \\int e(z|x) \\log d(x|z) \\, dz \\, dx\\] 如果解码器\\(d(x|z)\\)是一个确定性模型加上高斯噪声\\(N(x | f_d(z), \\sigma^2)\\)，而编码器\\(e(z|x)\\)是确定性的\\(\\delta(z - f_e(x))\\)，那么可以简化为： \\[D = \\frac{1}{\\sigma^2} \\mathbb{E}_{p(x)} \\| f_d(f_e(x)) - x \\|_2^2\\] Rate\\(R\\)的定义： \\[\\begin{align*} R &amp;= \\int p(x) \\int e(z|x) \\log \\frac{e(z|x)}{m(z)} \\, dz \\, dx \\\\&amp;=\\mathbb{E}_{p(x)}[ H_{ce}​(e(z|x),m(z))−H(e(z|x))] \\end{align*}\\] 这里\\(m(z)\\)是先验边际分布。通过 KL 散度的平均来定义了这个率： \\[R = \\mathbb{E}_{p(x)} \\left[ D_{KL}(e(z|x) \\| m(z)) \\right]\\] 这反映的是\\(z\\)后验分布与\\(z\\)的边缘分布的区别。如果我们使用\\(m(z)\\)来设计一个最优编码，那么\\(R\\)就是使用\\(m(z)\\)而不是实际的后验分布\\(p(z) = \\int dx \\, p(x)e(z|x)\\)进行编码时所需支付的额外比特数。 Rate-Distortion Bounds： 根据前面提到的互信息的变分界限（BA下界和上界），我们有以下不等式： \\[H - D \\leq I(x; z) \\leq R\\] 其中\\(H\\)是（微分）熵。 Rate-Distortion Curve： 可实现的\\(R\\)和\\(D\\)值形成的曲线称为率失真曲线（Rate-Distortion Curve）。这条曲线展示了在不同\\(R\\)和\\(D\\)条件下的可达值。例如，当\\(D = 0\\)时，我们可以完美地编码和解码数据，此时\\(R \\geq H\\)，即所需的最小比特数为数据的熵。 水平的底线对应于零失真设置，即\\(D = 0\\)。这可以通过使用\\(e(z|x) = \\delta(z - x)\\)的简单编码器来实现。 左侧的垂直线对应于零率设置，即潜变量与\\(z\\)独立。在这种情况下，解码器\\(d(x|z)\\)与\\(z\\)无关。即此时\\(I(x; z)\\)为0。这种模型能够达到的最小失真仍然是数据的熵，即\\(D \\geq H\\)。 使用互信息的变分界限可以更精确地刻画率和失真之间的权衡关系。实际上，我们无法在对角线上达到点，因为这要求在不等式中均取等号，即需要我们的模型\\(e(z|x)\\)和\\(d(x|z)\\)是完美的，这被称为“非参数限制”。在有限数据设置中，我们总会产生额外的误差，因此 RD 曲线会向上偏移。 我们可以通过最小化以下目标函数\\(J\\)来生成不同的解决方案： \\[J = D + \\beta R\\] 其中：\\(D\\)是失真（distortion），表示编码与解码之间的误差。\\(R\\)是rate，表示编码所需的比特数。\\(\\beta\\)是一个权重超参数，用于调节失真和rate之间的平衡。 目标函数可以展开为： \\[J = \\int dx \\, p(x) \\int dz \\, e(z|x) \\left( -\\log d(x|z) + \\beta \\log \\frac{e(z|x)}{m(z)} \\right)\\] 当\\(\\beta = 1\\)时，公式与变分自编码器（VAE）目标一致: \\[\\mathcal{L} = - (D + R) = E_{p(x)} \\left[ E_{e(z|x)}[\\log d(x|z)] - E_{e(z|x)} \\left[ \\log \\frac{e(z|x)}{m(z)} \\right] \\right]\\] 3. 信息瓶颈（The information bottleneck） 3.1 原版信息瓶颈（Vanilla IB） 信息瓶颈（The information bottleneck）的目标是通过引入一个中间表示\\(z\\)，在输入\\(x\\)和输出\\(y\\)之间传递信息。我们说\\(z\\)是\\(x\\)的表示，可以用条件分布\\(p(z|x)\\)来描述。 充分性：表示\\(z\\)对于任务\\(y\\)是充分的，条件为\\(y \\perp x | z\\)，或等价于\\(I(z; y) = I(x; y)\\)，即\\(H(y|z) = H(y|x)\\)。 最小充分统计量：如果\\(z\\)是充分的，并且没有其他\\(z\\)具有更小的\\(I(z; x)\\)值，则称其为最小充分统计量。 目标是找到一个表示\\(z\\)，使得\\(I(z; y)\\)最大化，同时\\(I(z; x)\\)最小化，即优化以下目标： \\[\\min_\\beta I(z; x) - I(z; y)\\] 其中\\(\\beta \\geq 0\\)，优化的分布为\\(p(z|x)\\)和\\(p(y|z)\\)。 在信息瓶颈原理中，我们假设\\(Z\\)是\\(X\\)的一个函数，但与\\(Y\\)独立，形成图模型\\(Z \\leftarrow X \\rightarrow Y\\)。这对应于以下联合分布： \\[p(x, y, z) = p(z|x) p(y|x) p(x)\\] 这表明\\(Z\\)可以捕捉\\(X\\)的任何信息，但不能包含仅与\\(Y\\)相关的信息。优化的表示只捕获对\\(Y\\)有用的关于\\(X\\)的信息，并且\\(Z\\)应该最小化对\\(X\\)的信息，以避免“浪费容量”。 如果所有随机变量都是离散的，并且\\(z = e(x)\\)是\\(x\\)的确定性函数，则可以使用传统算法来最小化信息瓶颈目标。如果所有变量都是联合高斯的，目标也可以通过解析求解。但一般情况下，精确求解该问题是不可行的。 3.2. 变分信息瓶颈（Variational IB） 根据 KL 散度的非负性，我们介绍到对于任意分布\\(q\\)，有： \\[\\int dx p(x) \\log p(x) \\geq \\int dx p(x) \\log q(x)\\] 首先定义符号： \\(e(z|x) = p(z|x)\\)：编码器 \\(b(z|y) \\approx p(z|y)\\)：反向编码器 \\(d(y|z) \\approx p(y|z)\\)：分类器（解码器） \\(m(z) \\approx p(z)\\)：边际分布 推导\\(I(z; y)\\)： \\[\\begin{align*} I(z; y) &amp;= \\int \\! dydz \\, p(y, z) \\log \\frac{p(y, z)}{p(y)p(z)} \\\\ &amp;= \\int \\! dydz \\, p(y, z) \\log p(y|z) - \\int \\! dydz \\, p(y, z) \\log p(y) \\\\ &amp;= \\int \\! dydz \\, p(z)p(y|z) \\log p(y|z) - \\text{const} \\\\ &amp;\\geq \\int \\! dydz \\, p(y, z) \\log d(y|z) \\\\ &amp;= \\langle \\log d(y|z) \\rangle \\end{align*}\\] 这里利用了\\(H(p(y))\\)是与表示无关的常量。符号\\(\\langle \\cdot \\rangle\\)表示对联合分布\\(p(x, y, z)\\)相关项的期望值 随后，通过从\\(p(y, z) = \\int dx \\, p(x)p(y|x)p(z|x)\\)采样，近似求期望值。 同样地，我们可以推导\\(I(z; x)\\)的上界： \\[\\begin{align*} I(z; x) &amp;= \\int dzdx \\, p(x, z) \\log \\frac{p(z, x)}{p(x)p(z)} \\\\ &amp;= \\int dzdx \\, p(x, z) \\log p(z|x) - \\int dz \\, p(z) \\log p(z) \\\\ &amp;\\leq \\int dzdx \\, p(x, z) \\log p(z|x) - \\int dz \\, p(z) \\log m(z) \\\\ &amp;= \\int dzdx \\, p(x, z) \\log \\frac{e(z|x)}{m(z)} \\\\ &amp;= \\langle \\log e(z|x) \\rangle - \\langle \\log m(z) \\rangle \\end{align*}\\] 通过近似从\\(p(x, z) = p(x)p(z|x)\\)采样，我们可以计算期望。 综合以上结果，我们得到信息瓶颈目标函数的上界： \\[\\beta I(x; z) - I(z; y) \\leq \\beta (\\langle \\log e(z|x) \\rangle - \\langle \\log m(z) \\rangle) - \\langle \\log d(y|z) \\rangle\\] 因此，VIB 的目标函数可以表示为： \\[\\begin{align*} L_{VIB} &amp;= \\beta \\, \\mathbb{E}_{p_D(x)e(z|x)} \\left[ \\log e(z|x) - \\log m(z) \\right] - \\mathbb{E}_{p_D(x)e(z|x)d(y|z)} \\left[ \\log d(y|z) \\right] \\\\ &amp;= -\\mathbb{E}_{p_D(x)e(z|x)d(y|z)} \\left[ \\log d(y|z) \\right] + \\beta \\, \\mathbb{E}_{p_D(x)} \\left[ D_{KL} \\left( e(z|x) \\parallel m(z) \\right) \\right] \\end{align*}\\] 这个目标函数可以通过随机梯度下降（SGD）对编码器、解码器和边际分布的参数进行最小化。我们假设这些分布是可重参数化的。对于编码器\\(e(z|x)\\)，通常使用条件高斯分布；对于解码器\\(d(y|z)\\)，通常使用 softmax 分类器。对于边际分布\\(m(z)\\)，由于其需要近似聚合的后验分布\\(p(z)\\)，通常使用灵活的模型，比如高斯混合模型。 3.3 条件熵瓶颈（Conditional Entropy Bottleneck） 信息瓶颈的基本目标是最大化互信息\\(I(Z; Y)\\)的同时最小化互信息\\(I(Z; X)\\)。可以表示为： \\[\\min I(X; Z) - \\lambda I(Y; Z) \\quad (\\lambda \\geq 0)\\] 然而，从信息图的视角来看，\\(I(Z; X)\\)中包含了一些与\\(Y\\)相关的信息。一个合理的替代目标是最小化残余互信息\\(I(X; Z | Y)\\)： \\[\\min I(X; Z | Y) - \\lambda&#39; I(Y; Z) \\quad (\\lambda&#39; \\geq 0)\\] 这就是条件熵瓶颈（CEB）。 我们假设\\(p(Z | X, Y) = p(Z | X)\\)，则根据条件互信息的定义： \\[I(X; Z | Y) = I(X; Z) - I(Y; Z)\\] 因此，CEB 可以看作是标准的 IB 方法，当\\(\\lambda&#39; = \\lambda + 1\\)时，两者等价。 相较于\\(I(X; Z)\\)，\\(I(X; Z | Y)\\)的上界更容易确定，因为我们在\\(Y\\)的条件下进行计算。利用\\(p(Z | X, Y) = p(Z | X)\\)的性质，可以得出： \\[\\begin{align*} I(X; Z | Y) &amp;= I(X; Z) - I(Y; Z) \\\\ &amp;= H(Z) - H(Z | X) - [H(Z) - H(Z | Y)] \\\\ &amp;= - H(Z | X) + H(Z | Y) \\\\ &amp;= \\int dz dx \\, p(x, z) \\log p(z | x) \\\\ &amp;\\leq \\int dz dx \\, p(x, z) \\log e(z | x) - \\int dz dy \\, p(z, y) \\log b(z | y) \\\\ &amp;= \\langle \\log e(z | x) \\rangle - \\langle \\log b(z | y) \\rangle \\end{align*}\\] 结合以上推导，我们得到最终的条件熵瓶颈目标： \\[\\min \\beta \\left( \\langle \\log e(z|x) \\rangle - \\langle \\log b(z|y) \\rangle \\right) - \\langle \\log d(y|z) \\rangle\\] 学习条件反向编码器\\(b(z|y)\\)通常比学习无条件边际\\(m(z)\\)更容易。此外，我们知道，当\\(I(X; Z | Y) = I(X; Z) - I(Y; Z) = 0\\)时，此时的\\(\\beta\\)值对应于一个最优的表示。相比之下，在使用信息瓶颈（IB）时，如何衡量与最优性的距离并不明确。 五、算法信息理论 考虑一个从均匀伯努利分布独立生成的长度为\\(n\\)的比特序列。该分布每个元素的最大熵为\\(H_2(0.5) = 1\\)，因此长度为\\(n\\)的序列的编码长度为\\(- \\log_2 p(D|\\theta) = - \\sum_{i=1}^{n} \\log_2 \\text{Ber}(x_i | \\theta = 0.5) = n\\)。然而，从直观上来看，这样的序列并没有包含太多信息。 可见统的信息理论主要基于某个随机分布的性质，这种分布被假设为生成我们观察到的数据。然而，这种理论并没有很好地反映出人们对“信息”的直观理解，而算法信息理论提出了一种不同的方法来量化一个序列的信息量。 1. Kolmogorov复杂性 Kolmogorov复杂性是算法信息理论的核心概念，定义为生成特定比特字符串\\(x = x_{1:n}\\)的最短程序的长度。这一程序被输入到一个通用图灵机\\(U\\)中，以生成字符串\\(x\\)。公式定义为： \\[K(x) = \\min_{p \\in B^*} [\\ell(p) : U(p) = x]\\] \\(B^*\\)是任意长度比特字符串的集合。 \\(\\ell(p)\\)是程序\\(p\\)的长度。 Kolmogorov复杂性具有一些类似于Shannon熵的性质。我们可以忽略常数项，得到以下不等式： \\[K(x|y) \\leq K(x) \\leq K(x, y)\\] 这些不等式的解释如下： \\(K(x∣y)\\)：在已知\\(y\\)的情况下，生成\\(x\\)的最短程序长度。 \\(K(x)\\)：生成\\(x\\)的最短程序长度。 *\\(K(x, y)\\)：生成\\(x\\)和\\(y\\)的最短程序长度。 这些不等式表明，已知条件信息\\(y\\)会减少生成\\(x\\)所需的信息量。 尽管Kolmogorov复杂性是一个理论上重要的概念，但它是不可计算的。为了解决这个问题，引入了Levin复杂性： \\[L(x) = \\min_{p \\in B^*} [\\ell(p) + \\log(\\text{time}(p)) : U(p) = x]\\] *\\(\\text{time}(p)\\)是程序\\(p\\)的运行时间。 Levin复杂性可以通过“Levin搜索”或“通用搜索”来计算，即以时间片的方式运行所有程序，直到第一个程序停止。这种方法的时间复杂度为： \\[\\text{time}(LS(x)) = 2^{L(x)}\\] 虽然Levin复杂性是可计算的，但计算效率仍然较低。为此，可以通过参数化的近似来得到Kolmogorov复杂性的上界。例如，假设\\(q\\)是某个比特字符串的分布，可以证明： \\[K(x) \\leq -\\log q(x) + K(q)\\] 如果\\(q\\)是一个参数化模型，可以通过\\(q\\)参数的编码长度来近似\\(K(q)\\)。 Kolmogorov复杂性还可以用于定义序列的随机性定义，而无需使用随机变量或通信信道的概念。我们定义一个字符串\\(x\\)是可压缩的，当其最短描述长度小于字符串本身的长度： \\[K(x) &lt; \\ell(x) = n\\] 否则，称字符串为算法随机的。这种随机性定义称为Martin-Löf随机性。 例如： 字符串\\(x = (10101010 \\ldots)\\)是可压缩的，因为它是模式“10”的重复。 字符串\\(x = (11001001 \\ldots)\\)也是可压缩的，但不如前者明显，因为它是\\(\\pi^2\\)的二进制扩展。 字符串\\(x = (10110110 \\ldots)\\)被认为是“真正随机的”，因为它来源于量子波动。 基于序列的信息理论为著名的Lempel-Ziv无损数据压缩方案奠定了基础，这形成了zip编码的基础。利用Kolmogorov复杂性，我们可以定义一种普遍相似性度量： \\[d(x, y) = \\frac{\\max[K(x|y), K(y|x)]}{\\max[K(x), K(y)]}\\] 其中，诸如\\(K(x)\\)的项可以通过某种通用压缩器（如LZ）的编码成本进行近似。这导致了规范化压缩距离（NCD）的出现。 2. Solomonoff归纳 在Solomonoff归纳中，我们关注的是预测问题。假设我们观察到了一系列数据\\(x_{1:t}\\)​，这些数据来自某个未知的分布\\(\\mu(x_{1:t})\\)。我们的目标是通过某个模型\\(\\nu\\)来近似这个分布，以便预测未来的值，即\\(\\nu(x_{t+1}|x_{1:t})\\)。这被称为归纳问题。 我们假设\\(\\nu\\)属于模型集合\\(M\\)，其中\\(M\\)是一个可数模型（分布）的集合。对于每个模型\\(\\nu\\)，我们有一个先验概率\\(w_\\nu\\)​。在Solomonoff归纳中，模型集合\\(M\\)被假设为所有可计算函数的集合，先验定义为： \\[w_\\nu = 2^{-K(\\nu)}\\] 这个“通用先验”能够建模任何可计算的分布\\(\\mu\\)。这里的权重选择是基于奥卡姆剃刀原理，即我们应该偏好那些最简单的能够解释数据的模型。 给定这个先验，我们可以使用以下贝叶斯混合模型计算序列的先验预测分布： \\[\\xi(x_{1:t}) = \\sum_{\\nu \\in M} w_\\nu \\nu(x_{1:t})\\] 从这个先验预测分布出发，我们可以在时间步骤\\(t\\)计算后验预测分布： \\[\\begin{align*} \\xi(x_t|x_{&lt;t}) &amp;= \\frac{\\xi(x_{1:t})}{\\xi(x_{&lt;t})} \\\\ &amp;= \\frac{\\sum_{\\nu \\in M} w_\\nu \\nu(x_{1:t})}{\\xi(x_{&lt;t})} \\\\ &amp;= \\sum_{\\nu \\in M} w_\\nu \\frac{\\nu(x_{1:t})}{\\xi(x_{&lt;t})} \\\\ &amp;= \\sum_{\\nu \\in M} w_\\nu \\frac{\\nu(x_{&lt;t})}{\\xi(x_{&lt;t})} \\nu(x_t|x_{&lt;t}) \\\\ &amp;= \\sum_{\\nu \\in M} w(\\nu|x_{&lt;t}) \\nu(x_t|x_{&lt;t}) \\end{align*}\\] 在最后一步中，我们利用了后验权重的性质： \\[w(\\nu|x_{1:t}) = \\frac{p(\\nu|x_{1:t})}{p(x_{1:t})} = \\frac{w_\\nu \\nu(x_{1:t})}{\\xi(x_{1:t})}\\] 考虑在每个时间步骤\\(t\\)上，比较这种预测分布与真实分布的准确性。我们用平方误差来表示： \\[s_t(x_{&lt;t}) = \\sum_{x_t \\in X} (\\mu(x_t|x_{&lt;t}) - \\xi(x_t|x_{&lt;t}))^2\\] 考虑到直到时间\\(n\\)的总期望误差： \\[S_n = \\sum_{t=1}^{n} \\sum_{x_{&lt;t} \\in X} \\mu(x_{&lt;t}) s_t(x_{&lt;t})\\] Solomonoff证明了这个预测器的总误差在极限情况下的一个重要界限： \\[S_\\infty \\leq \\ln(w^{-1}_\\mu) = K(\\mu) \\ln 2\\] 这一结果表明，总误差被生成数据的环境复杂性所界定，简单的环境易于学习，最优预测器的预测迅速接近真实值。 我们还可以考虑一种假设，即数据是由某个未知的确定性程序\\(p\\)生成的，满足\\(U(p) = x^*\\)，其中\\(x^*\\)是观察到的前缀\\(x = x_{1:t}\\)的无限扩展。假设程序的先验定义为： \\[\\Pr(p) = 2^{-\\ell(p)}\\] 那么序列的先验预测分布为： \\[M(x) = \\sum_{p: U(p) = x^*} 2^{-\\ell(p)}\\] 可以证明\\(M(x) = \\xi(x)\\) 由此，我们可以计算后验预测分布\\(M(x_t|x_{&lt;t}) = \\frac{M(x_{1:t})}{M(x_{&lt;t})}\\)。 由于Solomonoff归纳依赖于Kolmogorov复杂性来定义其先验，因此它是不可计算的。然而，可以通过各种方式对这一方案进行近似。例如，可以使用元学习（meta learning）来训练通用序列预测器，如Transformer或LSTM，使得这些模型能够近似一个通用预测器。 3. AXI和通用AGI 最后讲了AIXI，是一个对Solomonoff归纳法的扩展，书中说的比较简略：","categories":[{"name":"Probabilistic Machine Learning","slug":"Probabilistic-Machine-Learning","permalink":"https://jia040223.github.io/categories/Probabilistic-Machine-Learning/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jia040223.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"统计","slug":"统计","permalink":"https://jia040223.github.io/tags/%E7%BB%9F%E8%AE%A1/"},{"name":"信息论","slug":"信息论","permalink":"https://jia040223.github.io/tags/%E4%BF%A1%E6%81%AF%E8%AE%BA/"}]},{"title":"[Probabilistic Machine Learning]: Fundamentals-Graphical models","slug":"Fundamentals-Graphical models","date":"2024-10-20T11:55:51.000Z","updated":"2024-10-21T12:09:14.677Z","comments":true,"path":"2024/10/20/Fundamentals-Graphical models/","permalink":"https://jia040223.github.io/2024/10/20/Fundamentals-Graphical%20models/","excerpt":"","text":"概率图是随机变量集合的一种表现形式。在概率图中，节点代表随机变量，边的有无则表示这些变量之间的条件独立性假设。所以，称为“独立性图”似乎更为合适。根据图的不同类型，概率图模型可以分为有向图、无向图或有向无向混合图的模型。 一、有向图模型（贝叶斯网络） 基于有向无环图（DAGs）的有向概率图模型（DPGM）通常被称为贝叶斯网络（Bayes nets）。不过这里的“贝叶斯”其实只是定义概率分布的一种方法。 1. 联合分布的表示 DAG的一个关键特性是必定存在拓扑排序。根据拓扑排序的顺序，我们可以看成一个有序马尔可夫链，即一个节点在给定其父节点的情况下，与拓扑顺序中的所有前驱节点条件独立。公式如下： \\[x_i \\perp x_{pred(i)\\backslash pa(i)} | x_{pa(i)}\\] 其中，\\(pa(i)\\)是节点\\(i\\)的父节点，而\\(pred(i)\\)是拓扑顺序中节点\\(i\\)的前驱节点。因此，联合分布可以如下表示（假设我们使用节点顺序 1 到\\(N_G\\)）： \\[p(x_{1:N_G}) = p(x_1) p(x_2|x_1) p(x_3|x_1, x_2) ... p(x_{N_G} | x_1, ..., x_{N_G-1}) = \\prod_{i=1}^{N_G} p(x_i|x_{pa(i)})\\] 其中，\\(p(x_i|x_{pa(i)})\\)是节点 i 的条件概率分布（CPD）。这种表示的主要优点是，由于图中编码的条件独立性假设，定义联合分布所需的参数数量可以大大减少。对于离散随机变量，未结构化的联合分布需要 \\(O(K^{N_G})\\) 个参数，而DAG中每个节点最多有\\(N_P\\)个父节点，仅需要 O(\\(N_G*K^{(N_P+1)}\\)) 个参数。如果DAG是稀疏的，这个数量会显著减少。 2. 例子 书中给了马尔科夫链的例子来具体说明。 显然二阶马尔科夫链也是一种有向图模型： 书中还给了\"student\" network和Sigmoid belief nets的例子。 Sigmoid belief nets其实就是一种潜变量的生成模型 3. 高斯贝叶斯网络（Gaussian Bayes Nets） 高斯贝叶斯网络（Gaussian Bayes Nets）也是一种有向概率图模型，其中所有的变量都是连续实值的，并且条件概率分布（CPD）具有线性高斯形式。具体来说，CPD 的形式如下： \\[p(x_i|x_{\\text{pa}(i)}) = \\mathcal{N}(x_i|\\mu_i + w_i^T x_{\\text{pa}(i)}, \\sigma_i^2)\\] 这表示每个变量\\(x_i\\)在给定它的父节点\\(x_{\\text{pa}(i)}\\)的条件下，服从一个正态分布，其均值是父节点的线性组合加上一个局部均值\\(\\mu_i\\)，标准差为\\(\\sigma_i\\)​。 通过将这些局部 CPD 乘起来，我们可以得到一个大的联合高斯分布\\(p(x) = \\mathcal{N}(x|\\mu, \\Sigma)\\)，其中\\(x \\in \\mathbb{R}^{N_G}\\)​ 表示所有变量的联合分布。最终，通过推导，我们得到整体均值向量\\(\\mu = (\\mu_1, \\mu_2, \\dots, \\mu_{N_G})\\)是所有局部均值的拼接。而协方差则\\(\\Sigma = U S^2 U^T\\)。其中\\(S^2\\)是标准差对角矩阵的平方。 具体过程如下： 4. 条件独立性（Conditional Independence, CI） 本部分首先定义了条件独立性(CI)的基本概念。\\(x_A ⊥_G x_B|x_C\\)表示在图\\(G\\)中，\\(A\\)与\\(B\\)在给定\\(C\\)的情况下是条件独立的。图\\(G\\)中的条件独立性陈述集合记为\\(I(G)\\)，而在某个分布\\(p\\)中成立的所有条件独立性陈述集合记为\\(I(p)\\)。 图\\(G\\)被称为分布\\(p\\)的I-map（独立性图）或分布\\(p\\)相对于\\(G\\)是马尔科夫的（Markov wrt G），当且仅当图G中的所有条件独立性陈述都成立于\\(p\\)中（即\\(I(G) ⊆ I(p)\\)）。这意味着图\\(G\\)可以作为分布\\(p\\)的代理，帮助我们推理\\(p\\)的条件独立性属性。 *\\(G\\)是\\(p\\)的最小I-map，当且仅当没有比\\(G\\)更小的图\\(G′\\)可以满足\\(I(G&#39;) ⊆ I(p)\\)。 4.1 全局马尔科夫性质 (d-separation) 接下来介绍了如何从有向无环图（DAG）中推导出条件独立性（CI）的概念，具体通过d-分离（d-separation）的方式实现。 d-分离的定义： 对于一条无向路径P，如果以下任意条件之一成立，则P被节点集C（观测集）d-分离： P包含链式结构：\\(s → m → t\\)或\\(s ← m ← t\\)，并且中间节点m属于C。 P包含分叉结构：\\(s ↙m↘ t\\)，并且中间节点m属于C。 P包含碰撞结构（v-结构）：\\(s ↘m↙ t\\)，并且中间节点m不属于C，且m的后代节点也不在C中。 如果对于任意\\(a∈A\\)和\\(b∈B\\)的所有无向路径都被\\(C\\)d-分离，则称节点集A与B在给定C的条件下是d-分离的。 全局马尔科夫性质： 当A与B在给定C的情况下是d-分离时，我们把这种条件独立性写为\\(X_A ⊥_G X_B|X_C\\)。这个性质也称为有向全局马尔科夫性质，用于判定图中的条件独立性。 具体为什么者三种情况成立，解释如下： \\(X \\rightarrow Y \\rightarrow Z\\) 联合概率分布: \\[p(x, y, z) = p(x) p(y|x) p(z|y)\\] 条件概率: \\[p(x, z|y) = \\frac{p(x, y, z)}{p(y)} = \\frac{p(y)p(x|y)p(z|y)}{p(y)} = p(x|y)p(z|y)\\] 因此，\\(X \\perp Z | Y\\)。观察中间节点\\(Y\\)会将链断开（如同马尔可夫链）。 \\(X \\leftarrow Y \\rightarrow Z\\) 联合概率分布: \\[p(x, y, z) = p(y) p(x|y) p(z|y)\\] 条件概率: \\[p(x, z|y) = \\frac{p(x, y, z)}{p(y)} = \\frac{p(y)p(x|y)p(z|y)}{p(y)} = p(x|y)p(z|y)\\] 因此，\\(X \\perp Z | Y\\)。观察根节点\\(Y\\)将其子节点分开（如同朴素贝叶斯分类器）。 \\(X \\rightarrow Y \\leftarrow Z\\) 联合概率分布: \\[p(x, y, z) = p(x) p(z) p(y|x, z)\\] 条件概率: \\[p(y∣x,z)p(y)p(x, z|y) = \\frac{p(x)p(z)p(y|x, z)}{p(y)}\\] 此时，\\(X \\not\\perp Z | Y\\)。但我们注意到在无条件分布中：\\(p(x, z) = p(x)p(z)\\)说明\\(X\\)和\\(Z\\)是边际独立的。然而，观察到一个共同子节点\\(Y\\)使得其父节点\\(X\\)和\\(Z\\)变得相关，这种现象被称为 Explaining Away或 Berkson's paradox。 4.2 Berkson's paradox的例子 书中给了几个例子来直观展示Berkson's paradox： 示例 1：掷硬币实验 假设我们掷两个硬币100次，并且只记录至少有一个硬币是正面朝上的结果。这意味着我们会忽略双反的情况。 在这种情况下，每当硬币1为反面时，硬币2必定为正面，这导致了一个假象，即硬币1和硬币2似乎相关。 但实际上，这种相关性是由于选择偏差引起的，即我们只记录特定条件下的数据，从而扭曲了它们的实际独立性。 示例 2：高斯分布图模型 (DPGM) 我们有一个\\(X \\rightarrow Z \\leftarrow Y\\)的图结构，且联合分布为：\\(p(x, y, z) = N(x | -5, 1) N(y | 5, 1) N(z | x + y, 1)\\) 当我们不对\\(Z\\)进行条件化时，\\(X\\)和\\(Y\\)是独立的。然而，如果我们只选择\\(z &gt; 2.5\\)的样本，会发现\\(X\\)和\\(Y\\)变得相关，这也是由于选择偏差导致的。 4.3 Markov Blankets Markov Blankets是指，使某个节点与图中其他所有节点条件独立的最小节点集。对于一个节点\\(i\\)，它的马尔可夫包络\\(mb(i)\\)包括它的父节点、子节点和共同父节点（与其子节点有共同父母的节点），形式化表示为： \\[mb(i) ≜ ch(i) ∪ pa(i) ∪ copa(i)\\] 其中\\(ch(i)\\)表示节点\\(i\\)的子节点，\\(pa(i)\\)是它的父节点，\\(copa(i)\\)是它子节点的共同父节点。 我们通过分解节点的联合分布，证明节点\\(i\\)的条件概率\\(p(X_i | X_{-i})\\)仅取决于马尔可夫包络中的节点。最终条件概率为： \\[p(x_i | x_{-i}) = p(x_i | x_{mb(i)}) \\propto p(x_i | x_{mb(i)}) \\prod_{Y_j \\in ch(i)} p(x_k | x_{pa(k)})\\] 4.4 其他马尔可夫性质 通过d-分离准则，可以得出局部马尔可夫性质： \\[i \\perp nd(i) \\setminus pa(i) | pa(i)\\] 其中\\(nd(i)\\)表示节点\\(i\\)的非后代节点，即除了它的后代以外的其他节点。局部马尔可夫性质表示，一个节点在已知它的父节点的条件下，与它的非后代独立。 同时，考虑上面结论的一个特殊情况，就可以得到有序马尔可夫性质： \\[i \\perp pred(i) \\setminus pa(i) | pa(i)\\] 其中\\(pred(i)\\)是节点\\(i\\)的前驱节点（根据某个拓扑排序）。这些性质与全局马尔可夫性质 、局部马尔可夫性质 、有序马尔可夫性质之间有递推关系，最终这些性质是等价的。 5. 采样（Sampling）和推理（Inference） 从一个有向概率图模型中生成样本是简单的。我们可以按照拓扑顺序访问节点，即先访问父节点再访问子节点，然后根据父节点的值为每个节点采样。这个过程称为祖先采样（ancestral sampling），它能够生成来自联合分布的独立样本。 在PGMs中，推理指的是在已知一组可观察节点\\(V\\)的情况下，计算一组查询节点\\(Q\\)的后验分布，同时对不相关的干扰变量\\(R\\)进行边际化： \\[p_\\theta(Q | V) = \\frac{p_\\theta(Q, V)}{p_\\theta(V)} = \\frac{\\sum_{R} p_\\theta(Q, V,R)}{p_\\theta(V)}\\] 如果\\(Q\\)是单个节点，则\\(p_\\theta(Q | V)\\)被称为节点\\(Q\\)的后验边际。 例子： 设\\(V = x\\)为观察到的声音波形序列，\\(Q = z\\)为未知的说话单词，\\(R = r\\)为与信号相关的其它因素（如音调或背景噪声）。我们的目标是计算在给定声音的情况下，单词的后验分布： \\[p_\\theta(z | x) = \\sum_r p_\\theta(z, r | x) = \\sum_{r}{\\frac{p_{\\theta}(z,r,x)}{\\sum_{r&#39;,z&#39;}{p_{\\theta}}(z&#39;,r&#39;,x)}}\\] 为了简化，我们可以将随机因子\\(R\\)与查询集合\\(Q\\)合并，定义隐藏变量的完整集合\\(H = Q \\cup R\\)： \\[p_\\theta(h | x) = \\frac{p_\\theta(h, x)}{p_\\theta(x)} = \\frac{p_\\theta(h, x)}{\\sum_{h&#39;} p_\\theta(x, h&#39;)}\\] 计算复杂度：推理任务的计算复杂度依赖于图的条件独立性属性，通常是NP-hard的，但对于某些图结构（如链、树和稀疏图），可以有效地用动态规划方法解决。 6. 学习 学习的目标是从数据中计算后验分布\\(p(\\theta | D)\\)。在机器学习中，通常计算参数的点估计，例如最大化后验：\\(\\hat{\\theta} = \\arg\\max p(\\theta | D)\\) 6.1 从完整数据中学习 下面给出一个具体的图模型进行示范： 图示描述了一个典型的监督学习问题，其中有\\(N\\)个局部变量\\(x_n\\)和\\(y_n\\)，以及两个全局变量，代表数据样本共享的参数。局部变量为观察到的（训练集中），以实心（阴影）节点表示；全局变量为未观察到的，以空心（无阴影）节点表示。 根据图的条件独立性属性，联合分布因式分解为每个节点的乘积： \\[\\begin{align*} p(\\theta, D) &amp;= p(\\theta_x) p(\\theta_y) \\prod_{n=1}^{N} p(y_n | \\theta_y) p(x_n | y_n, \\theta_x) \\\\ &amp;= \\left( p(\\theta_y) \\prod_{n=1}^{N} p(y_n | \\theta_y) \\right) \\left( p(\\theta_x) \\prod_{n=1}^{N} p(x_n | y_n, \\theta_x) \\right) \\\\ &amp;= [p(\\theta_y) p(D_y | \\theta_y)] [p(\\theta_x) p(D_x | \\theta_x)] \\end{align*}\\] 其中\\(D_y = \\{y_n\\}_{n=1}^{N}\\)​ 和\\(D_x = \\{x_n, y_n\\}_{n=1}^{N}\\)分别为足以估计\\(\\theta_y\\)和\\(\\theta_x\\)​ 的数据。 我们可以独立地计算每个参数的后验分布： \\[p(\\theta, D) = \\prod_{i=1}^{N_G} p(\\theta_i)p(D_i | \\theta_i)\\] 从而得出最大似然估计： \\[\\hat{\\theta} = \\arg\\max_{\\theta} \\prod_{i=1}^{N_G} p(D_i | \\theta_i)\\] 6.2 不完整数据中学习(隐变量) 如上图，当存在隐变量时，似然函数的因式分解不再适用，因此无法独立地估计CPD。对于观测数据的似然性，可以写成： \\[\\begin{align*} p(D | \\theta) &amp;= \\sum_{z_{1:N}} \\prod_{n=1}^{N} p(z_n | \\theta_z) p(x_n | z_n, \\theta_x) \\\\ &amp;= \\prod_{n=1}^{N} \\sum_{z_{n}} p(z_n | \\theta_z) p(x_n | z_n, \\theta_x) \\end{align*}\\] 对数函数无法在求和上进行分解： \\[\\ell(\\theta) = \\sum_{n} \\log \\sum_{z_n} p(z_n | \\theta_z) p(x_n | z_n, \\theta_x)\\] 所以就有了如下的两种方法： 使用EM算法 期望步骤（E步）：在这个步骤中，算法推断隐变量\\(z_n\\)​ 的期望值，而不是返回其完整的后验分布\\(p(z_n | x_n, \\theta^{(t)})\\)。这样做是为了降低计算复杂性，返回期望充分统计量（ESS），即隐变量的加权计数。 最大化步骤（M步）：在这个步骤中，使用E步中计算的ESS来最大化完全观察数据的对数似然期望值。这意味着在这个步骤中，参数更新基于对隐变量的期望。 使用SGD拟合 EM是批处理算法，使用随机梯度下降（SGD）更适合大规模数据集。我们需要为每个示例计算观测数据的边际似然： \\[p(x_n | \\theta) = \\sum_{z_n} p(z_n | \\theta_z) p(x_n | z_n, \\theta_x)\\] 对数似然计算： \\[l(\\theta) = \\log p(D | \\theta) = \\sum_{n=1}^{N} \\log p(x_n | \\theta)\\] 梯度计算： \\[\\begin{align*} \\nabla_\\theta \\ell(\\theta) &amp;= \\sum_n \\nabla_\\theta \\log p(x_n|\\theta) \\\\ &amp;= \\sum_n \\frac{1}{p(x_n|\\theta)} \\nabla_\\theta p(x_n|\\theta) \\\\ &amp;= \\sum_n \\frac{1}{p(x_n|\\theta)} \\nabla_\\theta \\left( \\sum_{z_n} p(z_n, x_n|\\theta) \\right) \\\\ &amp;= \\sum_n \\sum_{z_n} \\frac{p(z_n, x_n|\\theta)}{p(x_n|\\theta)} \\nabla_\\theta \\log p(z_n, x_n|\\theta) \\\\ &amp;= \\sum_n \\sum_{z_n} p(z_n|x_n, \\theta) \\nabla_\\theta \\log p(z_n, x_n|\\theta) \\end{align*}\\] 当然，这种几种做法需要我们知道隐变量的后验分布，但在深度学习中，很多时候是不知道的。这时候比如VAEs就是直接优化最大似然下界，相当于只进行了EM算法中的M步。 二、无向图模型（马尔可夫随机场） 在某些领域，比如图像，去定义像素之间的联系的方向是不自然的。我们可以说一个像素与周围像素有关，但为这种关系定义一个方向（即建模一个有向图模型），其实并不是自然或者合理的。此时用无向图模型，不指定边的方向，更自然地处理这些问题。相比有向图模型，无向图模型（UPGM）具有对称性，特别适用于空间或关系数据。 1. 联合分布 在无向图中，由于没有拓扑顺序，无法使用链规则来表示联合分布\\(p(x_1, \\ldots, x_N)\\)。相反，联合分布通过图中的最大团（maximal cliques）来表示。每个最大团关联一个势函数，记为\\(\\psi_c(x_c; \\theta_c)\\)，其中\\(x_c\\)​ 是该团的变量，\\(\\theta_c\\)​ 是其参数。这些势函数可以是任何非负函数。 团是一个无向图中的完全子图，其中任意两个节点都相互连接。在一个图中，最大团是不能再扩展的团，即如果再添加任何一个节点，这个团将不再是完全连接的。 Hammersley-Clifford定理 如果联合分布\\(p\\)满足由无向图\\(G\\)所隐含的条件独立性（CI）属性，那么可以写成以下形式： \\[p(x|\\theta) = \\frac{1}{Z(\\theta)} \\prod_{c \\in C} \\psi_c(x_c; \\theta_c)\\] 其中\\(C\\)是图\\(G\\)的所有最大团的集合，\\(Z(\\theta)\\)是归一化因子（partition function），确保整体分布的总和为1： \\[Z(\\theta) = \\sum_x \\prod_{c \\in C} \\psi_c(x_c; \\theta_c)\\] Gibbs分布 Hammersley-Clifford定理中的分布可以重写为： \\[p(x|\\theta) = \\frac{1}{Z(\\theta)} \\exp(-E(x; \\theta))\\] 其中\\(E(x; \\theta)\\)是状态\\(x\\)的能量，定义为所有团的能量之和： \\[E(x; \\theta) = \\sum_c E(x_c; \\theta_c)\\] 势函数可以通过能量定义为： \\[\\psi_c(x_c; \\theta_c) = \\exp(-E(x_c; \\theta_c))\\] 这表明低能量状态与高概率状态相关联。Gibbs分布也称为基于能量的模型，广泛用于物理和生物化学领域，同时也在机器学习中用于定义生成模型。此外，条件随机场（CRFs）是一种特定形式的能量基础模型，形式为\\(p(y|x, \\theta)\\)，其中势函数基于输入特征\\(x\\)。 2. 全可见的马尔可夫随机场（Fully visible MRFs） Fully visible MRFs指的是无向图模型中不存在潜变量，即除了参数以外，所有的节点都是数据的一部分。 书中给了几个例子，这里只记录Ising models一个例子 Ising模型是一种特定的马尔可夫随机场（MRF），用于表示二元变量的联合分布。在一个二维格子（lattice）中，节点表示原子，每个原子可以有两个状态（+1或-1），这通常用于表示磁性材料中的自旋。其对应的无向图如下： 联合分布可以表示为： \\[p(x|\\theta) = \\frac{1}{Z(\\theta)} \\prod_{i \\sim j} \\psi_{ij}(x_i, x_j; \\theta)\\] 其中\\(i \\sim j\\)表示节点\\(i\\)和\\(j\\)是邻居。 势函数：对于Ising模型，势函数定义为： \\[\\psi_{ij}(x_i, x_j; \\theta) = \\begin{cases} e^{J_{ij}} &amp; \\text{如果 } x_i = x_j \\\\ e^{-J_{ij}} &amp; \\text{如果 } x_i \\neq x_j \\end{cases}\\] \\(J_{ij}\\)​ 是节点之间的耦合强度。若两个节点不相连，则\\(J_{ij} = 0\\)。 能量模型：Ising模型通常以能量为基础定义： \\[p(x|\\theta) = \\frac{1}{Z(J)} \\exp(-E(x; J))\\] 能量函数为： \\[E(x; J) = -J \\sum_{i \\sim j} x_i x_j\\] 其中，\\(x_i x_j\\)的值为： \\[+1（当 x_i = x_j​ 时）\\\\ −1（当 x_i \\neq x_j 时）\\] 耦合强度与温度：耦合强度\\(J\\)控制邻近节点之间的相互作用程度。可以通过引入温度\\(T\\)来调整\\(J\\)，设\\(J&#39; = J/T\\)，这意味着：温度较低时（冷），节点之间的耦合较强（\\(J\\)较大）。温度较高时（热），节点之间的耦合较弱（\\(J\\)较小）。 磁性系统的特性：如果所有权重为负（\\(J &lt; 0\\)），节点倾向于与邻居不同，形成反铁磁系统，导致系统受挫，因为在二维格子中不可能所有邻居都不同；如果所有边的权重为正（\\(J &gt; 0\\)），邻近节点更可能处于相同状态，这被称为铁磁模型。 引入外场：除了成对项（pairwise terms），通常还会加入单项（unary terms），称为外部场。模型可以表示为： \\[p(x|\\theta) = \\frac{1}{Z(\\theta)} \\prod_i \\psi_i(x_i; \\theta) \\prod_{i \\sim j} \\psi_{ij}(x_i, x_j; \\theta)\\] 对于每个节点，外部势函数定义为： \\[\\psi_i(x_i) = \\begin{cases} e^{\\alpha} &amp; \\text{如果 } x_i = +1 \\\\ e^{-\\alpha} &amp; \\text{如果 } x_i = -1 \\end{cases}\\] 其能量模型为： \\[E(x|\\theta) = -\\alpha \\sum_i x_i - J \\sum_{i \\sim j} x_i x_j\\] 3. MRFs与潜在变量 对于包含潜变量的MRFs，书中举了著名的Boltzmann机作为例子。 普通Boltzmann机器允许任意的图结构，但确切的推理和学习在普通Boltzmann机器中是不可行的，近似推理（如Gibbs抽样）通常很慢。 限制Boltzmann机器（RBMs）将节点分为两层，且同层节点之间没有连接。这种结构使得推理更加高效，因为隐藏节点在给定可见节点的条件下是独立的。RBMs通常具有二元节点，能量项形式为\\(w_{dk} x_d z_k\\)，其中\\(z_k\\)决定隐藏单元的活跃状态。 深度Boltzmann机器通过堆叠多个RBM层来构建深度Boltzmann机器，进一步提高模型的表示能力。 深度belief网络（DBNs）结合了RBM和生成模型（DPGM），作为潜在分布编码的先验，并将其转换为观察数据。DBN可以以贪婪方式进行训练，支持快速的自下而上的推理。 4. 最大熵模型 最大熵模型的目标是在特征符合经验期望的条件下，构建具有最大熵的分布。其形式为： \\[p(x|\\theta) = \\frac{1}{Z(\\theta)} \\exp(\\theta^T \\phi(x))\\] 在前面我们也提到，指数族分布是满足最大熵的分布。 在某些应用中，特征\\(\\phi(x)\\)可以预先定义，但也可以通过无监督学习进行诱导。这种特征诱导（Feature induction）的方法通常从基础特征开始，逐步生成新的特征组合（书中也没有细讲）。 总归最大熵模型也算是很经典也很熟悉的一个模型了。 5. 条件独立性 在给定三个节点集\\(A\\)、\\(B\\)和\\(C\\)的情况下，如果在图\\(G\\)中，去掉所有\\(C\\)中的节点后，\\(A\\)中的任一节点与\\(B\\)中的任一节点之间没有路径相连，则称\\(X_A\\)​ 在\\(X_C\\)​ 条件下与\\(X_B\\)​ 条件独立。用符号表示为： \\[X_A \\perp_G X_B | X_C\\] 这也被称为UPGMs的全局马尔可夫性质。例如，在下面的图中，{X1, X2} 条件独立于 {X6, X7} 给定 {X3, X4, X5}。 使节点\\(t\\)条件独立于图中其他所有节点的最小节点集称为\\(t\\)的马尔可夫毯（Markov blanket），记作\\(mb(t)\\)。也就是说： \\[t \\perp V \\setminus cl(t) | mb(t)$，其中$cl(t) \\equiv mb(t) \\cup \\{t\\}\\] \\(V\\)是图中所有节点的集合。 可以证明，在UPGM中，节点的马尔可夫毯正是其直接邻居。这被称为无向局部马尔可夫性质（undirected local Markov property）。例如，上图中对于节点 X5，其马尔可夫毯为 {X2, X3, X4, X6, X7}。 根据局部马尔可夫性质，可以得出，如果两个节点之间没有直接边，则它们在其余节点条件下是条件独立的。这称为成对马尔可夫性质（pairwise Markov property）： \\[s \\perp t | V \\setminus \\{s, t\\} \\iff G_{st} = 0\\] 其中\\(G_{st} = 0\\)表示\\(s\\)和\\(t\\)之间没有边。 全局马尔可夫性质蕴含局部马尔可夫性质，局部马尔可夫性质又蕴含成对马尔可夫性质。相对不明显的是，成对马尔可夫性质可以推导出全局马尔可夫性质，因此这些马尔可夫性质实际上是相同的。 6. 采样（sampling）与推断（Inference） 与有向概率图模型相比，从UPGM中采样（sampling）可能会比较慢，因为UPGM中的变量没有顺序。此外，除非知道归一化常数\\(Z\\)的值，否则很难计算任何配置的概率。 由于这些挑战，通常使用马尔可夫链蒙特卡洛（MCMC）方法来从UPGM中生成样本。MCMC方法通过构建一个状态空间并在该空间中进行随机游走，逐步收敛到目标分布。 对于特殊的的UPGM，也可以使用结点树算法（junction tree algorithm）通过动态规划生成样本。 至于Inference，书中仅给了一个去噪的例子，具体内容会在第九章进行讲述。 模型描述：假设我们有一个由二进制像素\\(z_i\\)​ 组成的图像，但我们只观察到像素的噪声版本\\(x_i\\)​。该联合模型的形式为： \\[p(x,z)=p(z)p(x|z) = \\left[ \\frac{1}{Z} \\prod_{i \\sim j} \\psi_{ij}(z_i, z_j) \\right] \\prod_{i} p(x_i | z_i)\\] 其中\\(p(z)\\)是一个Ising model先验，而\\(p(x_i | z_i) = N(x_i | z_i, \\sigma^2)\\)，\\(z_i\\)取值为 {-1, +1}。 链图（Chain Graph）：这个模型在先验上使用UPGM，而在似然部分使用有向边，形成一种称为链图的混合模型。 后验推断：推断任务是计算后验边际\\(p(z_i | x)\\)，以及后验最大后验估计（MAP）：\\(\\text{argmax}_z \\, p(z | x)\\) 由于对于大网格的精确计算是不可行的，因此必须使用近似方法。可以使用多种算法进行推断，包括均值场变分推断（mean field variational inference）、吉布斯采样（Gibbs sampling）、循环信念传播（loopy belief propagation）等。 7. 学习 即使在完全观测的情况下，计算最大似然估计（MLE）也可能非常耗费计算资源，因为需要处理归一化常数\\(Z(\\theta)\\)。而计算参数的后验分布\\(p(\\theta | D)\\)则更加困难，因为还需要额外的归一化常数\\(p(D)\\)，这被称为双重不可解问题（doubly intractable）。 7.1 从完全观测数据中学习 完全观测数据指的是训练过程中没有隐藏变量或缺失数据（即完全观测数据设置）。此时MRF的分布形式为： \\[p(x|\\theta) = \\frac{1}{Z(\\theta)} \\exp\\left( \\sum_{c} \\theta^T_c \\phi_c(x) \\right)\\] 其中，\\(c\\)表示团，\\(Z(\\theta)\\)是归一化常数，\\(\\theta_c\\)​ 是参数向量，\\(\\phi_c(x)\\)是对应的特征函数。 则对数似然可以表示为： \\[\\ell(\\theta) \\triangleq \\frac{1}{N} \\sum_{n} \\log p(x_n | \\theta) = \\frac{1}{N} \\sum_{n} \\left[ \\sum_{c} \\theta_c^T \\phi_c(x_n) - \\log Z(\\theta) \\right]\\] 在指数族中，我们已经知道了 \\[\\frac{\\partial \\log Z(\\theta)}{\\partial \\theta_c} = \\mathbb{E}[\\phi_c(x) | \\theta] = \\sum_{x} p(x | \\theta) \\phi_c(x)\\] 所以其梯度为： \\[\\frac{\\partial \\ell}{\\partial \\theta_c} = \\frac{1}{N} \\sum_{n} \\left[ \\phi_c(x_n) - \\mathbb{E}[\\phi_c(x)|\\theta] \\right]\\] 其中，\\(\\mathbb{E}[\\phi_c(x)|\\theta]\\)表示在模型下特征函数的期望值。当数据的期望值与模型的期望值相等时，梯度为零，这被称为矩匹配（moment matching）。评估来自数据的期望值，称为正相（positive phase），因为\\(x\\)被固定为观测值。评估来自模型的期望值，称为负相（negative phase），因为\\(x\\)在模型中可以自由变化。 在特定情况下，可以使用迭代比例拟合算法（iterative proportional fitting, IPF）进行迭代优化。然而，在一般情况下，通常使用梯度下降方法进行参数估计。 7.2 计算的细节问题 计算MRF和条件随机场（CRF）时，MLE的最大计算瓶颈是计算对数归一化常数\\(Z(\\theta)\\)的导数： \\[\\frac{\\partial \\log Z(\\theta)}{\\partial \\theta} = \\mathbb{E}_{p(x|\\theta)}[\\nabla_\\theta \\log \\tilde{p}(x|\\theta)]\\] 由于需要在每一步梯度下降训练时从模型中采样，计算梯度变得非常耗时。所以除了MLE，还可以使用其他估计方法：对比散度（contrastive divergence）和伪似然估计（pseudolikelihood estimation）。这两个是生成模型里面比较常见的方法。 7.3最大伪似然估计（Maximum Pseudolikelihood Estimation, MPLE） 伪似然是一种替代MLE的简单方法，定义如下： \\[\\ell_{PL}(\\theta) = \\frac{1}{N} \\sum_{n=1}^{N} \\sum_{d=1}^{D} \\log p(x_{nd}|x_{n,-d}, \\theta)\\] 伪似然优化的是全条件概率的乘积，也称为复合似然（composite likelihood）。这里\\(x_{n,-d}\\)表示除了\\(x_d\\)​ 外的其他变量。\\(x_{nd}\\)通常表示第\\(n\\)个样本的第\\(d\\)个变量或特征 MPLE在某些情况下是MLE的近似，但通常不是最优的（在某些特殊模型如高斯MRF中，MPLE与MLE等价） 与MLE相比，伪似然更快，因为计算每个节点的全条件概率只需对单个节点的状态求和，而不需要计算全局归一化常数。同时，尽管伪似然在一般情况下不等同于MLE，但它在大样本极限下是一致估计量。一些具体实践中的表现如下： 7.4 从不完全观测数据中学习 在一些应用中，我们可能只观察到部分数据，其他数据（隐藏变量）是不可见的。例如，我们可能希望从噪声图像\\(x\\)中推断出一个“干净”的图像\\(z\\)。这样的模型被称为隐 Gibbs 随机场。 模型的联合分布形式为： \\[p(x, z | \\theta) = \\frac{\\exp(\\theta^T \\phi(x, z))}{Z(\\theta)}\\] 对数似然\\(\\ell(\\theta)\\)的表达式为： \\[\\begin{align*} \\ell(\\theta) &amp;= \\frac{1}{N} \\sum_{n=1}^{N} \\log \\left( \\sum_{z_n} p(x_n, z_n | \\theta) \\right) \\\\ &amp;= \\frac{1}{N} \\sum_{n=1}^{N} \\log \\left( \\frac{1}{Z(\\theta)} \\sum_{z_n} \\tilde{p}(x_n, z_n | \\theta) \\right) \\\\ &amp;= \\frac{1}{N} \\sum_{n=1}^{N} \\left( \\log \\sum_{z_n} \\tilde{p}(x_n, z_n | \\theta) - \\log Z(\\theta) \\right) \\end{align*}\\] 我们可以把第一项写成如下形式： \\[\\begin{align*} \\log \\sum_{z_n} \\tilde{p}(x_n, z_n | \\theta) &amp;= \\log \\sum_{z_n} \\exp(\\theta^T \\phi(x_n, z_n)) \\\\ &amp;\\triangleq \\log Z(\\theta, x_n) \\end{align*}\\] 这表示在给定\\(x_n\\)​ 的情况下，隐藏变量\\(z\\)的所有可能值的对数分区函数。 这样对数似然的梯度可以表示为： \\[\\frac{\\partial \\ell}{\\partial \\theta} = \\frac{1}{N} \\sum_{n} \\left( E_{z \\sim p(z | x_n, \\theta)}[\\phi(x_n, z)] - E_{(z,x) \\sim p(z,x | \\theta)}[\\phi(x, z)] \\right)\\] 这个表达式的意思是，梯度是由两个期望值的差组成的： 第一个期望值是条件在观测到的\\(x_n\\)下，对应于隐藏变量\\(z\\)的期望。 第二个期望值是在无条件情况下对\\(z\\)和\\(x\\)的期望。 三、 条件随机场（CRFs） CRF 是一种建模条件分布的马尔科夫随机场的模型，它通过输入节点\\(x\\)预测输出节点\\(y\\)的联合概率。其模型形式为： \\[p(y|x, \\theta) = \\frac{1}{Z(x, \\theta)} \\prod_{c} \\psi_c(y_c; x, \\theta)\\] 这里的分区函数\\(Z(x, \\theta)\\)不仅依赖于参数\\(\\theta\\)，还依赖于输入\\(x\\)，反映了输入对模型输出的影响。 势函数\\(\\psi_c(y_c; x, \\theta)\\)可以采用对数线性形式：\\(\\psi_c(y_c; x, \\theta) = \\exp(\\theta^T_c \\phi_c(x, y_c))\\)，CRF 可以使用非线性势函数（如深度神经网络）来增强建模能力。 CRF 能够捕捉输出标签之间的依赖关系，适用于结构化预测问题，例如序列标签或图节点的标签。它允许对输出的有效值施加约束，如在句法分析中，必须遵循语法规则。 1. 一维的CRFs 1D CRFs 是基于链式结构的图模型，用于定义给定输入序列\\(x_{1:T}\\)下的输出序列\\(y_{1:T}\\)​ 的联合概率分布。公式如下： \\[\\begin{equation} p(y_{1:T} | x, \\theta) = \\frac{1}{Z(x, \\theta)} \\prod_{t=1}^{T} \\psi(y_t, x_t; \\theta) \\prod_{t=2}^{T} \\psi(y_{t-1}, y_t; \\theta) \\end{equation}\\] 其中\\(ψ(y_t, x_t; θ)\\)是节点势，\\(ψ(y_{t-1}, y_t; θ)\\)是边势。 值得注意的是有一种模型可以看成这类任务的另一种建模方法，即最大熵马尔可夫模型（MEMM）： \\[\\begin{equation} p(y_{1:T} | x, \\theta) = p(y_1 | x_1; \\theta) \\prod_{t=2}^{T} p(y_t | y_{t-1}, x_t; \\theta) \\end{equation}\\] 在最大熵马尔可夫模型中，条件概率\\(p(y_t|y_{t-1}, x_t; θ)\\)是局部归一化的。这意味着每个条件的计算仅依赖于前一个标签和当前输入，而不能有效地利用全局上下文信息，这在某些情况下可能导致模型性能的下降。具体模型可以看成一个DPGM： 具体应用示例： CRFs 在1980年代到2010年代的自然语言处理（NLP）领域得到了广泛应用，尤其是在序列标注任务中。然而，随着深度学习的发展，RNN 和 Transformer 等模型逐渐取代了 CRFs。 名词短语分块（Noun Phrase Chunking） 任务描述：在信息提取中，将句子解析为名词短语和动词短语，并为每个单词分配词性标签。 使用 BIO（Beginning, Inside, Outside）标注方法来标识名词短语的起始、内部和外部。例如，序列 OBIIO 和 OBIOBIO 是有效的标注，表明它们分别代表一个名词短语和两个相邻的名词短语。 命名实体识别（Named Entity Recognition） 任务描述：不仅对名词短语进行标注，还将其分类为不同类型（如人名、地名、组织名等）。 BIO 扩展：采用扩展的 BIO 标签来处理不同类型的命名实体（B-Per, I-Per, B-Loc, I-Loc, B-Org, I-Org, Other）。 长距离依赖：通过考虑词之间的长距离相关性，提高模型性能。这种模型被称为跳链 CRF（skip-chain CRF）。 自然语言解析（Natural Language Parsing） 使用概率上下文无关文法（PCFG），PCFG 是一种生成模型，定义了一组重写或生成规则，形式为\\(\\sigma \\rightarrow \\sigma&#39; \\sigma&#39;&#39;\\)或\\(\\sigma \\rightarrow x\\)，其中\\(\\sigma, \\sigma&#39;, \\sigma&#39;&#39;\\)是非终结符（类似于词性），而\\(x\\)是终结符（即单词）。 每个规则都有一个相关的概率，从而定义了一个词序列的概率分布。但为了计算观察到特定序列\\(x = x_1, \\ldots, x_T\\)​ 的概率，需要对所有生成该序列的树进行求和。这一过程可以通过算法在\\(O(T^3)\\)的时间复杂度内完成。 可以通过深度神经网络来定义特征，使得解析器能够更好地捕捉语言中的复杂模式。 2. 二维的CRFs CRFs 还可以应用于图像处理问题，这些问题通常定义在二维网格上。模型的条件概率形式为： \\[p(y|x) = \\frac{1}{Z(x)} \\left[ \\sum_{i \\sim j} \\psi_{ij}(y_i, y_j) \\right] \\prod_i p(y_i|x_i)\\] 这里，\\(\\psi_{ij}(y_i, y_j)\\)是节点之间的潜在函数。 具体应用示例： 语义分割 语义分割的任务是为图像中的每个像素分配标签。虽然可以使用卷积神经网络（CNN）解决此问题，但由于卷积是局部操作，可能无法捕获长程依赖。 所以将CNN的输出传入CRF，尤其是完全连接的CRF，可以更好地捕捉长距离连接。完全连接CRF的推理复杂度较高，但通过高斯势函数可以使用均值场算法进行有效推理。 \\[p(y|x) = \\frac{1}{Z(x)} \\left[ \\prod_i p(y_i|f(x)_i) \\cdot \\prod_{(i,j) \\in E} \\psi(y_i, y_j | x) \\right]\\] 这里，\\(E\\)是CRF中的边集，\\(f(x)_i\\)​ 是CNN的输出 目标检测 为给定类别（如人或车）找到图像中的对象位置。传统的滑动窗口检测器可能对形状可变的对象（如人的身体）表现不佳。 部件分解策略：将对象分解为部件，使用成对的CRF来确保部件之间的空间一致性。例如，使用势函数 \\[\\psi(y_i, y_j | x) = \\exp(-d(y_i, y_j))\\] 其中\\(d(y_i, y_j)\\)是\\(i\\)和\\(j\\)的距离函数，我们也可以设定让它与输入\\(x\\)相关，这样便可以鼓励相邻部件靠近。 最终模型形式类似： \\[p(y|x) = \\frac{1}{Z(x)} \\left[ \\prod_i p(y_i|f(x)_i) \\cdot \\prod_{(i,j) \\in E} \\psi(y_i, y_j | x) \\right]\\] 3. 参数估计 对于参数的估计，我们仍然使用最大似然估计，我们假设势函数是线性的： \\[\\psi_c(y_c; x, \\theta) = \\exp(\\theta^T_c \\phi_c(x, y_c) )\\] 根据势函数的定义，似然函数为： \\[\\log p(y_n | x_n, \\theta) = \\sum_c \\theta^T_c \\phi_c(y_{n,c}, x_n) - \\log Z(x_n; \\theta)\\] 其中，分区函数\\(Z(x_n; \\theta)\\)定义为： \\[Z(x_n; \\theta) = \\sum_y \\exp(\\theta^T \\phi(y, x_n))\\] 接下来，我们定义对数似然： \\[\\begin{align*} \\ell(\\theta) &amp;= \\frac{1}{N} \\sum_n \\log p(y_n | x_n, \\theta) \\\\ &amp;= \\frac{1}{N} \\sum_n [\\sum_c \\theta^T_c \\phi_c(y_{n,c}, x_n) - \\log Z(x_n; \\theta)] \\end{align*}\\] 根据链式法则，似然函数的导数为： \\[\\frac{\\partial \\ell}{\\partial \\theta_c} = \\frac{1}{N} \\sum_n \\left( \\phi_c(y_{n,c}, x_n) - \\frac{\\partial}{\\partial \\theta_c} \\log Z(x_n; \\theta) \\right)\\] 由于\\(\\frac{\\partial}{\\partial \\theta_c} \\log Z(x_n; \\theta)\\)可以表示为期望值（前面证明过）： \\[\\frac{\\partial}{\\partial \\theta_c} \\log Z(x_n; \\theta) = \\mathbb{E}_{p(y | x_n, \\theta)}[\\phi_c(y, x_n)]\\] 因此，最终的梯度为： \\[\\frac{\\partial \\ell}{\\partial \\theta_c} = \\frac{1}{N} \\sum_n \\left( \\phi_c(y_{n,c}, x_n) - \\mathbb{E}_{p(y | x_n, \\theta)}[\\phi_c(y, x_n)] \\right)\\] 更一般情况下，CRF 可以写作： \\[p(y | x; \\theta) = \\frac{\\exp(f(x, y; \\theta))}{Z(x; \\theta)}\\] 其中\\(f(x, y; \\theta)\\)是评分函数（负能量函数），高分对应于可能的情况。 对数似然的梯度为： \\[\\nabla_\\theta \\ell(\\theta) = \\frac{1}{N} \\sum_{n=1}^N \\left( \\nabla_\\theta f(x_n, y_n; \\theta) - \\nabla_\\theta \\log Z(x_n; \\theta) \\right)\\] 计算对数分区函数的导数是可行的，前提是我们能够计算对应的期望值。然而，需要注意的是，我们必须对每个训练样本计算这些导数，这比马尔可夫随机场（MRF）的情况要慢，因为MRF的对数分区函数是一个常数，独立于观测数据。 四、 有向图模型和无向图模型的比较 1. 表达能力 一个图\\(G\\)是分布\\(p\\)的 I-map 如果图中编码的所有条件独立（CI）关系都在分布中成立；完美图则能够精确表示分布的所有 CI 属性。 可以证明，DPGMs 和 UPGMs 能够建模不同的分布集，意味着两者在建模能力上并不优于对方。 DPGM 的优越性：DPGM 可以精确表示 v-structure（如\\(A \\to C \\leftarrow B\\)），表明\\(A\\)与\\(B\\)独立，但在给定\\(C\\)时不独立。若将箭头去掉，得到的无向图会错误地表示其他独立关系。事实上，这种CI性质无向图模型建模不出来。 UPGM 的优越性：如四循环的例子，UPGM 可以正确表示某些 CI 关系（例如\\(A \\perp C | B, D\\)），而相应的 DPGM 无法精确编码所有关系。 2. 有向图模型和无向图模型的转换 2.1 从有向图模型转化为无向图模型 在将有向概率图模型（DPGM）转换为无向概率图模型（UPGM）时，首先我们需要道德化（Moralization）。 即对于共享子节点的“未婚”父节点（即没有边连接的父节点），需要添加边以连接它们。这一步确保条件独立性（CI）属性在转换后依然保持一致。 一旦道德化完成，移除所有的箭头，得到一个无向图。 以学生网络为例： 其联合分布如下： \\[P(C, D, I, G, S, L, J, H) = P(C)P(D|C)P(I)P(G|I, D)P(S|I)P(L|G)P(J|L, S)P(H|G, J)\\] 接下来，为每个条件概率分布（CPD）定义势函数，得到： \\[p(C, D, I, G, S, L, J, H) = \\psi_C(C) \\psi_D(D, C) \\psi_I(I) \\psi_G(G, I, D) \\psi_S(S, I) \\psi_L(L, G) \\psi_J(J, L, S) \\psi_H(H, G, J)\\] 所有的潜在函数都是局部归一化的，因为它们都是 CPD，因此归一化常数\\(Z = 1\\) 转换后的无向图如上所示，我们看到已经添加了以下的边：D-I，G-J，L-S，这些边连接了共享子节点的父节点，确保了在无向图中能够存储每个家庭的条件概率分布。 但需要注意的是转换过程中，某些条件独立性（CI）关系可能会被放宽或忽略，这样只是可以保证转换后的图能够表示原有图的一部分信息。 2.2 从无向图模型转化为有向图模型 将无向概率图模型（UPGM）转换为有向概率图模型（DPGM），我们首先需要创建虚拟节点：对于每个潜在函数\\(\\psi_c(x_c; \\theta_c)\\)，我们引入一个“虚拟节点”\\(Y_c\\)​，并将设置一个状态\\(y^*_c\\)​。 接着，我们定义条件概率分布\\(p(Y_c = y^*_c | x_c) = \\psi_c(x_c; \\theta_c)\\)。这表示在给定特征\\(x_c\\)的条件下，虚拟节点\\(Y_c\\)​ 取特定值的概率，与原UPGM中的潜在函数相对应。这样转换后的整体联合分布形式为\\(p_{\\text{undir}}(x) \\propto p_{\\text{dir}}(x, y^*)\\) 具体示例： 考虑一个UPGM，其中联合分布为： \\[p(A, B, C, D) = \\frac{\\psi(A, B, C, D)}{Z}\\] 为了转换成DPGM，我们添加一个虚拟节点\\(E\\)，使其成为所有其他节点的子节点，并设置\\(E = 1\\)。然后定义 CPD 为： \\[p(E = 1 | A, B, C, D) \\propto \\psi(A, B, C, D)\\] 同样，这个过程中某些条件独立性（CI）关系可能会被放宽或忽略。 3. 有向图模型和无向图模型的结合 链图（Chain graphs）是一种可能包含有向和无向边的概率图模型，但不允许有向循环。它能够有效地结合两种类型的边，从而捕捉更复杂的依赖关系。 例如下图给出了一个简单的联合模型示例 \\[p(y_{1:D}|x_{1:D}) = \\left[ \\frac{1}{Z} \\prod_{i \\sim j} \\psi_{ij}(x_i, x_j) \\right] \\prod_{i=1}^{D} p(y_i | x_i)\\] 其中，\\(p(x)\\)由无向概率图模型（UPGM）定义，而\\(p(y|x)\\)则由有向概率图模型（DPGM）指定。 再比如如下的离散时间的二阶马尔科夫链：假设观测值是连续的\\(x_t \\in \\mathbb{R}^D\\)，其转移函数可以表示为线性高斯条件概率分布（CPD）： \\[p(x_t | x_{t-1}, x_{t-2}, \\theta) = \\mathcal{N}(x_t | A_1 x_{t-1} + A_2 x_{t-2}, Σ)\\] 这种模型也被称为向量自回归模型（VAR process of order 2），广泛应用于时间序列预测和经济计量学中。 链图可以通过部分有向无环图（PDAG）来描述，PDAG可分解为一个有向图，其链组件的节点之间仅通过无向边连接。这允许对不同的条件概率分布进行更灵活的建模。例如下图： \\[p(A, B, \\ldots, I) = p(A) p(B) p(C, D, E | A, B) p(F, G | C, D) p(H) p(I | C, E, H)\\] 这里的每个条件概率分布（CPD）对应于条件随机场（CRF），例如： \\[p(C, D, E | A, B) = \\frac{1}{Z(A, B)} \\phi(A, C) \\phi(B, E) \\phi(C, D) \\phi(D, E)\\] \\[p(F, G | C, D) = \\frac{1}{Z(C, D)} \\phi(F, C) \\phi(G, D) \\phi(F, G)\\] 五、 扩展概率图模型（PGM） 1. 因子图（Factor Graphs） 因子图（Factor Graphs）是一种扩展了的概率图模型（PGM），它将变量之间的依赖关系明确表示了出来。主要有两种形式的因子图：双部因子图（Bipartite Factor Graph）和Forney因子图（Forney Factor Graph, FFG）。 1.1 双部因子图（Bipartite Factor Graphs） 双部因子图是一种无向图，具有两种不同类型的节点： 圆形节点代表随机变量。 方形节点代表因子，因子对应于某些函数或潜在分布。 例子解释 考虑一个马尔可夫随机场（MRF），如下图所示： 我们可以将每个最大团中的势函数转换为一个因子图。如下所示： 其中涉及变量\\(x_1, x_2, x_3, x_4\\)​，其联合分布表示为： \\[f(x_1, x_2, x_3, x_4) = f_{124}(x_1, x_2, x_4) f_{234}(x_2, x_3, x_4)\\] 这里，每个因子代表一个这些变量的势函数。通过这种方式，因子图更直观详细地描述了变量间的关系。 我们还可以对每条边而不是每个团关联一个势函数，如下所示： 其表示为： \\[f(x_1, x_2, x_3, x_4) = f_{14}(x_1, x_4) f_{12}(x_1, x_2) f_{34}(x_3, x_4) f_{23}(x_2, x_3) f_{24}(x_2, x_4)\\] 因此，通过因子图的表示法，我们可以更精确地控制变量之间的关系。 有向概率图模型也可以转换为因子图。每个条件概率分布（CPD）被转换为一个因子，并连接到使用该 CPD 的所有变量。下面是一个例子： \\[f(x_1, x_2, x_3, x_4, x_5) = f_1(x_1) f_2(x_2) f_{123}(x_1, x_2, x_3) f_{34}(x_3, x_4) f_{35}(x_3, x_5)\\] 其中，因子\\(f_{123}(x_1, x_2, x_3)\\)代表条件概率\\(p(x_3|x_1, x_2)\\)等。 1.2 Forney因子图（Forney Factor Graphs, FFG） Forney因子图也叫做标准因子图，与双部因子图不同的是： 节点表示因子。 边表示变量。变量与多个因子通过边关联。 例子解释 假设有如下因子化的函数： \\[f(x_1, ..., x_5) = f_a(x_1) f_b(x_1, x_2) f_c(x_2, x_3, x_4) f_d(x_4, x_5)\\] 该函数的 Forney 因子图如图所示 每条边代表变量。例如，\\(x_3\\)​ 只参与一个因子\\(f_c\\)，并且它只连接到一个节点，这叫做“半边”（half-edge）。这种表示方法非常适合表示某些有自然生成顺序的变量（如时间序列）。 FFG 还有一种表示来支持层次（组成）结构，即可以将复杂的变量依赖结构表示为一个黑盒。例如，如上图的(b) 展示了一个层次结构的示例，其中因子\\(f_{prior}(x_1, x_2, x_3, x_4)\\)代表复杂的联合分布，因子\\(f_{lik}(x_4, x_5)\\)代表似然函数。 当变量需要参与超过两个因子时，还可以引入等式约束节点。例如上图 展示了如何将多个变量通过等式约束节点连接。这个约束的作用是确保所有连接到该因子的变量具有相同的值，其形式化定义如下： \\[f_=(x, x_1, x_2) = \\delta(x - x_1) \\delta(x - x_2)\\] 这里，\\(\\delta(u)\\)是 Dirac delta 函数（若\\(u\\)是连续的）或 Kronecker delta 函数（若\\(u\\)是离散的），用于确保连接到此因子的变量值相等。 上图显示了等式约束节点的简化表示，这里我们使用变量\\(x\\)在多个因子之间共享，确保它们的值一致。我们可以将因子\\(f_{y|x}(y, x)\\)视为条件概率\\(p(y|x)\\)，并将相同的因子重复使用，这也是一种参数共享（parameter tying）。 六、结构因果模型（Structural Causal Models, SCM） 传统的概率模型只能用于描述世界的静态关系，而因果模型可以用于描述世界发生变化时的影响，允许我们进行干预并推断这些干预的效果。通过干预某个变量，因果模型能够预测系统中的其他变量如何受到影响。 一个SCM被定义为三元组\\(M = (U, V, F)\\)： \\(U = \\{ U_i : i = 1, \\ldots, N \\}\\)：“噪声”变量的集合，作为模型的输入。 \\(V = \\{ V_i : i = 1, \\ldots, N \\}\\)：内生变量的集合，属于模型自身。 *\\(F = \\{ f_i : i = 1, \\ldots, N \\}\\)：一组确定性函数，形式为：\\(V_i = f_i(V_, U_i)\\)其中\\(pa_i\\)是变量\\(i\\)的父节点，\\(U_i \\in U\\)是外部输入。 SCM 假设所有的因果相关因素都被包含在模型中，这被称为“因果马尔科夫假设”，即模型中的噪声项\\(U\\)是相互独立的。如果噪声项之间存在依赖关系，可以通过引入隐藏的父节点来表示这些依赖关系。 1. 示例：教育对财富的因果影响 以教育水平（X）对财富（Y）的影响为例 \\(X\\)：表示个人的教育水平，可以用一个数字来表示（例如，0 = 高中，1 = 大学，2 = 研究生）。 \\(Y\\)：表示某一时刻的财富水平。 \\(Z\\)：表示因教育所产生的债务。 在一些情况下，增加教育水平\\(X\\)可能会导致财富\\(Y\\)的增加。这种情况下，从\\(X\\)到\\(Y\\)的边被添加。 然而，接受更多教育可能会产生大量费用（在某些国家），这可能会对财富产生干扰作用。因此，从\\(X\\)到\\(Z\\)的边被添加，以反映教育水平增加通常伴随的债务增加。 同时，从\\(Z\\)到\\(Y\\)的边被添加，表示债务的增加通常会导致财富的减少。 图形表示如下，通过有向边表示变量之间的因果关系： 对应的SCM形式为： \\[X = f_X(U_x)\\] \\[Z = f_Z(X, U_z)\\] \\[Y = f_Y(X, Z, U_y)\\] 其中\\(f_X, f_Y, f_Z\\)是某组函数，\\(U_x, U_y, U_z\\)​ 是噪声变量。 我们还可以明确表示噪声项，这清楚地表明了噪声项在先验上是独立的。 2. 结构方程模型（Structural equation models） SEM 是 SCM 的一个特例，其中所有的函数关系都是线性的，噪声项服从高斯分布。这种模型在经济学和社会科学中常用，因为它建模了因果关系，并且计算上较为简单。例如前面例子： 这里我们可以用SEM表示为： \\[X = U_x\\] \\[Z = c_z + w_{xz}X + U_z\\] \\[Y = c_y + w_{xy}X + w_{zy}Z + U_y\\] 这里，\\(c\\)是常数项，\\(w\\)是权重，\\(U\\)是外生噪声。 假设噪声项服从高斯分布： \\[p(U_x) = N(U_x | 0, \\sigma^2_x)\\] \\[p(U_z) = N(U_z | 0, \\sigma^2_z)\\] \\[p(U_y) = N(U_y | 0, \\sigma^2_y)\\] 则模型可以转换为高斯的有向图模型： \\[p(X) = N(X | \\mu_x, \\sigma^2_x)\\] \\[p(Z|X) = N(Z | c_z + w_{xz}X, \\sigma^2_z)\\] \\[p(Y|X, Z) = N(Y | c_y + w_{xy}X + w_{zy}Z, \\sigma^2_y)\\] 3. SCM的干预操作（Do operator） 干预指的是改变一个或多个局部机制的行为。例如，可以强制某个变量达到特定值，记作\\(\\text{do}(X_i = x_i)\\)。这意味着我们强制将变量\\(X_i\\)​ 的值设置为\\(x_i\\)。当进行完美干预时，应“切断”指向\\(X_i\\)的边，表示该变量的值现在独立于其通常的父节点。这种操作称为“图形手术（graph surgery）”。 使用教育模型作为例子，可以通过强制\\(Z\\)为某个值（例如，支付所有人的学生贷款）来进行干预。此时，条件概率\\(p(X | \\text{do}(Z = z))\\)和\\(p(X | Z = z)\\)是不同的，因为干预改变了模型。 如果我们看到某人没有债务，可能推断他们没有接受高等教育（即\\(p(X \\geq 1 | Z = 0)\\)较小）。但如果我们实施了干预（如免除所有人的学生贷款），那么观察到某人没有债务不会影响我们对其教育背景的看法（即\\(p(X \\geq 1 | do(Z = 0)) = p(X \\geq 1)\\)）。 在更现实的场景中，可能无法将变量设置为特定值，但可以从当前值出发进行某种程度的改变。例如，可能会将每个人的债务减少一个固定的金额\\(\\Delta = -10,000\\)。这可以表示为：\\(Z = f&#39;_Z(Z, U_z)\\)其中\\(f&#39;_Z(Z, U_z) = f_Z(Z, U_z) + \\Delta\\)。 为了建模这类情景，除了上面对模型图进行修改外，我们也可以在原图上创建一个增强DAG，其中每个变量都增加了一个额外的父节点，表示该变量的机制是否以某种方式改变。在增强DAG中，可以使用标准的概率推断进行条件计算。例如：\\(p(Q | \\text{do}(A_z = a), E = e) = p(Q | A_z = a, E = e)\\) 4. 反事实推理（Counterfactuals） 反事实推理关注的是结果的原因。例如，在服用阿司匹林后，如果头痛消失，可能会问：“如果我没有服用阿司匹林，我的头痛是否仍然会消失？”Judea Pearl 据此提出了因果层次结构，包括三个分析层次，每个层次的假设越来越强，具体如下： 反事实预测过程包括： 反推：推断给定证据的潜在因素分布\\(p(U_i | A_i = a, Y_i = y_i)\\)。 干预：在结构因果模型中修改因果机制，将\\(A_i = a\\)替换为\\(A_i = a&#39;\\)。 预测：通过修改后的模型计算\\(p(Y_{a&#39;} | A_i = a, Y_i = y_i)\\)。 简单来说就是先根据观测结果得到潜在变量的分布，然后在这个潜在变量的分布的基础上，进行do operation并计算结果的分布。","categories":[{"name":"Probabilistic Machine Learning","slug":"Probabilistic-Machine-Learning","permalink":"https://jia040223.github.io/categories/Probabilistic-Machine-Learning/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jia040223.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"统计","slug":"统计","permalink":"https://jia040223.github.io/tags/%E7%BB%9F%E8%AE%A1/"}]},{"title":"[Probabilistic Machine Learning]: Fundamentals-Statistics","slug":"Fundamentals-Statistics","date":"2024-10-14T06:17:48.000Z","updated":"2024-10-27T10:47:21.223Z","comments":true,"path":"2024/10/14/Fundamentals-Statistics/","permalink":"https://jia040223.github.io/2024/10/14/Fundamentals-Statistics/","excerpt":"","text":"本学习笔记用于记录我学习Probabilistic Machine Learning的学习笔记，分享记录，也便于自己实时查看。 前面Probability部分重点是关注给定参数\\(\\theta\\)后，数据\\(D\\)的分布，即\\(P(D|\\theta)\\)，而Statistics部分则是关注给定数据分布下，参数\\(\\theta\\)的概率，即\\(P(\\theta|D)\\)。 一、贝叶斯统计 贝叶斯统计也是比较熟悉了，主要就是用贝叶斯公式进行计算后验： 这里\\(P(\\theta)\\)叫做先验，\\(P(\\theta|D)\\)是后验，\\(P(D|\\theta)\\)叫做似然。 书中以抛硬币实验来讲述了贝叶斯统计的众多概率，这里简单总结一下： Prior ：均匀分布或者Beta分布，抛硬币我们可以选择Beta分布来指定更强的先验： \\[p(θ)=Beta(θ∣α,β)∝θ ^{α−1} (1−θ) ^{β−1}\\] Posterior： 在Beta分布先验条件下可计算得到后验为： \\[p(θ∣D)∝θ^{N_1}​(1−θ)^{N_0}​⋅θ^{α−1}(1−θ)^{β−1}=Beta(θ∣α+N_1​,β+N_0​)\\] MAP 估计：即让后验最大 \\[\\hat{\\theta}_{\\text{MAP}} = \\frac{\\alpha + N_1 - 1}{\\alpha + N_1 - 1 + \\beta + N_0 - 1}\\] 用均匀分布先验则和MLE得到的结果一致。 Posterior Mean：很多时候会使用后验的均值而非峰值作为参数，: \\[\\hat{\\theta} = \\int \\theta \\cdot p(\\theta|D) d\\theta\\] Posterior Variance：表达估计的不确定性，对于抛硬币可得到标准差 \\[\\sigma = \\sqrt{V[\\theta|D]} \\approx \\sqrt{\\frac{\\hat{\\theta}(1 - \\hat{\\theta})}{N}}\\] 所以随着样本量\\(N\\)的增大，不确定性以\\(\\frac{1}{\\sqrt{N}}\\)​ 的速度下降。不确定性（方差）在\\(\\hat{\\theta} = 0.5\\)时达到最大，在\\(\\hatθ\\)接近0或1时达到最小。这表明，确定一个硬币偏向比确定它是公平的要容易得多。 Credible Intervals：置信区间，后验分布的 100(1 - α)% 置信区间定义 \\[C_\\alpha(D) = (l, u) : P(l \\leq \\theta \\leq u|D) = 1 - \\alpha\\] Posterior Predictive Distribution：假设我们希望预测未来的观测值，贝叶斯最优方法是通过边缘化未知参数来计算后验预测分布： \\[p(y|D) = \\int p(y|\\theta) p(\\theta|D) d\\theta\\] 有时计算该积分可能会很困难，这时可以使用点估计方法，选择一个参数估计值\\(\\hat{\\theta} = \\delta(D)\\)，例如 MLE 或 MAP，从而近似为： \\[p(y|D) \\approx p(y|\\hat{\\theta})\\] Marginal Likelihood：对于优化没有影响，主要在于对模型的选择上： \\[p(D|M) = \\int p(\\theta|M) p(D|\\theta, M) d\\theta \\quad\\] 还提到了一个定理de Finetti's theorem（德·芬尼蒂定理）：如果数据是可交换的，那么必然存在一个隐藏的随机变量\\(\\theta\\)，数据在给定\\(\\theta\\)的条件下是独立同分布的。这个定理为贝叶斯方法提供了理论基础。 二、频率学派统计 与贝叶斯统计不同，频率学派不将参数当作随机变量，而是依赖采样分布来表示不确定性。它通过反复采样来评估数据中的随机性和不确定性，而不是使用先验分布和后验分布。核心思想是重复实验的假设：通过观察如果在不同的数据集上重复实验，估计的量（例如参数）会如何变化，这种变化构成了不确定性的依据。 这个也很熟悉了，简单来说就是频率学派认为参数是一个值，通过不断地实验就能去估计这个值。虽然这个有一定的缺点，但其一些准则在实践中也是被广泛使用的。 1. Sampling distributions 采样分布是对某个估计器（如最大似然估计，MLE）的结果变化进行的描述。 举例来说，假设从一个真实模型\\(p(x|\\theta^*)\\)中采样多个数据集\\(D^{(s)}\\)，然后对每个数据集应用估计器来得到参数估计\\(\\hat{\\theta}(D^{(s)})\\)。通过让数据集的数量\\(S\\)趋向无穷，我们可以得到估计器的采样分布。这个分布反映了在不同的样本下，参数估计的变化情况。 2. Bootstrap 自助法 当估计器比较复杂或者样本量较小的时候，可以使用Bootstrap方法来近似采样分布。自助法的核心是通过从原始数据集中随机采样生成多个伪数据集，然后计算每个伪数据集的参数估计，最终得到估计值的经验分布。主要有两种方法： 参数自助法假设我们知道参数\\(\\theta^*\\)，我们可以生成伪数据集并计算估计值。但现实是\\(\\theta^*\\)是未知的，所以我们使用从数据中估计出的参数\\(\\hat{\\theta}\\)，这就称为“参数自助法”。 另一种是非参数自助法，它不依赖于特定的生成模型，而是直接从原始数据集中进行有放回的采样，这样每个新生成的数据集与原始数据集有相同的大小，但通常会有重复数据点。 3. 渐近正态性（Asymptotic Normality） 当样本量足够大时，最大似然估计（MLE）的采样分布会趋向于正态分布。这称为MLE的渐近正态性。在数学上，它表述为： \\[\\sqrt{N}(\\hat{\\theta} - \\theta^*) \\rightarrow N(0, F(\\theta^*)^{-1})\\] Fisher 信息矩阵其中\\(F(\\theta^*)\\)是费舍尔信息矩阵。 费舍尔信息矩阵衡量的是似然函数在真参数处的曲率，表明数据中包含的“信息量”。渐近正态性意味着，当样本量\\(N\\)趋于无穷时，估计值的分布会收敛于一个以真参数\\(\\theta^*\\)为中心的高斯分布。 4. Fisher 信息矩阵 Fisher 信息矩阵（Fisher Information Matrix, FIM）与对数似然函数的曲率密切相关。这一矩阵在频率学派统计中有重要作用，主要用于刻画最大似然估计（MLE）的采样分布。此外，Fisher 信息矩阵在贝叶斯统计中也有应用，例如推导 Jeffreys 的无信息先验，以及在优化问题中作为自然梯度下降的一部分。 定义如下： score function ： \\[s(\\theta)\\equiv\\nabla_\\theta\\log p(x|\\theta)\\] Fisher 信息矩阵 ： \\[F(\\theta) \\equiv \\mathbb{E}_{x \\sim p(x|\\theta)} \\left[ \\nabla_\\theta \\log p(x|\\theta) \\nabla_\\theta \\log p(x|\\theta)^T \\right]\\] 其第\\(i,j\\)项为： \\[F_{ij} = \\mathbb{E}_{x \\sim \\theta} \\left[ \\frac{\\partial}{\\partial \\theta_i} \\log p(x|\\theta) \\frac{\\partial}{\\partial \\theta_j} \\log p(x|\\theta) \\right]\\] 可以看到Fisher 信息矩阵与负对数似然函数（NLL, Negative Log Likelihood）有关系： \\[\\text{NLL}(\\theta) = - \\log p(D|\\theta)\\] 我们有如下定理： 定理 4.1 如果\\(\\log p(x|\\theta)\\)是二阶可微的，并且在某些正则条件下，Fisher 信息矩阵等于 NLL 的期望 Hessian 矩阵： \\[ F(\\theta)_{ij} = \\mathbb{E}_{x \\sim \\theta} \\left[ \\frac{\\partial}{\\partial \\theta_i} \\log p(x|\\theta) \\frac{\\partial}{\\partial \\theta_j} \\log p(x|\\theta) \\right] = - \\mathbb{E}_{x \\sim \\theta} \\left[ \\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} \\log p(x|\\theta) \\right] \\] 然后书上给了一些常见分布的例子 二项分布的 FIM：\\(F(\\theta) = \\mathbb{E}_{x \\sim \\theta}[-s&#39;(\\theta|x)] = \\frac{n}{\\theta(1 - \\theta)}\\) 单变量高斯分布的 FIM：\\(F(\\theta) = \\begin{pmatrix} \\frac{1}{v} &amp; 0 \\\\ 0 &amp; \\frac{1}{2v^2} \\end{pmatrix}\\) 逻辑回归的 FIM：\\(F(w) = \\mathbb{E}_{p(y|X,w,\\lambda)}[\\nabla^2 L(w)] = X^T \\Lambda X + \\lambda I\\)，其中\\(\\Lambda_{nn} = \\sigma(w^T x_n)(1 - \\sigma(w^T x_n))\\)。 指数族分布的 FIM：\\(F_\\eta = \\text{Cov}[T(x)]\\) 5. 频率学派的Counterintuitive properties 首先是频率主义的置信区间，它基于抽样分布来估计参数的不确定性。其定义是，如果重复抽取样本并计算每个样本的置信区间，那么有 95% 的区间会包含真实参数。但对于一个具体的样本，无法说参数有 95% 的概率落在置信区间内。换句话说，频率主义认为参数是固定的，数据是随机的，所以无法给出“参数在此区间的概率”。 贝叶斯方法则把数据固定，参数看作是随机的。因此，贝叶斯的可信区间给出了参数在某区间内的概率。这是人们在直觉上通常更关心的问题：已知数据后，参数落在某个范围的概率。 文中提到一个具体的例子： 然后就是p 值的误导性，p 值是指在原假设（H0）成立时，观察到某个统计量或更极端结果的概率。频率主义的假设检验通过计算 p 值来决定是否拒绝原假设，通常认为 p 值很小就意味着原假设不太可能成立。 问题是p 值经常被错误地解释为“原假设为真的概率”，但实际上它只是给出了在原假设成立的情况下，观察到数据的概率。它并没有告诉我们在看到数据后原假设是否成立，或者备择假设（H1）是否更有可能成立。 文中用了一个类比来说明 p 值的误导性。假设“如果一个人是美国人，他大概率不是国会议员”，我们观测到某人是国会议员，但这并不能推导出“这个人很可能不是美国人”。 这是一个典型的错误推理，类似于依赖 p 值来判断假设的真实性。相反，贝叶斯方法会使用贝叶斯定理结合数据推导出假设的后验概率，更符合人们的直觉。 三、共轭先验 如果先验分布\\(p(\\theta)\\)属于某个参数化家族\\(F\\)，并且后验分布\\(p(\\theta|D)\\)也在该家族中，则称\\(p(\\theta)\\)为\\(p(D|\\theta)\\)的共轭先验。这意味着贝叶斯更新后，分布保持在同一个家族中，便于计算。 书中详细介绍了常见分布的共轭先验，这里仅做总结： 3.1 二项分布 共轭先验：贝塔分布 \\[p(\\theta) = \\text{Beta}(\\theta | \\alpha, \\beta)\\] 更新公式：在观察到\\(k\\)次成功和\\(n\\)次试验后，后验分布为： \\[p(\\theta | k, n) = \\text{Beta}(\\theta | \\alpha + k, \\beta + n - k)\\] 3.2. 多类分布 共轭先验：狄利克雷分布： \\[p(\\boldsymbol{\\theta}) = \\text{Dirichlet}(\\boldsymbol{\\theta} | \\boldsymbol{\\alpha})\\] 更新公式：如果观察到类别\\(i\\)的次数为\\(n_i\\)​，则后验分布为 \\[p(\\boldsymbol{\\theta} | \\mathbf{n}) = \\text{Dirichlet}(\\boldsymbol{\\theta} | \\boldsymbol{\\alpha} + \\mathbf{n})\\] 其中\\(\\mathbf{n} = (n_1, n_2, \\ldots, n_k)\\)表示每个类别的观察次数。 3.3 单变量高斯模型（Univariate Gaussian Model） 3.3.1 给定\\(\\sigma^2\\)的后验 共轭先验：另一个高斯分布： \\[N(\\mu|m_0, \\tau_0^2)\\] 更新公式：后验分布也为高斯分布，参数为： \\[ \\begin{align} \\hat{\\tau}^2 &amp;= \\frac{1}{\\frac{1}{\\sigma^2} + \\frac{N}{\\tau_0^2}} \\\\ \\hat{m} &amp;= \\hat{\\tau}^2 \\left( \\frac{m_0}{\\tau_0^2} + \\frac{N\\bar{y}}{\\sigma^2} \\right) \\end{align} \\] 这里，\\(\\bar{y}\\)是样本均值。 3.3.2 给定\\(μ\\)的后验 共轭先验为逆伽马分布： \\[\\text{IG}(\\sigma^2|\\alpha_0, \\beta_0) \\propto (\\sigma^2)^{-\\alpha_0 - 1} \\exp\\left(-\\frac{\\beta_0}{\\sigma^2}\\right)\\] 更新公式：后验分布也是逆伽马分布，参数为： \\[ \\begin{align} &amp;\\hat{\\alpha} = \\alpha_0 + \\frac{N}{2} \\\\ &amp;\\hat{\\beta} = \\beta_0 + \\frac{1}{2}\\sum_{n=1}^{N}(y_n - \\mu)^2 \\end{align} \\] 3.3.3 对于均值\\(\\mu\\)和方差\\(\\sigma^2\\)的推断 共轭先验：正态-逆伽马分布（NIG）： \\[NIG(\\mu, \\sigma^2 | m, \\kappa, a, b) \\equiv N(\\mu | m, \\frac{\\sigma^2}{\\kappa}) IG(\\sigma^2 | a, b)\\] 一般就用正态-逆卡方分布（NIX）： \\[NI\\chi^2(\\mu, \\sigma^2 | m, \\kappa, \\nu, \\tau^2) \\equiv N(\\mu | m, \\frac{\\sigma^2}{\\kappa}) \\chi^{-2}(\\sigma^2 | \\nu, \\tau^2)\\] 更新公式：后验参数更新： \\[ \\begin{align} &amp;\\hat{m} = \\frac{\\kappa m + N\\hat{x}}{\\hat{\\kappa}} \\\\ &amp;\\hat{\\kappa} = \\kappa + N \\\\ &amp;\\hat{\\nu} = \\nu + N \\\\ &amp;\\hat{\\nu} \\hat{\\tau}^2 = \\nu \\tau^2 + \\sum_{n=1}^{N}(y_n - \\bar{y})^2 + \\frac{N\\kappa}{\\kappa + N}(m - \\bar{y})^2 \\end{align} \\] 方差的后验边际分布： \\[p(\\sigma^2 | D) = \\chi^{-2}(\\sigma^2 | \\hat{\\nu}, \\hat{\\tau}^2)\\] 均值的后验边际分布： \\[p(\\mu | D) = T(\\mu | \\hat{m}, \\frac{\\hat{\\tau}^2}{\\hat{\\kappa}}, \\hat{\\nu})\\] 3.4. 对于多变量高斯模型 3.4.1 在给定 Σ 的情况下推断 µ 共轭先验：高斯分布: \\[p(\\mu) = N(\\mu | m, V)\\] 后验分布： : \\[p(\\mu | D, \\Sigma) = N(\\mu | \\hat{m}, \\hat{V})\\] 更新公式： \\[ \\begin{align} &amp;\\hat{V}^{-1} = V^{-1} + N\\Sigma^{-1} \\\\ &amp;\\hat{m} = \\hat{V}(\\Sigma^{-1}(Ny) + V^{-1}m) \\end{align} \\] 3.4.2 在给定 µ 的情况下推断 Σ 共轭先验：逆Wishart分布: \\[p(\\Sigma) = IW(\\Sigma | \\Psi^{-1}, \\nu)\\] 后验分布： : \\[p(\\Sigma | D, \\mu) \\propto IW(\\Sigma | \\hat{\\Psi}, \\hat{\\nu})\\] 更新公式： \\[ \\begin{align} &amp;\\hat{\\nu} = \\nu + N\\\\ &amp;\\hat{\\Psi} = \\Psi + S_\\mu \\end{align} \\] 3.4.3 同时推断 Σ 和 µ 共轭先验： \\[p(\\mu, \\Sigma) = N(\\mu | m, V) IW(\\Sigma | \\Psi^{-1}, \\nu)\\] 后验分布： \\[p(\\mu, \\Sigma | D) \\propto |\\Sigma|^{-\\frac{N + \\nu + D + 2}{2}} \\exp\\left(-\\frac{1}{2} \\text{tr}(\\Sigma^{-1} M)\\right)\\] 其中\\(M\\)是更新后的散点矩阵。 3.5 指数族模型 唯一存在共轭先验的分布族是指数族，具体如下： 共轭先验：我们可以将先验分布写成与似然函数相似的形式： \\[p(\\eta|\\tilde{\\tau}, \\tilde{\\nu}) = \\frac{1}{Z(\\tilde{\\tau}, \\tilde{\\nu})} \\exp\\left(\\tilde{\\tau}^T \\eta - \\tilde{\\nu} A(\\eta)\\right)\\] 其中，\\(\\tilde{\\nu}\\)是先验的强度，\\(\\frac{\\tilde{\\tau}}{\\tilde{\\nu}}\\)​ 是先验均值，\\(Z(\\tilde{\\tau}, \\tilde{\\nu})\\)是归一化因子。 后验分布： \\[ \\begin{align} p(\\eta|D) &amp;= \\frac{p(D|\\eta) p(\\eta)}{p(D)} \\\\ &amp;= \\frac{h(D)}{Z(\\tilde{\\tau}, \\tilde{\\nu})p(D)} \\exp\\left( (\\tilde{\\tau} + s(D))^T \\eta - (\\tilde{\\nu} + N) A(\\eta)\\right) \\\\ &amp;= \\frac{1}{Z(\\hat{\\tau}, \\hat{\\nu})} \\exp\\left(\\hat{\\tau}^T \\eta - \\hat{\\nu} A(\\eta)\\right) \\end{align} \\] 其中： \\[ \\begin{align} &amp;\\hat{\\tau} = \\tilde{\\tau} + s(D)\\\\ &amp;\\hat{\\nu} = \\tilde{\\nu} + N \\\\ &amp;Z(\\hat{\\tau}, \\hat{\\nu}) = \\frac{Z(\\tilde{\\tau}, \\tilde{\\nu})h(D)}{p(D)} \\end{align} \\] 我们看到，后验分布与先验分布具有相同的形式，只是更新了充分统计量和样本大小。后验均值为先验均值与经验均值（即最大似然估计）之间的组合： \\[\\begin{align*} E[\\eta|D] &amp;= \\frac{\\hat{\\tau}}{\\hat{\\nu}} \\\\ &amp;= \\frac{\\tilde{\\tau} + s(D)}{\\tilde{\\nu} + N} \\\\ &amp;= \\frac{\\tilde{\\nu}}{\\tilde{\\nu} + N} \\frac{\\tilde{\\tau}}{\\tilde{\\nu}} + \\frac{N}{\\tilde{\\nu} + N} \\frac{s(D)}{N} \\\\ &amp;= \\lambda E[\\eta] + (1 - \\lambda) \\hat{\\eta}_{MLE} \\end{align*}\\] 其中，\\(\\lambda = \\frac{\\tilde{\\nu}}{\\tilde{\\nu} + N}\\)​。 边际似然： \\[p(D) = \\frac{Z(\\hat{\\tau}, \\hat{\\nu}) h(D)}{Z(\\tilde{\\tau}, \\tilde{\\nu})}\\] 后验预测密度：我们现在推导给定过去数据\\(D = (x_1, ..., x_N)\\)时，未来观测\\(D&#39; = (x&#39;_1, ..., x&#39;_{N&#39;})\\)的预测密度，如下： \\[\\begin{align*} p(D&#39;|D) &amp;= \\int p(D&#39;|\\eta) p(\\eta|D) d\\eta \\\\&amp;=h(D&#39;) \\frac{Z(\\tilde{\\tau} + s(D) + s(D&#39;), \\tilde{\\nu} + N + N&#39;)}{Z(\\tilde{\\tau} + s(D), \\tilde{\\nu} + N)} \\end{align*}\\] 四、无信息先验 在缺乏领域特定知识时，我们不希望主观定义不合理的先验，于是我们便可以选择无信息先验客观。主要有如下几种： 1. 最大熵先验（Maximum entropy priors） 最大熵先验是一种不做过多假设的先验分布，适合在没有充足信息的情况下使用。通过最大化熵来选择先验，这种方法依赖于拉普拉斯提出的“不充分理由原则”，即当我们没有理由偏向某个特定值时，应选择“平坦”的分布。例如，对于伯努利分布的参数 θ（取值范围 [0,1]），最大熵先验是均匀分布。 我们也可以根据已知约束来定义最大熵先验，使其在满足这些约束的同时使得熵最大化。书中举了一个例子： 2. 杰弗里斯先验（Jeffreys priors） Jeffreys priors通过保证对参数化不敏感，即在不同的参数化方式下，后验分布不会改变。杰弗里斯先验的一个关键特性是对参数的变化保持不变，这意味着无论采用何种参数化方式，结果应该是一致的。 参数\\(\\theta\\)的 Jeffreys Prior 为以下形式： \\[p_{J}(\\theta) \\propto \\sqrt{\\mathcal{I}(\\theta)}\\] 其中，\\(\\mathcal{I}(\\theta)\\)是我们所熟知的Fisher信息量。证明如下： 例如对于伯努利分布，其杰弗里斯先验是 Beta(1/2, 1/2) 分布。 3. 不变性先验（Invariant priors） 不变性先验是指当我们知道某些不变性时，可以将其编码进先验中。例如： 平移不变先验：对位置参数的推断可以使用平移不变先验，这种先验在任何相同宽度的区间上都分配相同的概率质量。 尺度不变先验：对尺度参数的推断可以使用尺度不变先验，其满足任意比例缩放后保持相同概率质量。 4. 参考先验（Reference priors） 参考先验通过最大化数据集上的后验与先验之间的KL散度来定义。它旨在使先验尽可能远离所有可能的后验分布，从而保持非信息性。参考先验可以看作是对不同数据集的互信息最大化问题。对于一维情况，参考先验等同于Jeffreys priors，而在高维情况下，计算起来则更复杂。 五、层次先验（Hierarchical priors） 贝叶斯模型需要为参数\\(\\theta\\)指定先验\\(p(\\theta)\\)，而先验的参数（超参数\\(\\xi\\)）也是未知的。为了处理这种不确定性，我们可以对超参数\\(\\xi\\)再定义一个先验，从而构建层次贝叶斯模型。这种模型的形式化表达为：\\(p(\\xi, \\theta, D) = p(\\xi) p(\\theta | \\xi) p(D | \\theta)\\) 这表明数据\\(D\\)通过参数\\(\\theta\\)依赖于超参数\\(\\xi\\)，从而形成一个层次结构。 在实际问题中，如果我们有多个相关的数据集\\(D_j\\)​，各个数据集有自己的参数\\(\\theta_j\\)，那么分别独立估计每个\\(\\theta_j\\)​ 可能会产生不可靠的结果，特别是当某个数据集较小时。层次模型可以通过共享超参数\\(\\xi\\)来借用数据量大的群体的信息，帮助数据量小的群体进行更好的估计. 1. 层次二项模型的例子 问题背景：假设我们想估计不同群体中某种疾病的患病率，每个群体的样本量是\\(N_j\\)​，阳性病例数是\\(y_j\\)。我们可以假设\\(y_j\\)服从二项分布\\(\\text{Bin}(N_j, \\theta_j)\\)，其中\\(\\theta_j\\)​ 是该群体的患病率。 如果直接对每个群体单独估计\\(\\theta_j\\)，特别是当样本量\\(N_j\\)很小时，可能会导致不可靠的结果。比如，如果\\(y_j = 0\\)，我们可能会估计\\(\\hat{\\theta_j} = 0\\)，尽管实际的患病率可能更高。 解决方案：为了避免这种问题，可以假设所有的\\(\\theta_j\\)不是独立的，而是从一个共同的 Beta 分布中抽取，即\\(\\theta_j \\sim \\text{Beta}(a, b)\\)。这个假设允许我们通过共享的先验\\(\\xi = (a, b)\\)来提高估计的可靠性。这种模型的联合分布可以写作： \\[p(D, \\theta, \\xi) = p(\\xi) \\prod_{j=1}^{J} \\text{Beta}(\\theta_j | \\xi) \\prod_{j=1}^{J} \\text{Bin}(y_j | N_j, \\theta_j)\\] 后验推断：可以通过Hamiltonian Monte Carlo（HMC）算法来进行后验推断，生成超参数\\(\\xi\\)和群体参数\\(\\theta_j\\)​ 的样本。对于每个群体，后验均值\\(E[\\theta_j | D]\\)会根据数据量的大小进行调整。对于数据较少的群体，估计值会向全体群体的均值（共享信息）靠拢，这种现象被称为收缩（shrinkage）。 2. 层次高斯模型的例子 问题背景：现在考虑实数值数据的情况，假设我们有多个群体的数据，每个群体的数据\\(y_{ij}\\)服从正态分布\\(N(\\theta_j, \\sigma^2)\\)，其中\\(\\theta_j\\)​ 是该群体的均值，\\(\\sigma^2\\)是固定的方差。 与二项模型类似，我们可以假设各群体的均值\\(\\theta_j\\)​ 来自一个共同的正态分布\\(\\theta_j \\sim N(\\mu, \\tau^2)\\)。这个模型的联合分布为： \\[p\\propto p(\\mu)p(\\tau^2) \\prod_{j=1}^{J} N(\\theta_j | \\mu, \\tau^2) N(y_j | \\theta_j, \\sigma_j^2)\\] 其中\\(p(\\mu)\\)和\\(p(\\tau^2)\\)是超参数的先验分布，可以假定\\(\\sigma_j^2\\)是知道的。 对于每个群体，后验均值\\(E[\\theta_j | D]\\)会介于单独的最大似然估计值\\(\\hat{\\theta_j}\\)和全局均值\\(\\mu\\)之间。根据公式： \\[E[\\theta_j | D, \\mu, \\tau^2] = w_j \\mu + (1 - w_j) \\hat{\\theta_j}\\] ​其中收缩系数\\(w_j = \\frac{\\sigma_j^2}{\\sigma_j^2 + \\tau^2}\\)​​。数据量较小或不确定性较高的群体（即\\(\\sigma_j^2\\)较大的群体）会有更大的收缩，意味着它们的估计值会更多地依赖于全局均值。 为了解决算法在进行后验推断时的计算效率问题，可以采用非中心化参数化（non-centered parameterization）。这种方法通过重新表达\\(\\theta_j = \\mu + \\tau \\eta_j\\)，其中\\(\\eta_j \\sim N(0, 1)\\)，从而减少参数之间的依赖性，提升推断的计算效率。 六、经验贝叶斯 对于层次贝叶斯模型，在全贝叶斯推断中，我们对底层参数和超参数同时进行推断，计算\\(p(\\theta, \\xi | D)\\)的联合后验分布。虽然这种方法在统计上是更为严格的，但计算量通常较大。经验贝叶斯提供了一种近似方法，首先通过最大化边际似然（如\\(p(D|\\xi)\\)）估计超参数\\(\\xi\\)，然后在给定这些估计值的条件下推断底层参数的后验分布（如\\(p(\\theta|\\hat{\\xi}, D)\\)）。这种方法通过对超参数做点估计，而非推断它们的后验分布，因此简化了计算。 通过边际似然最大化来估计超参数是经验贝叶斯的核心步骤。具体来说，经验贝叶斯在给定数据\\(D\\)的条件下，通过最大化边际似然\\(p(D|\\xi)\\)来找到最优的超参数估计值\\(\\hat{\\xi}\\)​。这种方法有时也被称为II类最大似然，因为它不是直接优化底层参数\\(\\theta\\)，而是先优化超参数\\(\\xi\\)。 1. 经验贝叶斯在层次二项模型中的应用 在二项分布的层次模型中，经验贝叶斯的边际似然可以通过积分将底层参数\\(\\theta_j\\)​ 消除掉，从而直接用超参数\\(\\xi\\)表示边际似然。具体公式是： \\[p(D|\\xi) = \\prod_j \\int \\text{Bin}(y_j|N_j, \\theta_j) \\text{Beta}(\\theta_j | a, b) d\\theta_j\\] 经验贝叶斯方法通过最大化此边际似然来估计超参数\\(a\\)和\\(b\\)，然后在给定这些估计值后，再计算每个\\(\\theta_j\\)的后验分布。 2. 经验贝叶斯在层次高斯模型中的应用 在层次高斯模型中，经验贝叶斯可以通过边际化\\(\\theta_j\\)​ 得到边际似然，并利用最大似然估计超参数\\(\\mu\\)和\\(\\tau^2\\)。在这个例子中，边际似然公式是： \\[p(D|\\mu, \\tau^2, \\sigma^2) = \\prod_{j=1}^{J} N(y_j | \\mu, \\tau^2 + \\sigma^2)\\] 然后通过矩匹配方法估计\\(\\tau^2\\)和\\(\\mu\\)。 3. 经验贝叶斯在马尔可夫模型中的应用 经验贝叶斯还可以用于语言模型中的 n-gram 平滑问题。在这个上下文中，经验贝叶斯被用来估计马尔可夫链中状态转移矩阵的先验分布。通过为转移矩阵的每一行设定一个独立的狄利克雷分布作为先验，经验贝叶斯可以通过最大化边际似然来估计先验参数\\(\\alpha\\)和\\(m\\)，从而得到自适应的平滑方法。这个方法的优势在于，它能够根据数据自动调整平滑参数\\(\\lambda_j\\)，从而提高模型的表现。 在 n-gram 语言模型中，我们希望计算不同词之间的转移概率，比如在二元模型（bigram model）中，给定词\\(X_t = j\\)，下一个词\\(X_{t+1} = k\\)的概率可以由转移矩阵\\(A_{jk}\\)​ 来表示。传统的加一平滑方法对每一个可能的词对\\((j, k)\\)都假设了一个等价的概率，但这种假设往往过于简单。删除插值（deleted interpolation）是一个更复杂的方案，定义了如下的转移矩阵表示： \\[A_{jk} = (1 - \\lambda) f_{jk} + \\lambda f_k\\] 其中，\\(f_{jk} = \\frac{N_{jk}}{N_j}\\)是从词\\(j\\)到词\\(k\\)的 bigram 频率，\\(f_k = \\frac{N_k}{N}\\)​​ 是词\\(k\\)的 unigram 频率，而\\(\\lambda\\)是一个通过交叉验证选择的平滑参数。 然而，删除插值方法没有考虑不同的上下文在词的频率中可能有不同的重要性。贝叶斯方法则可以为每个上下文动态地调整平滑参数\\(\\lambda_j\\)​。 通过经验贝叶斯的方法对删除插值进行了重新解释。首先，假设转移矩阵的每一行都遵循独立的 Dirichlet 先验分布： \\[A_j \\sim Dir(\\alpha_0 m_1, \\dots, \\alpha_0 m_K) = Dir(\\alpha_0 m) = Dir(\\alpha)\\] 其中，\\(A_j\\)是转移矩阵的第\\(j\\)行，\\(m\\)是先验均值向量，满足\\(\\sum_k m_k = 1\\)，而\\(\\alpha_0\\)​ 是先验强度。通过贝叶斯推断，可以得到转移矩阵行\\(A_j\\)的后验分布： \\[A_j \\sim Dir(\\alpha + N_j)\\] 其中\\(N_j = (N_{j1}, \\dots, N_{jK})\\)是从状态\\(j\\)转移到其他状态的计数向量。在此基础上，后验预测密度为： \\[p(X_{t+1} = k | X_t = j, D) = \\frac{N_{jk} + \\alpha_j m_k}{N_j + \\alpha_0}\\] 这个公式可以改写为删除插值的形式： \\[p(X_{t+1} = k | X_t = j, D) = (1 - \\lambda_j) f_{jk} + \\lambda_j m_k\\] 其中，\\(\\lambda_j = \\frac{\\alpha_j}{N_j + \\alpha_0}\\)是动态调整的平滑参数，表示给定上下文\\(j\\)时，将先验分布与经验数据相结合的权重。 EB 方法的核心思想是通过数据来估计 Dirichlet 分布的超参数\\(\\alpha\\)和\\(m\\)。在这个问题中，有一种近似方法来估计先验均值\\(m\\)： \\[m_k \\propto |\\{ j : N_{jk} &gt; 0 \\}|\\] 这个估计意味着某个词\\(k\\)的先验概率与该词出现在多少种不同的上下文中有关，而不是它的具体出现次数。这种估计方法可以解决某些平滑方法中的不足。举个例子，如果在一个数据集中“you see”频繁出现，那么虽然 “you” 和 “see” 的 unigram 频率相同，但是它们在新上下文中出现的概率不应该相等。贝叶斯模型通过先验分布的参数\\(m_k\\)​ 可以自适应地处理这种情况。 七、模型选择 在统计建模中，选择合适的模型至关重要。所有模型都存在一定的误差，然而某些模型能更好地适应数据，提供有用的预测。选择模型时，我们需要考虑模型的假设和拟合能力，确保它能在现实世界中应用。 1. 贝叶斯模型选择 贝叶斯模型选择的一个自然的想法是利用后验概率来确定最有可能生成数据的模型。公式如下： \\[\\hat{m} = \\arg\\max_{m \\in M} p(m|D)\\] 这里的\\(p(m|D)\\)表示给定数据\\(D\\)下模型\\(m\\)的后验概率。根据贝叶斯定理，我们可以表示为： \\[p(m|D) = \\frac{p(D|m)p(m)}{\\sum_{m \\in M} p(D|m)p(m)}\\] 如果模型的先验是均匀的，即\\(p(m) = \\frac{1}{|M|}\\)，那么最大后验模型是： \\[\\hat{m} = \\arg\\max_{m \\in M} p(D|m)\\] 但是不同的模型设计是有好坏之分的。 示例：硬币是否公平？ 假设我们想知道某个硬币是否是公平的。我们可以设定两个模型： \\(M_0\\)：假设硬币是公平的，即\\(\\theta = 0.5\\)。 \\(M_1\\)​：假设硬币是偏向的，即\\(\\theta\\)可以是任意值。 通过比较这两个模型的边际似然，我们可以决定哪个模型更有可能解释观察到的数据。例如： 在公平硬币模型下，观察到\\(N\\)次投掷的边际似然是： \\[p(D|M_0) = \\left( \\frac{1}{2} \\right)^N\\] 在偏向硬币模型下，边际似然更复杂，需要计算贝塔分布的积分： \\[p(D|M_1) = \\int p(D|\\theta)p(\\theta|M_1)d\\theta\\] 如果观察到的正面次数较多，模型\\(M_1\\)​ 的可能性会更高。可能说明\\(M_0\\)的先验并不好。 2. 贝叶斯模型平均 如果我们的目标是进行准确的预测，综合所有模型的预测结果通常比只依赖单一模型更好。贝叶斯模型平均可以表示为： \\[p(y|D) = \\sum_{m \\in M} p(y|m)p(m|D)\\] 这里\\(p(y|m)\\)是模型\\(m\\)对新数据\\(y\\)的预测。通过对所有模型的预测进行加权平均，我们可以得到更稳健的结果。 与机器学习中的集成技术类似，我们取预测器的加权组合。然而，集成的权重不必总和为 1，尤其是在贝叶斯模型平均中，如果有一个最佳模型\\(m^*\\)，在大样本极限下，\\(p(m|D)\\)将成为一个在\\(m^*\\)上的退化分布，其他模型将被忽略。 3 边际似然估计 为了进行贝叶斯模型选择，我们需要计算在给定先验的条件下的边际似然： \\[p(D|m) = \\int p(D|\\theta, m)p(\\theta|m)d\\theta\\] 这里的积分通常难以直接计算。对于共轭先验模型，边际似然可以解析计算，这类模型因其先验与后验分布形式一致，使得边际似然的计算变得简单明了。 但对于其它的，我们可以使用变分推断或蒙特卡洛方法来估计。文中还给了一个Harmonic mean estimator的方法： 4. 交叉验证与边际似然的联系 交叉验证是一种评估模型预测能力的常用方法。它通过将数据划分为训练集和验证集来评估模型的表现。留一交叉验证（LOO-CV）是一种特殊的情况，其中每次留出一个样本进行测试，其他样本用于训练。 交叉验证的结果可以用来估计模型的泛化能力。 它与对数边际似然（LML）有很一定的关系： 5. 条件边际似然 边际似然用于回答“从先验生成训练数据的可能性有多大？”。它适用于在不同的固定先验之间进行假设检验，但很多时候我们更关心的是“后验能够生成数据分布中的新样本的概率是多少？”，这与模型的泛化性能相关联。 研究表明，边际似然有时可能与模型的泛化性能负相关。这是因为边际似然可能会出现先验较差但模型快速适应数据的情况 为了解决这个问题，研究者们提出了条件对数边际似然（CLML），公式为： \\[CLML(m) = \\sum_{n=K}^{N} \\log p(D_n|D_{1:n-1}, m)\\] 其中，\\(K \\in \\{1, \\dots, N\\}\\)是算法的一个参数。CLML通过给定前\\(K\\)个数据点的后验分布来评估后续\\(N - K\\)个数据点的边际似然。这种方法减少了数据点顺序对结果的依赖。特别地，当\\(K = N - 1\\)并对所有数据顺序进行平均时，这种方法相当于留一法（LOO）估计。 6. 贝叶斯留一法估计 对于监督模型来说，一个我们关注的点是ELPD（expected log-pointwise predictive density），ELPD 是对未来数据的预测性能进行估计的度量： \\[ELPD(m) = \\mathbb{E}_{(x^*, y^*)} \\left[ \\log p(y^*|x^*, D, m) \\right]\\] 由于未来数据未知，因此可以使用 LOO 近似，即将部分数据点从数据集中移除并计算其预测分布： \\[ELPD_{\\text{LOO}}(m) = \\sum_{n=1}^N \\log p(y_n|x_n, D_{-n}, m)\\] 直接计算\\(ELPD_{\\text{LOO}}\\)​ 需要计算\\(N\\)次不同的后验分布，这比较慢。可以通过只计算一次后验分布\\(p(\\theta|D, m)\\)，然后使用重要性采样近似 LOO 积分。 重要性采样的核心思想是定义目标分布\\(f(\\theta) = p(\\theta|D_{-n}, m)\\)，并使用已知的提议分布\\(g(\\theta) = p(\\theta|D, m)\\)，计算重要性权重： \\[w_{s,-n} = \\frac{f(\\theta_s)}{g(\\theta_s)} \\propto \\frac{1}{p(D_n|\\theta_s)}\\] 将这些权重进行归一化后，可以用来近似 LOO 估计： \\[ELPD_{\\text{IS-LOO}}(m) = \\sum_{n=1}^N \\log \\sum_{s=1}^S \\hat{w}_{s,-n} p(y_n|x_n, \\theta_s, m)\\] 重要性采样的一个问题是，权重的方差可能非常大，导致一些权重值过大。为了解决这个问题，可以对每个样本的权重拟合一个 Pareto 分布，从而对权重进行平滑。这样可以减少异常值对 LOO 估计的影响。 八、模型检测与假设检验 1. 后验预测检查（Posterior Predictive Checks）： 通过已知数据和模型生成未来的合成数据，以评估真实数据与模型生成的数据是否相似。如果模型生成的数据与真实数据差异很大，说明模型无法捕捉数据中的某些特征，模型可能不适合。 2.贝叶斯p值 通过计算贝叶斯p值来量化模型的合理性。如果观测到的测试统计量位于预测分布的极端部分（即p值接近0或1），说明模型无法合理解释数据 3. 假设检验 与频率学派的假设检验相对，贝叶斯方法提供了假设检验的两种替代方案： 使用贝叶斯因子进行模型比较：贝叶斯假设检验不再将检验统计量与临界值进行比较，而是评估在两种模型下数据的边际似然比——零假设\\(M_0\\)​ 和替代假设\\(M_1\\)​。这个比值称为贝叶斯因子，表示为： \\(B_{1,0} = \\frac{p(D | M_1)}{p(D | M_0)}\\)如果\\(B_{1,0} &gt; 1\\)，我们倾向于支持\\(M_1\\)，否则我们更倾向于支持\\(M_0\\)​。 * 基于参数估计：即估计在假设条件下附近的概率。例如，要测试硬币是否公平，我们计算正面概率\\(\\theta\\)的后验分布，并检查接近 0.5 的区域中有多少概率质量。","categories":[{"name":"Probabilistic Machine Learning","slug":"Probabilistic-Machine-Learning","permalink":"https://jia040223.github.io/categories/Probabilistic-Machine-Learning/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jia040223.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"数学","slug":"数学","permalink":"https://jia040223.github.io/tags/%E6%95%B0%E5%AD%A6/"}]},{"title":"[旅游日志] :青甘大环线","slug":"青甘大环线","date":"2024-10-09T10:53:32.000Z","updated":"2024-10-09T11:55:06.761Z","comments":true,"path":"2024/10/09/青甘大环线/","permalink":"https://jia040223.github.io/2024/10/09/%E9%9D%92%E7%94%98%E5%A4%A7%E7%8E%AF%E7%BA%BF/","excerpt":"","text":"2024年的国庆假期，笔者忙完了保研，于是和高中同学一起去青甘大环线进行了7天的自驾游，在这记录保存一下笔者看到的祖国大西北的美景。 青海湖 大环线的第一站，青海湖比我想象中大很多，可惜景点只有一小部分。 茶卡盐湖 第二站去的茶卡盐湖，说是中国的天空之镜，可惜去的是晚上，没有欣赏到。btw，去茶卡盐湖一定要买小火车，笔者走了好久但还是没走到终点。 水上雅丹 第二天去的水上雅丹，总体来说比较独特的地貌。 翡翠湖 翡翠湖估计是我觉得这趟旅行最好看的经典了，真的非常好看。 莫高窟 莫高窟洞窟内不让拍照，只拍了外面，总体来说体验一波佛教文化。 鸣沙山和月牙泉 鸣沙山是很高沙丘，笔者玩了滑沙的项目。国庆晚上会有万人演唱会，氛围还是可以的。 嘉峪关 天下第一雄关，也算是人文景观了。 七彩丹霞 最后驿站去的七彩丹霞，还是十分好看的。 其它 一些有意思的图片，第一张是买的沙瓶纪念品。","categories":[{"name":"生活blog","slug":"生活blog","permalink":"https://jia040223.github.io/categories/%E7%94%9F%E6%B4%BBblog/"},{"name":"旅行日志","slug":"生活blog/旅行日志","permalink":"https://jia040223.github.io/categories/%E7%94%9F%E6%B4%BBblog/%E6%97%85%E8%A1%8C%E6%97%A5%E5%BF%97/"}],"tags":[{"name":"旅行日志","slug":"旅行日志","permalink":"https://jia040223.github.io/tags/%E6%97%85%E8%A1%8C%E6%97%A5%E5%BF%97/"}]},{"title":"[Probabilistic Machine Learning]: Fundamentals-Probability","slug":"Fundamentals-Probability","date":"2024-10-08T09:02:34.000Z","updated":"2024-10-27T10:55:03.643Z","comments":true,"path":"2024/10/08/Fundamentals-Probability/","permalink":"https://jia040223.github.io/2024/10/08/Fundamentals-Probability/","excerpt":"","text":"本学习笔记用于记录我学习Probabilistic Machine Learning的学习笔记，分享记录，也便于自己实时查看。 一、Probability基础知识 1. Probability space 概率空间是一个三元组\\((Ω，F，P)\\)，其中\\(Ω\\)是样本空间，是实验可能结果的集合；\\(F\\)是事件空间，它是\\(Ω\\)所有可能子集的集合；\\(P\\)是概率函数，它是从事件\\(E \\subsetΩ\\)到\\([0,1]\\)中的一个数(即\\(P: F→[0,1]\\))的映射，它满足一定的一致性等要求，具体如下： 2. 其它 离散随机变量定义，连续随机变量定义，条件概率，贝叶斯公式... 具体略过 二、常见分布 1. Discrete distributions 1.1 Bernoulli and binomial distributions： 伯努利分布和二项分布，也很熟悉了 1.2 Categorical and multinomial distribution： 其实也就是对伯努利分布和二项分布在更多的类别上的分布： 1.3 Poisson distribution 泊松分布，本科课程也重点学习过： 1.4 Negative binomial distribution 负二项分布又称帕斯卡分布（巴斯卡分布），它表示，已知一个事件在伯努利试验中每次的成功的概率是\\(p\\)，在一连串伯努利实验中，直到失败\\(r\\)次，此时成功次数作为随机变量\\(x\\)。 \\(r=1\\)时，即为几何分布。 2. 分布在R上的Continuous distributions 2.1 Gaussian （Normal） 高斯分布，最经典的分布 2.2 Half-normal 半正态分布即一个高斯分布的绝对值（比如很多时候建模需要非负） 2.3 Student t-distribution t-分布也比较熟悉了： 2.4 Cauchy distribution Cauchy distribution是t-分布的特例： 2.5 Laplace distribution 拉帕拉斯分布也是很有名的分布，把高斯分布的平方改成了绝对值： 2.6 Sub-Gaussian and super-Gaussian distributions 其实就是比较尾部衰减速度。超高斯分布式指随机过程\\(X\\)的四阶累计量恒大于零，并且关于其均值对称分布。而亚高斯分布就是恒小于零。 例如拉普拉斯分布就是超高斯分布一种，而均匀分布就是亚高斯分布一种。 具体对比如图： 3. 分布在正实数上的Continuous distributions 3.1 Gamma distribution 伽马分布以伽马函数为基础，非常灵活。 “指数分布”和“\\(χ^{2}\\)分布”都是伽马分布的特例。 概率分布的可视化如下： 3.2 Exponential distribution 指数分布，伽马分布的特例。本科课程也重点学习过。 3.3 Chi-squared distribution \\(χ^{2}\\)分布，伽马分布的特例。也比较熟悉了。 3.4 Inverse gamma 倒伽马分布是伽马分布变量的倒数。倒\\(χ^{2}\\)分布是其特例。 3.5 Pareto distribution 帕累托分布是以意大利经济学家维弗雷多·帕雷托命名的。 是从大量真实世界的现象中发现的幂定律分布。其形式如下： 可以注意到，对概率分布取对数，则会得到一个线性函数，所以NLP中大名鼎鼎的齐夫定律便服从这个分布： 对于尾部比较大的分布的建模，帕累托分布是有用的，现实中许多形式的数据都具有这种特性。如： 财富在个人之间的分布（80%的人掌握20%的财富） 人类居住区的大小 对维基百科条目的访问 一般认为这是因为数据是由各种潜在因素产生的，当这些潜在因素混合在一起时，自然会导致这种重尾的分布。 其概率分布的直观展示如下： 4. 分布在[0, 1]上的Continuous distributions 4.1 Beta distribution 所谓的以\\(\\alpha, \\beta\\)为参数的 Beta 分布\\(f(x; \\alpha, \\beta)\\)，其实描述的就是我们在做抛硬币实验的过程中，我们当前如果已经观测到\\(\\alpha + 1\\)次正面，\\(\\beta + 1\\)次反面，那么此时硬币正面朝上的真实概率的可能性分布。 即，Beta 分布是一个作为伯努利分布和二项式分布的共轭先验分布的密度函数。 5.Multivariate continuous distributions 5.1 Multivariate normal (Gaussian) 多元高斯函数是最重要最经典的多元分布了，下面会专门详细学习。 5.2 Multivariate Student distribution 多元t分布的形状与多元高斯比较类似，主要是峰值更低，尾部缩减更慢。 当\\(v\\)趋近于无穷时，其逐渐逼近多元高斯分布，其均值和协方差矩阵如下： 5.3 Circular normal (von Mises Fisher) distribution 现实中，有些数据仅仅分布于一个单位球上，而不是欧式空间的任何一点都有概率。此时，冯·米塞斯分布就是针对这种情况。 冯·米塞斯分布就是高斯分布在单位球上的拓展。 5.4 Matrix normal distribution (MN) Matrix normal distribution是作用于矩阵的正态分布，其定义如下： 它可以转化为作用于向量上的多元高斯分布，只要将矩阵正态分布进行向量化处理便可以得到多元正态分布形式，如上所示。 这两者完全等价 (证明过程可以参考https://en.wikipedia.org/wiki/Matrix_normal_distribution)，公式中的符号\\(\\otimes\\)表示 Kronecker积，\\(V\\otimes U\\)表示多元正态分布的协方差矩阵；符号\\(\\text{vec}\\left(\\cdot\\right)\\)表示将给定矩阵按列组织成一个向量。 这两者完全等价，但在实践中，考虑到协方差矩阵\\(V\\otimes U\\in\\mathbb{R}^{(mn)\\times (mn)}\\)，假设我们想生成一个大小为\\(100\\times 200\\)的随机矩阵\\(X\\)，并要求矩阵\\(X\\)在概率上服从矩阵正态分布。此时，若利用多元正态分布进行生成，则需要协方差矩阵\\(V\\otimes U\\)的大小为\\((100\\times 200)\\times (100\\times 200)=20000\\times 20000\\)，元素数量为\\(4\\times 10^8\\)，显然，这个数字很惊人，毕竟存储这么大的矩阵就需要消耗计算机比较多的内存了，所以矩阵正态分布有它的优势。 5.5 Wishart distribution Wishart分布是伽马分布的多元形式，也是十分重要的分布。卡方分布也是它的特例。 多元高斯分布和Wishart分布有很紧密的联系，设\\(Y_{1}\\ldots Y_{n}\\ iid\\sim\\ N(0,\\Sigma)\\)，其中\\(Y_{i}(i = 1,\\ldots,n)\\)是\\(p\\)维列向量，则随机矩阵\\(W = \\sum_{i = 1}^{n}{Y_{i}Y_{i}^{T}}\\)的分布就是wishart分布，记作\\(W\\sim Wishart(\\Sigma,n)\\)，可以发现，当协方差矩阵退化为单位1，得到的就是卡方分布。 5.6 Inverse Wishart distribution 与倒伽马分布和伽马分布的关系类似，服从Wishart分布的随机变量的倒数就服从倒Wishart分布。 5.7 Dirichlet distribution 狄利克雷分布是Beta分布的多元形式，自然的其也是多项分布的共轭先验分布。 共轭先验在Beta分布里面已经提到过，目前笔者也只是稍微了解了一点，后面笔者也打算专门去深入了解一下。 狄利克雷分布可以用来定义“不确定性”的问题。考虑一个3面骰子。如果我们知道每个结果都是等可能的，我们可以使用“尖峰”对称狄利克雷，如Dir(20,20,20)，即我们确信结果将是不可预测的。相比之下，如果我们不确定结果会是什么样子(例如，它可能是一个有偏的骰子)，那么我们可以使用“平坦”对称狄利克雷，例如Dir(1,1,1)，它可以生成广泛的可能的结果分布。 三、高斯联合分布 实践中最广泛使用的连续随机变量联合概率分布是多元高斯分布了，也叫多元正态分布(MVN)。这部分是因为其在数学上很方便，而且高斯分布假设在许多情况下是相当合理的。 1. The multivariate normal 定义 多元高斯分布的定义应该都很熟悉了： 可以对协方差矩阵进行限制： Gaussian shells 随着维度\\(D\\)的增加，样本\\(x∼N(0,I_D)\\)中大部分点并不位于原点附近，而是集中在距离原点\\(r = \\sqrt{D}\\)​ 处的一个薄壳或环形区域。这是因为虽然概率密度随着\\(\\frac{r^2}{2}\\)指数衰减（距离增大概率密度减小），但球体的体积 随距离增加而增加，导致大多数点集中在距离原点\\(\\sqrt{D}\\)处的一个薄环上。此现象称为“高斯肥皂泡”。 计算点\\(x\\)到原点的平方距离\\(d(x) = \\sum_{i=1}^{D} x_i^2\\)​，其中\\(x_i \\sim N(0, 1)\\)。 期望值：\\(\\mathbb{E}[d^2] = D\\)。 方差：\\(\\text{Var}(d^2) = D\\)。 所以随着D的增大，the coefficient of variation（标准差与期望的比值）会趋近于0 Marginals and conditionals of an MVN 对于一个满足多元高斯分布的向量\\(x\\)进行分块为\\(x1，x2\\)，会发现其边缘分布均为高斯分布，条件分布也均为高斯分布。 并且\\(p(x_1|x_2)\\)的后验均值是\\(x_2\\)的线性函数，但协方差于\\(x_2\\)无关，这是高斯分布的一个特殊性质。具体如下： 其它表达形式 高斯分布有其它表达形式，这些形式有对应的优势，例如边缘化公式在矩形式下更简单，而条件化公式在信息形式下更简单 2. Linear Gaussian systems 线性高斯系统定义如下，一个变量\\(z\\)和条件分布\\(p(y|z)\\)均为高斯分布： 此时联合分布\\(p(z,y)\\)的形式如下： 而后验分布\\(p(z|y)\\)也是一个高斯分布： 四、The exponential family 指数族包括了众多上面提到的常见分布，比如高斯分布、二项分布、多项式分布、 泊松分布、gamma分布、beta分布等等。 其在机器学习里面起着至关重要的作用，主要因为其独特的优点： 1. 定义 指数族分布（Exponential Family Distribution）： 指数族分布是一类可以写成如下形式的分布： \\[p(x|\\eta) = h(x) \\exp \\left( \\eta^T T(x) - A(\\eta) \\right)\\] 其中，\\(\\eta\\)是自然参数，\\(T(x)\\)是充分统计量，\\(A(\\eta)\\)是归一化常数，确保概率分布的积分为1。具体定义如下： 在数族分布中，如果自然参数\\(\\eta\\)之间相互独立，则可以更方便地进行推导和计算。所谓的独立性意味着没有非零的\\(\\eta\\)满足\\(\\eta^T T(x) = 0\\)，即自然参数\\(\\eta\\)不能通过其他参数线性组合来为零。此时我们称一个指数族分布为最小，因为这意味着我们不能通过减少自然参数的数量来进一步简化分布的参数化，否则分布会变得冗余。 在多项式分布中，由于参数有一个和为1的约束条件，导致自然参数之间并不完全独立。因此，严格来说，多项式分布并不是最小指数族。但尽管多项式分布中自然参数有依赖性，但可以通过重新参数化，将\\(K\\)个参数中的一个去掉，使用\\(K-1\\)个独立的参数来表示整个分布。这样可以将原来的问题转换为最小指数族的形式，使得参数之间更加独立。 2. 例子 2.1 伯努利分布： \\[\\begin{align*} P(x|\\mu) &amp;=\\mu^x(1-\\mu)^{1-x} \\\\&amp;=exp (ln(\\mu^x(1-\\mu)^{1-x})) \\\\&amp;=exp(xln(\\frac{\\mu}{1-\\mu})+ln(1-\\mu)) \\end{align*}\\] 对比可知有如下关系: [规范参数]\\(\\eta = \\phi(\\mu)=ln(\\frac{\\mu}{1-\\mu})\\) [充分统计量]\\(T(x)=x\\) [累积函数]\\(A(\\eta)=-ln(1-\\mu)\\) [基础度量值]\\(h(x)=1\\) *\\(\\lambda = logistic(\\eta)=\\frac{1}{1+e^{-\\eta}}\\) 上面也提到了多项式分布怎么减少参数量（让参数变为互相独立的）。 2.2 Categorical distribution 与伯努利分布类似，由于参数求和为1，所以独立变量只有\\(K-1\\)个： 2.3 单变量高斯分布 高斯分布可做如下变换： \\[\\begin{align*} P(x|\\mu,\\sigma^2)&amp;=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{1}{2\\sigma^2}(x-\\mu)^2) \\\\&amp;=\\frac{1}{\\sqrt{2\\pi}}exp(\\frac{\\mu}{\\sigma^2}x-\\frac{1}{2\\sigma^2}x^2-\\frac{1}{2\\sigma^2}\\mu^2-ln\\sigma) \\end{align*}\\] 同样对比可知: [规范参数]\\(\\eta = \\phi(\\lambda)=[\\frac{\\mu}{\\sigma^2},-\\frac{1}{2\\sigma^2}]\\) [充分统计量]\\(T(x)=[x,x^2]\\) [累积函数]\\(A(\\eta)=\\frac{1}{2\\sigma^2}\\mu^2+ln\\sigma\\) [基础度量值]\\(h(x)=\\frac{1}{\\sqrt{2\\pi}}\\) 因为高斯模型有两个参数,所以两个向量长度都为2 如果限制\\(\\sigma^{2} = 1\\)，则有如下形式： 此时\\(h(x)\\)不再是常数。 2.4 多元高斯分布 与单变量高斯分布推导类似，但比较复杂，如下所示： 2.5 不是指数族的例子 分布族为指数族的必要条件为它有共同支撑集，也即\\(S_\\theta = \\{x: p(x) &gt; 0\\}\\)与\\(\\theta\\)无关。 比如说均匀分布\\(R(0, \\theta)\\)就没有共同支撑集（因为它非零的区域为\\([0,\\theta]\\)），所以它不可能是指数族分布。 3. 重要性质 Log partition function 对数配分函数\\(A(\\eta)\\)有如下性质： 直接从定义证明即可 自然参数和矩参数转换 对数分区函数\\(A(\\eta)\\)的梯度等于充分统计量的期望，也就是矩参数（或均值参数）。即：\\(m = E[T(x)] = \\nabla_\\eta A(\\eta)\\)这表明我们可以通过计算\\(A(\\eta)\\)的梯度，从自然参数\\(\\eta\\)得到对应的矩参数\\(m\\)。 如果指数族是最小的，则可以从矩参数\\(m\\)转换回自然参数\\(\\eta\\)。这一过程通过对函数\\(A(\\eta)\\)的凸共轭函数（convex conjugate）\\(A^*(m)\\)实现，公式为：\\(\\eta = \\nabla_m A^*(m)\\) 其中，凸共轭函数\\(A^*(m)\\)定义为：\\(A^*(m) = \\sup_{\\eta \\in \\Omega} \\left( m^T \\eta - A(\\eta) \\right)\\)这意味着通过\\(A^*\\)的梯度可以从矩参数\\(m\\)转回自然参数\\(\\eta\\)。 4. 指数族的极大似然估计 指数族模型的似然函数形式：对于指数族分布模型，其似然函数可以写成以下形式：\\(p(D|\\eta) = \\prod_{n=1}^N h(x_n) \\exp \\left( \\eta^T \\sum_{n=1}^N T(x_n) - N A(\\eta) \\right)\\) 上式可以化简为： \\[p(D|\\eta) \\propto \\exp \\left( \\eta^T T(D) - N A(\\eta) \\right)\\] 这里\\(T(D)\\)是数据集的充分统计量之和： \\[T(D) = \\left[ \\sum_{n=1}^N T_1(x_n), \\ldots, \\sum_{n=1}^N T_K(x_n) \\right]\\] 不同的分布对应不同的充分统计量，例如： 对于Bernoulli分布，充分统计量\\(T(D)\\)为：\\(T(D) = \\left[ \\sum_n I(x_n = 1) \\right]\\) 对于一维高斯分布，充分统计量\\(T(D)\\)为：\\(T(D) = \\left[ \\sum_n x_n, \\sum_n x_n^2 \\right]\\) Pitman-Koopman-Darmois定理说明在某些正则条件下，指数族分布是唯一具有有限充分统计量的分布族。也就是说，在指数族分布中，充分统计量的个数不依赖于数据集的大小。 给定数据集\\(D\\)，指数族分布的对数似然函数为： \\[\\log p(D|\\eta) = \\eta^T T(D) - N A(\\eta) + \\text{const}\\] 由于\\(-A(\\eta)\\)是自然参数\\(\\eta\\)的凸函数，而\\(\\eta^T T(D)\\)是线性函数，因此可以得出：对数似然函数是凸的，从而存在唯一的全局最大值。 我们对对数似然函数求导，导数如下： \\[\\nabla_\\eta \\log p(D|\\eta) = T(D) - N E[T(x)]\\] 对于单个数据点\\(x\\)，导数为： \\[\\nabla_\\eta \\log p(x|\\eta) = T(x) - E[T(x)]\\] 至于\\(E[T(x)]\\)，我们用数据集进行估计即可： \\[E[T(x)] = \\frac{1}{N} \\sum_{n=1}^N T(x_n)\\] 五、随机变量之间的变换 1. 双射 双射的变换公式很熟悉了，主要就是涉及到雅可比矩阵行列式： 2. 蒙特卡罗近似 也很熟悉了，就是采样估计： 3. Probability integral transform 这个其实就是从均匀分布采样，然后通过逆映射进行计算，这样就相当于从原分布中进行采样了。也比较熟悉了： 六、 马尔可夫链 马尔可夫链涉及的知识比较多，书上讲的也都是比较基础的，本科课程也学习过。主要记录一下之前没见过的： 马尔可夫链的最大似然估计： MAP estimation：解决数据稀疏的问题，引入了Dirichlet先验： 七、比较两个分布的相似度 1. f-散度 f散度是一个函数，这个函数用来衡量两个概率密度p和q的区别，也就是衡量这两个分布多么的相同或者不同。像\\(KL\\)散度和\\(JS\\)散度都是它的一种特例 f散度定义如下： \\[{D_f}(\\mathcal P_1\\|\\mathcal P_2)=\\int f (\\frac{p_2(x)}{p_1(x)})\\cdot p_1(x)\\mathrm d x=\\mathbb E_{x\\sim\\mathcal P_1}\\left[f(\\frac{p_2(x)}{p_1(x)})\\right] \\\\\\] \\(f()\\)就是不同的散度函数，\\(D_f\\)就是在f散度函数下，两个分布的差异。规定 \\(f\\)是凸函数(为了用琴生不等式) \\(f ( 1 ) = 0\\)(如果两个分布一样，刚好公式=0) 下面给出一些常见的f-散度例子： KL 散度 当\\(f( r ) = rlog( r )\\)时，f-散度变为 KL 散度，公式为： \\[D_{KL}(p || q) = \\int p(x) \\log \\frac{p(x)}{q(x)} dx\\] α-散度 (Alpha Divergence) 当\\(f(x) = \\frac{4}{1 - \\alpha^2} (1 - x^{\\frac{1+\\alpha}{2}})\\)时，f-散度变为 α-散度，公式为： \\[D^\\alpha_A (p || q) = \\frac{4}{1 - \\alpha^2} \\left( 1 - \\int p(x)^{\\frac{1+\\alpha}{2}} q(x)^{\\frac{1-\\alpha}{2}} dx \\right)\\] 其中，\\(\\alpha \\neq \\pm 1\\)。另一种常用的参数化方式（Minka 方式）为： \\[DD^\\alpha_M(p || q) = \\frac{1}{\\alpha(1-\\alpha)} \\left( 1 - \\int p(x)^\\alpha q(x)^{1-\\alpha} dx \\right)\\] 当\\(\\alpha \\to 0\\)时，α-散度趋向于\\(D_{KL}(q||p)\\)。 当\\(\\alpha \\to 1\\)时，α-散度趋向于\\(D_{KL}(p||q)\\)。 当\\(\\alpha = 0.5\\)时，α-散度等于 Hellinger 距离（见下）。 Hellinger 距离 (Hellinger Distance) 平方的 Hellinger 距离定义为： \\[D_H^2(p || q) = \\frac{1}{2} \\int \\left( \\sqrt{p(x)} - \\sqrt{q(x)} \\right)^2 dx\\] 这相当于 f-散度，其中\\(f( r ) = (\\sqrt{r} - 1)^2\\)。 卡方距离 (Chi-Squared Distance) 卡方距离定义为： \\[\\chi^2(p || q) = \\frac{1}{2} \\int \\frac{(q(x) - p(x))^2}{q(x)} dx\\] 这对应于 f-散度，其中\\(f( r ) = ( r - 1 )^2\\)。 2. 积分概率度量 (Integral Probability Metrics, IPM) IPM 也用于计算两个分布\\(P\\)和\\(Q\\)之间的差异，其定义为： \\[ D_F(P, Q) = \\sup_{f \\in F} \\left| \\mathbb{E}_{p(x)}[f(x)] - \\mathbb{E}_{q(x&#39;)}[f(x&#39;)] \\right| \\] 其中，\\(F\\)是一类“光滑”的函数。常见的 IPM 度量包括： 最大均值差异 (Maximum Mean Discrepancy, MMD) 如果\\(F\\)是在正定核函数下的 RKHS（再生核希尔伯特空间），则对应的 IPM 被称为最大均值差异（MMD）。 Wasserstein 距离 (Wasserstein Distance) 如果\\(F\\)是满足 Lipschitz 条件的函数类\\(F = \\{ ||f||_L \\leq 1 \\}\\)，即 Lipschitz 常数有界（例如为1）的函数集合，则 IPM 变为 Wasserstein-1 距离： \\[ W_1(P, Q) = \\sup_{||f||_L \\leq 1} \\left| \\mathbb{E}_{p(x)}[f(x)] - \\mathbb{E}_{q(x)}[f(x&#39;)] \\right| \\]","categories":[{"name":"Probabilistic Machine Learning","slug":"Probabilistic-Machine-Learning","permalink":"https://jia040223.github.io/categories/Probabilistic-Machine-Learning/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jia040223.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"数学","slug":"数学","permalink":"https://jia040223.github.io/tags/%E6%95%B0%E5%AD%A6/"}]},{"title":"更新说明 2024.9.30","slug":"更新日志","date":"2024-09-29T16:34:30.000Z","updated":"2024-10-09T10:04:48.038Z","comments":true,"path":"2024/09/30/更新日志/","permalink":"https://jia040223.github.io/2024/09/30/%E6%9B%B4%E6%96%B0%E6%97%A5%E5%BF%97/","excerpt":"","text":"国庆假期到了，这学期课程压力比较小，所以也是能为以后的科研学习一下相关知识。但国庆假期还是给自己放了一个大长假，这段时间估计是不太会更新了。 Stanford CS236 最近Stanford CS236课程也算是看完了，后面可能还会有一些内容打算写一写吧。主要还是围绕diffusion，包括 ldm diffusion的condition控制 如何把diffusion用于离散的数据 前面的文章可能也会补一补。感觉diffusion涉及到的数学知识还是挺多的，后面有机会可以来补一补数学基础。 Probabilistic Machine Learning 打算学习一下Probabilistic Machine Learning这本书，后面应该也会边学边记录一下，也强烈给读者推荐这本书，特别对于像我这样致力于在AI领域进行研究但基础比较薄弱的同学。 这本书应该也是将来一段时间我的学习重点了，内容还是很多的。 数学 Stanford CS236课程还是涉及到挺多的数学知识，后面有机会可以来补一补数学基础。之前保研复习了一下微积分，线代，微分方程这些，后面可能会多看一看优化相关（比如什么拉格朗日对偶问题，每次遇到都是混过去了）的知识，同时对diffusion涉及的一些知识也多了解了解，可能包括： SDE和ODE的解法 傅里叶变换 优化理论 科研 国庆之后也可能会具体进行一些导师的项目，后面在科研上的学习有机会也可以记录一下。 碎碎念 感觉还是太菜了，什么都不会。感觉大学四年在课堂上学的东西真的太基础了。 以前本科的实习也就是看了几篇论文就开始做，然后也就用的别人的模型，在上面小修小补，以至于做了一学期的生成模型，现在看了Stanford CS236，感觉以前真的啥都不知道。 虽然可能跟着别人脚步走也能发论文吧，但还是希望能夯实一下理论基础，希望以后科研的日子能过得轻松一点。","categories":[{"name":"更新日志","slug":"更新日志","permalink":"https://jia040223.github.io/categories/%E6%9B%B4%E6%96%B0%E6%97%A5%E5%BF%97/"}],"tags":[{"name":"更新日志","slug":"更新日志","permalink":"https://jia040223.github.io/tags/%E6%9B%B4%E6%96%B0%E6%97%A5%E5%BF%97/"}]},{"title":"Diffusion Model原理","slug":"Diffusion Model原理","date":"2024-09-29T10:34:53.000Z","updated":"2024-10-27T10:51:25.364Z","comments":true,"path":"2024/09/29/Diffusion Model原理/","permalink":"https://jia040223.github.io/2024/09/29/Diffusion%20Model%E5%8E%9F%E7%90%86/","excerpt":"","text":"本学习笔记用于记录我学习Stanford CS236课程的学习笔记，分享记录，也便于自己实时查看。 引入 前面的课程中我们已经学习了许多生成模型的架构，例如VAEs，Score Based Models等。在课程的最后也是总算来到当前最火的生成模型架构：Diffusion Model。其实Diffusion Model与前面模型或多或少都有一定的联系，我们也可以从不同的视角来理解它。 笔者本科科研也算是学习研究了一些Diffusion相关的工作，但之前一直没有去梳理生成模型的发展，也没有深究其背后的数学原理。所以借此几乎，正好对一些知识进行整理，并对生成模型进行部分回顾。首先从DDPM和DDIM入手吧，这两篇文章也是之前科研实践学习过很多次了。 DDPM 首先我们知道，DDPM 是个马尔科夫模型（如下图），DDPM包括两个步骤。这两个步骤在原文中定义为前向加噪（forward，下图从右到左）和后向去噪（reverse，下图从左到右）。 从\\(x_0\\)到\\(x_T\\)的过程就是前向加噪过程，我们可以看到加噪过程就是对原始图片\\(x_0\\)不断添加噪声，使其最后信噪比趋近于0，此时得到的图片也就变成噪声了，而与之相对应的去噪过程就是还原过程，即从噪声不断去噪还原为图片。 我们通过往图片中加入噪声，使得图片变得模糊起来，当加的步骤足够多的时候（也就是T的取值越大的时候，一般取1000），图片已经非常接近一张纯噪声。纯噪声也就意味着多样性，我们的模型在去噪（还原）的过程中能够产生更加多样的图片。 这里的操作实际上就是指在图片加入噪声\\(noise\\)，噪声\\(noise\\)本身的分布可以是很多样的（btw，保研还被问过这个问题），而论文中采用的是标准正态分布，其理由是考虑到其优良的性质，在接下来的公式推理中见到。 推导 从上面的图可知，DDPM 将前向过程和逆向过程都设计为了马尔可夫链的形式： 称从\\(x_0\\)到\\(x_T\\)的马尔可夫链为前向过程 (forward process) 或扩散过程 (diffusion process)； 称从\\(x_T\\)到\\(x_0\\)的马尔可夫链为逆向过程 (reverse process) 或去噪过程 (denoising process). 所以我们的损失函数通过极大似然估计来进行。但这里我们又会遇到和VAE一样的问题，\\(log(P(x))\\)中的\\(P(x)\\)需要对\\(x_{1:T}\\)进行积分，此时我们便可以效仿VAE的做法，即把\\(x_{1:T}\\)作为类似VAE中的潜变量，去优化对数似然的下界ELBO（为什么是下界可以参考我都VAEs的文章，简单来说就是用琴生不等式即可）： \\[ \\begin{align*} ELBO &amp;= \\mathbb{E}_{\\mathbf{x}_{1:T} \\sim q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_0)} \\left[\\log \\frac{p_\\theta(\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_0)} \\right] \\\\&amp;= \\mathbb{E}_{\\mathbf{x}_{1:T} \\sim q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_0)} \\left[ \\log \\frac{p(\\mathbf{x}_T) \\prod_{t=1}^T p_\\theta(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t)}{\\prod_{t=1}^T q(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1})} \\right] \\end{align*} \\] 至于这里为啥要在给定\\(x_0\\)下计算，一方面是单纯的\\(q(\\mathbf{x}_{1:T})\\)我们没办法计算得出，而\\(q(\\mathbf{x}_{1:T}|\\mathbf{x}_0)\\)我们能求出其闭式解，另一方面在训练时我们的确已经\\(x_0\\)的信息。 OK，我们继续进行推导 \\[ \\begin{align} &amp;\\ \\ \\ \\ \\ \\text{ELBO}(\\mathbf x_0) \\\\ &amp;=\\mathbb E_{q(\\mathbf x_{1:T}\\vert\\mathbf x_0)}\\left[\\log\\frac{p(\\mathbf x_T)\\prod_{t=1}^{T}p(\\mathbf x_{t-1}\\vert\\mathbf x_t)}{\\prod_{t=1}^{T}q(\\mathbf x_t\\vert\\mathbf x_{t-1})}\\right]\\\\ &amp;=\\mathbb E_{q(\\mathbf x_{1:T}\\vert\\mathbf x_0)}\\left[\\log\\frac{p(\\mathbf x_T)\\prod_{t=1}^{T}p(\\mathbf x_{t-1}\\vert\\mathbf x_t)}{q(\\mathbf x_1\\vert\\mathbf x_0)\\prod_{t=2}^{T}q(\\mathbf x_t\\vert\\mathbf x_{t-1},\\mathbf x_0)}\\right]\\\\ &amp;=\\mathbb E_{q(\\mathbf x_{1:T}\\vert\\mathbf x_0)}\\left[\\log\\frac{p(\\mathbf x_T)\\prod_{t=1}^{T}p(\\mathbf x_{t-1}\\vert\\mathbf x_t)}{q(\\mathbf x_1\\vert\\mathbf x_0)\\prod_{t=2}^{T}\\frac{q(\\mathbf x_t\\vert\\mathbf x_0)q(\\mathbf x_{t-1}\\vert\\mathbf x_t,\\mathbf x_0)}{q(\\mathbf x_{t-1}\\vert\\mathbf x_0)} }\\right]\\\\ &amp;=\\mathbb E_{q(\\mathbf x_{1:T}\\vert\\mathbf x_0)}\\left[\\log\\frac{p(\\mathbf x_T)\\prod_{t=1}^{T}p(\\mathbf x_{t-1}\\vert\\mathbf x_t)}{q(\\mathbf x_T\\vert\\mathbf x_0)\\prod_{t=2}^{T}q(\\mathbf x_{t-1}\\vert\\mathbf x_t,\\mathbf x_0)}\\right]\\\\ &amp;=\\mathbb E_{q(\\mathbf x_{1:T}\\vert\\mathbf x_0)}\\left[\\log p(\\mathbf x_0\\vert\\mathbf x_1)\\right]+\\mathbb E_{q(\\mathbf x_{1:T}\\vert\\mathbf x_0)}\\left[\\log\\frac{p(\\mathbf x_T)}{q(\\mathbf x_T\\vert\\mathbf x_0)}\\right]+\\sum_{t=2}^T\\mathbb E_{q(\\mathbf x_{1:T}\\vert\\mathbf x_0)}\\left[\\log\\frac{p(\\mathbf x_{t-1}\\vert\\mathbf x_t)}{q(\\mathbf x_{t-1}\\vert\\mathbf x_t,\\mathbf x_0)}\\right]\\\\ &amp;=\\mathbb E_{q(\\mathbf x_{1}\\vert\\mathbf x_0)}\\left[\\log p(\\mathbf x_0\\vert\\mathbf x_1)\\right]+\\mathbb E_{q(\\mathbf x_{T}\\vert\\mathbf x_0)}\\left[\\log\\frac{p(\\mathbf x_T)}{q(\\mathbf x_T\\vert\\mathbf x_0)}\\right]+\\sum_{t=2}^T\\mathbb E_{q(\\mathbf x_t\\vert\\mathbf x_0)}\\mathbb E_{q(\\mathbf x_{t-1}\\vert\\mathbf x_t,\\mathbf x_0)}\\left[\\log\\frac{p(\\mathbf x_{t-1}\\vert\\mathbf x_t)}{q(\\mathbf x_{t-1}\\vert\\mathbf x_t,\\mathbf x_0)}\\right]\\\\ &amp;=\\underbrace{\\mathbb E_{q(\\mathbf x_{1}\\vert\\mathbf x_0)}\\left[\\log p(\\mathbf x_0\\vert\\mathbf x_1)\\right]}_\\text{reconstruction term}-\\underbrace{\\text{KL}(q(\\mathbf x_T\\vert\\mathbf x_0)\\Vert p(\\mathbf x_T))}_\\text{regularization term}-\\sum_{t=2}^T\\mathbb E_{q(\\mathbf x_t\\vert\\mathbf x_0)}\\underbrace{\\left[\\text{KL}(q(\\mathbf x_{t-1}\\vert\\mathbf x_t,\\mathbf x_0)\\Vert p(\\mathbf x_{t-1}\\vert\\mathbf x_t))\\right]}_\\text{denoising matching terms} \\end{align} \\] 同样出现了重构项、正则项和匹配项。重构项要求\\(x_1\\)能够重构\\(x_0\\)，正则项要求\\(x_T\\)的后验分布逼近先验分布，而匹配项则建立起相邻两项\\(x_{t−1},x_t\\)之间的联系。 现在，我们只需要为式中出现的所有概率分布设计具体的形式，就可以代入计算了。为了让 KL 散度可解，一个自然的想法就是把它们都设计为正态分布的形式。 前向过程 在DDPM的前向过程中，对于\\(t \\in [1,T]\\)时刻，\\(x_t\\)和\\(x_{t-1}\\)满足如下关系： \\[x_t = \\sqrt{1-\\beta_t}x_{t-1} + \\sqrt{\\beta_t }\\epsilon, \\ \\ \\ \\epsilon\\sim N(0,1)\\] 其中\\(β_t∈(0,1)\\)是事先指定的超参数，代表从\\(x_{t−1}\\)到\\(x_t\\)这一步的方差。 这里的系数设定为开根号的\\(\\beta\\)，是为了保证马尔科夫链的最后收敛为标准高斯分布。 \\(\\sqrt\\beta\\)和\\(\\sqrt{1-\\beta}\\)是怎么来的： 我们这里先不管\\(\\beta\\)，把两个系数分别设为\\(a\\)和\\(b\\)。 公式变为： \\[x_t = ax_{t-1} + b\\epsilon\\] 我们希望，当\\(t\\)趋于无穷的时候，\\(x_t \\sim N(0,1), x_{t-1} \\sim N(0,1)\\) 我们知道当两个高斯分布相加时， \\[X\\sim N(\\mu_X,\\sigma_X^2),Y\\sim N(\\mu_Y,\\sigma_Y^2)\\] \\[Z=aX+bY\\] 则 \\[Z \\sim N(a\\mu_X+b\\mu_Y, a^2\\sigma^2+b^2\\sigma^2)\\] 所以此时 \\[x_t~\\sim N(a\\mu_{t-1}+b\\mu_\\epsilon,a^2\\sigma_{t-1}^2+b^2\\sigma_\\epsilon^2)\\] \\[x_t\\sim N(a·0+b·0, a^2·1+b^2·1)\\] \\[x_t \\sim N(0,a^2+b^2)\\] 我们想让\\(x_{t-1}\\)和\\(\\epsilon\\)得到的\\(x_{t}\\)也服从标准正态分布，即\\(x_{t} \\sim N(0,1)\\)，那么我们就只能让\\(a^2+b^2=1\\)。 再令\\(\\beta=a^2\\)，则\\(a=\\sqrt{\\beta},b=\\sqrt{1-\\beta}\\)。 或者也可以令\\(\\alpha=b^2\\)，则\\(a=\\sqrt{\\alpha}x_{t-1}+\\sqrt{1-\\alpha}\\epsilon\\)。 说白了，这俩系数就是为了让两个服从标准正态分布的噪声相加得到的东西还是服从正态分布。 OK，在这基础上我们可以继续推导，让\\(x_t\\)用\\(x_0\\)来表示： 令\\(\\alpha_t=1-\\beta_t\\)，则公式变为： \\[x_t=\\sqrt{\\alpha_t}x_{t-1}+\\sqrt{1-\\alpha_t}\\epsilon\\] 继续推导： \\[\\begin{align*} x_t &amp;=\\sqrt{\\alpha_t}x_{t-1}+\\sqrt{1-\\alpha_t}\\epsilon\\\\ &amp;=\\sqrt{\\alpha_t}(\\sqrt{\\alpha_{t-1} }x_{t-2}+\\sqrt{1-\\alpha_{t-1} }\\epsilon)+\\sqrt{1-\\alpha_t}\\epsilon\\\\ &amp;=\\sqrt{\\alpha_t\\alpha_{t-1} }x_{t-2}+\\sqrt{\\alpha_t(1-\\alpha_{t-1})}\\epsilon + \\sqrt{1-\\alpha_t}\\epsilon\\\\ \\end{align*}\\] 上式最后一行第二项和第三项，可以看做两个正态分布相加。 由于两个正态分布\\(X\\sim N(\\mu_x,\\sigma_x^2), Y\\sim N(\\mu_y, \\sigma_y^2)\\)，相加后有 \\(aX+bY\\sim N(a\\mu_x+b\\mu_y,a^2\\sigma_x^2+b^2\\sigma_y^2)\\)。所以，合并两个正态分布，得到： \\[x_t=\\sqrt{\\alpha_t\\alpha_{t-1} }x_{t-2}+\\sqrt{1-\\alpha_t\\alpha_{t-1} }\\epsilon\\] 由数学归纳法，可以推导出： \\[x_t=\\sqrt{\\alpha_t\\alpha_{t-1}...\\alpha_1}x_0+\\sqrt{1-\\alpha_t\\alpha_{t-1}...\\alpha_1}\\epsilon\\] 再令\\(\\bar\\alpha_t=\\alpha_t\\alpha_{t-1}...\\alpha_1\\)，则公式可以进一步化简为： \\(x_t=\\sqrt {\\bar\\alpha_t}x_0+\\sqrt{1-\\bar\\alpha_{t} }\\epsilon\\)，由于 \\[\\lim_{t\\to\\infty}\\sqrt{\\bar\\alpha_t}=0,\\quad\\lim_{t\\to\\infty}\\sqrt{1-\\bar\\alpha_t}=1\\] 所以我们能够保证马尔科夫链最后能够收敛于标准正态分布 逆向过程 这里从我们熟知的贝叶斯公式出发： \\[P(A|B)=\\frac{P(B|A)P(A)}{P(B)}\\] 可知 \\[P(x_{t-1}|x_t)=\\frac{P(x_t|x_{t-1})P(x_{t-1})}{P(x_t)}\\] 这里我们的\\(P(x_{t-1})\\)和\\(P(x_t)\\)我们都不知道，但在已知\\(x_0\\)的情况下有： \\[P(x_{t-1}|x_t,x_0)=\\frac{P(x_t|x_{t-1},x_0)P(x_{t-1}|x_0)}{P(x_t|x_0)}\\] 把\\(x_0=\\sqrt{\\bar{\\alpha_t} }x_0\\)和\\(x_t=\\sqrt {\\bar\\alpha_t}x_0+\\sqrt{1-\\bar\\alpha_{t} }\\epsilon\\)带入上式，可得： \\[P(x_{t-1}|x_t,x_0)=\\frac{ N(\\sqrt{\\alpha_t}x_0,1-\\bar\\alpha_t) N(\\sqrt{\\bar\\alpha_{t-1} }x_0,1-\\bar\\alpha_{t-1}) }{ N(\\sqrt{\\bar\\alpha_{t} }x_0,1-\\bar\\alpha_{t}) }\\] 已知高斯分布的概率密度函数为： \\[f(x)=\\frac{1}{\\sqrt{2\\pi\\sigma} }exp(-\\frac{(x-\\mu)^2}{2\\sigma^2})\\] 所以 \\[P(x_{t-1}|x_t,x_0) \\propto exp-\\frac{1}{2} [ \\frac{(x_t-\\sqrt{\\alpha_t}x_{t-1})^2}{1-\\alpha_t} +\\frac{(x_{t-1}-\\sqrt{\\bar\\alpha_{t-1} }x_0)^2}{1-\\bar\\alpha_{t-1} } -\\frac{(x_{t}-\\sqrt{\\bar\\alpha_{t} }x_0)^2}{1-\\bar\\alpha_{t} } ]\\] 此时由于\\(x_{t-1}\\)是我们关注的变量，所以整理成关于\\(x_{t-1}\\)的形式： \\[P(x_{t-1}|x_t,x_0) \\propto exp-\\frac{1}{2} [ (\\frac{\\alpha_t}{1-\\alpha_t}+\\frac{1}{1-\\bar\\alpha_{t-1} })x_{t-1}^2 -(\\frac{-2\\sqrt{\\alpha_t}x_t}{1-\\alpha_t} + \\frac{-2\\sqrt{\\bar\\alpha_{t-1} }x_0}{1-\\bar\\alpha_{t-1} })x_{t-1} +C(x_t,x_0) ]\\] 其中第三项\\(C(x_t,x_0)\\)与\\(x_{t-1}\\)无关，作为指数上相加的部分，可以拿到最前面只影响最前面的系数。 所以此时： \\[P(x_{t-1}|x_t,x_0) \\propto exp-\\frac{1}{2} [ (\\frac{\\alpha_t}{1-\\alpha_t}+\\frac{1}{1-\\bar\\alpha_{t-1} })x_{t-1}^2 -(\\frac{-2\\sqrt{\\alpha_t}x_t}{1-\\alpha_t} + \\frac{-2\\sqrt{\\bar\\alpha_{t-1} }x_0}{1-\\bar\\alpha_{t-1} })x_{t-1}]\\] 又因为标准正态分布满足\\(\\propto exp - \\frac{x^2-2\\mu x + \\mu^2}{2\\sigma^2}\\)，所以我们可以得到\\(P(x_{t-1}|x_t,x_0)\\)对应的方差 \\[\\frac{1}{\\sigma^2}=\\frac{\\alpha_t}{1-\\alpha_t}+\\frac{1}{1-\\bar\\alpha_{t-1} } =\\frac{1-\\alpha_t\\bar\\alpha_{t-1} }{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})} =\\frac{1-\\bar\\alpha_{t} }{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}\\] 这里\\(\\alpha_t\\bar\\alpha_{t-1}=\\bar\\alpha_t\\)。所以： \\[\\sigma^2=\\frac{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}\\] 再看\\(x_{t-1}\\)的一次项，得到： \\[\\frac{2\\mu}{\\sigma^2}= (\\frac{-2\\sqrt{\\alpha_t}x_t}{1-\\alpha_t} + \\frac{-2\\sqrt{\\bar\\alpha_{t-1} }x_0}{1-\\bar\\alpha_{t-1} })\\] 把\\(\\sigma^2\\)和\\(x_0\\)带入上式，化简得到： \\[\\mu=\\frac{1}{\\sqrt{\\alpha_t} }(x_t-\\frac{1-\\alpha_t}{\\sqrt{1-\\bar\\alpha_t} }\\epsilon)\\] 所以说： \\[P(x_{t-1}|x_t, x_0)\\sim N(\\frac{1}{\\sqrt{\\alpha_t} }(x_t-\\frac{1-\\alpha_t}{\\sqrt{1-\\bar\\alpha_t} }\\epsilon), \\frac{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t})\\] 回顾一下我们写的这一大段公式，也就是说，我们已知了先验概率，推导出了后验概率的表达式，得到了在给定\\(x_0\\)后的\\(x_{t-1}\\)的分布的均值和方差。也就是说，上面公式中，我们的 \\[q(x_{t-1}\\vert x_t,x_0)\\sim N(\\frac{1}{\\sqrt{\\alpha_t} }(x_t-\\frac{1-\\alpha_t}{\\sqrt{1-\\bar\\alpha_t} }\\epsilon), \\frac{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t})\\] 接下来，\\(\\epsilon\\)的具体值，我们让模型去拟合就好了。 损失函数 我们之前已经推导了ELBO的具体形式： \\[\\text{ELBO}= \\underbrace{E_{x_1\\sim q(x_1\\vert x_0)}[\\log p_\\theta(x_0\\vert x_1)]}_{ {L_0} }- \\underbrace{KL(q(x_T \\vert x_0)\\|p(x_T))}_{ {L_T} }- \\sum_{t=2}^T\\underbrace{E_{x_t\\sim q(x_t\\vert x_0)}\\left[KL(q(x_{t-1}\\vert x_t,x_0)\\|p_\\theta(x_{t-1}\\vert x_t))\\right]}_{ {L_{t-1} }}\\] 这里\\(q(x_{t-1}\\vert x_t,x_0)\\)我们已经得到了，\\(q(x_{t}|x_0)\\)也是我们定义的。只需要定义\\(p_\\theta(x_{t-1}|x_t)\\)即可，为了计算方便，我们也选择与\\(q(x_{t-1}\\vert x_t,x_0)\\)一样的形式。 \\[p_\\theta(\\textbf{x}_{t-1}|\\textbf{x}_t) = \\mathcal{N}(\\textbf{x}_{t-1}; \\mu_\\theta(\\textbf{x}_t, t), \\frac{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}I)\\] 其中\\(\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t) = \\frac{1}{\\sqrt{\\alpha_t} } \\Big( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t} } \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t) \\Big)\\)，而\\({\\epsilon}_\\theta(\\mathbf{x}_t, t)\\)就是我们模型的输出。此时，我们带入可以得到 \\[\\begin{align} \\mathbf{x}_{t-1} &amp;= \\mathcal{N}(\\mathbf{x}_{t-1}; \\frac{1}{\\sqrt{\\alpha_t} } ( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t} } {\\epsilon}_\\theta(\\mathbf{x}_t, t) ), \\frac{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}I) \\end{align}\\] 带入上面KL散度的公式，可以得到损失函数\\(L_t\\)便为： \\[\\begin{aligned} L_t &amp;= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon} } \\Big[\\frac{1}{2 \\| \\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t) \\|^2_2} \\| \\color{blue}{\\tilde{\\boldsymbol{\\mu} }_t(\\mathbf{x}_t, \\mathbf{x}_0)} - \\color{green}{\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t)} \\|^2 \\Big] \\\\ &amp;= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon} } \\Big[\\frac{1}{2 \\|\\boldsymbol{\\Sigma}_\\theta \\|^2_2} \\| \\color{blue}{\\frac{1}{\\sqrt{\\alpha_t} } \\Big( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t} } \\boldsymbol{\\epsilon}_t \\Big)} - \\color{green}{\\frac{1}{\\sqrt{\\alpha_t} } \\Big( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t} } \\boldsymbol{\\boldsymbol{\\epsilon} }_\\theta(\\mathbf{x}_t, t) \\Big)} \\|^2 \\Big] \\\\ &amp;= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon} } \\Big[\\frac{ (1 - \\alpha_t)^2 }{2 \\alpha_t (1 - \\bar{\\alpha}_t) \\| \\boldsymbol{\\Sigma}_\\theta \\|^2_2} \\|\\boldsymbol{\\epsilon}_t - \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)\\|^2 \\Big] \\\\ &amp;= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon} } \\Big[\\frac{ (1 - \\alpha_t)^2 }{2 \\alpha_t (1 - \\bar{\\alpha}_t) \\| \\boldsymbol{\\Sigma}_\\theta \\|^2_2} \\|\\boldsymbol{\\epsilon}_t - \\boldsymbol{\\epsilon}_\\theta(\\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon}_t, t)\\|^2 \\Big] \\end{aligned}\\] 发现可以使用不用权重的简单形式就可以训练得到好的结果，即 \\[\\begin{aligned} L_\\text{simple} &amp;= \\mathbb{E}_{t \\sim [1, T], \\mathbf{x}_0, \\boldsymbol{\\epsilon}_t} \\Big[\\|\\boldsymbol{\\epsilon}_t - \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)\\|^2 \\Big] \\\\ &amp;= \\mathbb{E}_{t \\sim [1, T], \\mathbf{x}_0, \\boldsymbol{\\epsilon}_t} \\Big[\\|\\boldsymbol{\\epsilon}_t - \\boldsymbol{\\epsilon}_\\theta(\\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon}_t, t)\\|^2 \\Big] \\end{aligned}\\] 这样，我们就获得了DDPM的最终目标函数： \\[L_\\text{simple}(\\theta)=\\mathbb E_{t,x_0,\\epsilon}\\left[\\Vert\\epsilon-\\epsilon_\\theta(x_t,t)\\Vert^2\\right]\\] 具体训练流程和采样流程如下： DDIM DDPM虽好，但它只能一步一步老老实实通过\\(x_{t}\\)预测\\(x_{t-1}\\)，不能跨步运算，如果\\(T =1000\\)，那么生成一整图像就需要用网络推理1000次，效率很低。于是为了结局这个问题，DDIM出现了，而且最巧妙的是它不需要重新训练模型。 DDIM始于一个假设，它假设了 \\[P(x_{prev}|x_t,x_0)\\sim N(kx_0+mx_t,\\sigma_2)\\] \\[x_{prev}=kx_0+mx_t+\\sigma\\epsilon,\\ \\ \\ \\ \\ \\epsilon\\sim N(0,1)\\] 又因为加噪过程满足公式\\(x_t=\\sqrt {\\bar\\alpha_t}x_0+\\sqrt{1-\\bar\\alpha_{t} }\\epsilon\\) 把\\(x_t\\)带入\\(x_{t-1}\\)合并同类项得到： \\[\\begin{align*} x_{prev}&amp;=kx_0+m(\\sqrt{\\bar\\alpha_t}x_0+\\sqrt{1-\\bar\\alpha_t}\\epsilon)+\\sigma\\epsilon\\\\ &amp;=(k+m\\sqrt{\\bar\\alpha_t})x_0+\\epsilon&#39; \\end{align*}\\] \\[\\epsilon&#39;\\sim N(0,m^2(1-\\bar\\alpha_t)+\\sigma^2)\\] 又因为\\(x_{prev}=\\sqrt {\\bar\\alpha_{prev} }x_0+\\sqrt{1-\\bar\\alpha_{prev} }\\epsilon\\)，满足对应系数相同，有： \\[k+m\\sqrt{\\bar\\alpha_t}=\\sqrt{\\bar{\\alpha_{prev} }}\\\\ m^2(1-\\bar\\alpha_t)+\\sigma^2=1-\\bar\\alpha_{prev}\\] 求得： \\[m=\\frac{\\sqrt{1-\\bar\\alpha_{prev}-\\sigma^2} }{\\sqrt{1-\\bar\\alpha_t} }\\\\ k=\\sqrt{\\bar\\alpha_{prev} }-\\frac{\\sqrt{1-\\bar\\alpha_{prev}-\\sigma^2} }{\\sqrt{1-\\bar\\alpha_t} }\\sqrt{\\bar\\alpha_t}\\] 带入公式最终化简得： \\[x_{prev}=\\sqrt{\\bar{\\alpha_{prev} }} (\\frac{x_t-\\sqrt{1-\\bar\\alpha_t}\\epsilon_t}{\\sqrt{\\bar\\alpha_t} }) +\\sqrt{1-\\bar\\alpha_{prev}-\\sigma^2}\\epsilon_t+\\sigma^2\\epsilon\\] 其中\\(t\\)和\\(prev\\)可以相隔多个迭代步数，一般相隔20可以做到采样速度和采样质量比较好地平衡。所以一般DDPM要做1000步，而DDIM是需要50步就可以完成采样。 当这里的\\(\\sigma\\)选取0的时候，也就意味着变成了一个确定性采样的过程。此时的DDIM就变成了一个Flow Models，事实上论文里也是这么做的。 从不同角度看扩散模型 前面我们DDPM的推导过程中，其实可以把扩散模型看成一个给定后验的多层VAE。即认为设定了\\(p(x_{1:T}|x_0)\\)的形式，然后让模型来从潜变量中采样，最终生成图片。 而DDIM把这个过程变成了一个确定性过程，也就是说把潜变量和数据之间做了一个双射，所以此时也就可以看成Flow Models的一个了 事实上，扩散模型的连续和离散其实对应着随机过程里的概念。一般来说，discrete time指的是随机过程中的时间\\(t\\)只能取离散整数值，而continous-time则指的是时间参数\\(t\\)可以取连续值。discrete time随机过程中的参数在一个离散的时间点只能改变一次；而continuous-time随机过程的参数则可以随时发生变化。 DDPM和SDE 我们在DDPM里的加噪过程。每一个time step，我们都会按照如下的离散马尔可夫链进行加噪： \\[x_i = \\sqrt{1 - \\beta_i}x_{i-1} + \\sqrt{\\beta_i} \\epsilon_{i-1}, i=1,..., N\\] 为了将上述过程连续化，我们需要引入连续时间随机过程。而连续时间其实就是让每个离散的时间间隔\\(\\Delta t\\)无限趋近于0，其实也等价于求出\\(N \\to \\infty\\)​时，上述马尔可夫链的极限 在求极限之前，我们需要先引入一组辅助的noise scale\\(\\{\\bar{\\beta}_i = N \\beta_i\\}_{i=1}^N\\)，并将上面的式子改写如下： \\[x_i = \\sqrt{1 - \\frac{\\bar{\\beta}_i}{N} }x_{i-1} + \\sqrt{\\frac{\\bar{\\beta}_i}{N} }\\epsilon_{i-1}, i = 1,..., N\\] 在\\(N \\to \\infty\\)​时，上面的\\(\\{\\bar{\\beta}_i\\}_{i=1}^{N}\\)就成了一个关于时间\\(t\\)的连续函数\\(\\beta(t)\\)​，并且\\(t \\in [0, 1]\\)。随后，我们可以假设\\(\\Delta t = \\frac{1}{N}\\)​，在每个\\(i\\Delta t\\)时刻，连续函数\\(\\beta(t), x(t), \\epsilon(t)\\)都等于之前的离散值，即： \\[\\beta(\\frac{i}{N}) = \\bar{\\beta}_i, x(\\frac{i}{N}) = x_i, \\epsilon(\\frac{i}{N})=\\epsilon_i\\] 在\\(t \\in \\{0, 1, ..., \\frac{N-1}{N}\\}\\)​以及\\(\\Delta t=\\frac{1}{N}\\)的情况下，我们就可以用连续函数改写之前的式子： \\[\\begin{align} x(t+ \\Delta t) &amp;= \\sqrt{1-\\beta(t+\\Delta t)\\Delta t}\\ x(t) + \\sqrt{\\beta(t+\\Delta t)\\Delta t}\\ \\epsilon(t) \\\\ &amp; \\approx x(t) - \\frac{1}{2}\\beta(t+\\Delta t) \\Delta t\\ x(t) + \\sqrt{\\beta(t+\\Delta t)\\Delta t}\\ \\epsilon(t) \\\\ &amp; \\approx x(t) - \\frac{1}{2}\\beta(t)\\Delta t\\ x(t) + \\sqrt{\\beta(t)\\Delta t}\\ \\epsilon(t) \\end{align}\\] 上面的近似只有在\\(\\Delta t \\ll 1\\)时成立。我们将其再移项后就可以得到下式： \\[x(t+\\Delta t) - x(t) \\approx -\\frac{1}{2} \\beta(t)\\Delta t\\ x(t) + \\sqrt{\\beta(t)\\Delta t}\\ \\epsilon(t) \\\\ \\mathrm{d} x = -\\frac{1}{2}\\beta(t)x \\mathrm{d}t + \\sqrt{\\beta(t)} \\mathrm{d}w\\] 其中，\\(w\\)​表示的就是Wiener Process。这里面的第二个式子，就是一SDE方程。 至此，我们证明了DDPM连续化之后，就可以得到一个SDE方程，并且它是一种Variance Preserving的SDE。Variance Preserving的含义是当\\(t \\to \\infty\\)时，它的方差依然有界。 与此反向过程也是一个SDE方程，称为reverse SDE： \\[\\text{d}\\mathbf{x}= [\\mathbf{f}(\\mathbf{x}, t) - g^2(t)\\nabla _{\\mathbf{x} }\\log p(\\mathbf{x})]\\text{d}\\mathbf{t} + g(t)\\text{d}\\mathbf{w}\\] 这个反向过程中的未知量就只有分数函数\\(\\nabla_x \\log p_{t}(x)\\)​。至此，DDPM和分数模型也产生了联系，实际上二者之间是相互等价的。而DDPM和分数模型本质上都是在学习这个reverse SDE的解。 我们可以看到，DDPM每一步的去噪其实本质上与Annealed Langevin dynamics是一模一样的。 DDIM与ODE 首先对于一个SDE， \\[\\text{d}\\mathbf{x}= \\mathbf{f}(\\mathbf{x}, t)\\text{d}\\mathbf{t} + g(t)\\text{d}\\mathbf{w}\\] 我们写出它的福克-普朗克方程（Fokker-Planck equation）： \\[\\begin{align*} \\nabla _{t}p(\\mathbf{x}, t) &amp;= -\\nabla _{\\mathbf{x} }[\\mathbf{f}(\\mathbf{x}, t)p(\\mathbf{x}, t)] + \\frac{1}{2}g^{2}(t)\\nabla _{\\mathbf{x} }^{2}p(\\mathbf{x}, t)\\\\ &amp;= -\\nabla _{\\mathbf{x} }[\\mathbf{f}(\\mathbf{x}, t)p(\\mathbf{x}, t) - \\frac{1}{2}(g^{2}(t) - \\sigma^{2}(t))\\nabla_\\mathbf{x}p(\\mathbf{x}, t)] + \\frac{1}{2}\\sigma^{2}(t)\\nabla _{\\mathbf{x} }^{2}p(\\mathbf{x}, t)\\\\ &amp;= -\\nabla _{\\mathbf{x} }[(\\mathbf{f}(\\mathbf{x}, t) - \\frac{1}{2}(g^{2}(t) - \\sigma^{2}(t))\\nabla_\\mathbf{x}\\log p(\\mathbf{x}, t))p(\\mathbf{x})] + \\frac{1}{2}\\sigma^{2}(t)\\nabla _{\\mathbf{x} }^{2}p(\\mathbf{x}, t)\\\\\\end{align*}\\] 现在我们把福克-普朗克方程变成了这样： \\[\\nabla_{t}p(\\mathbf{x}, t) = -\\nabla_{\\mathbf{x} }[(\\mathbf{f}(\\mathbf{x}, t) - \\frac{1}{2}(g^{2}(t) - \\sigma^{2}(t))\\nabla_\\mathbf{x}\\log p(\\mathbf{x}, t))p(\\mathbf{x})] + \\frac{1}{2}\\sigma^{2}(t)\\nabla _{\\mathbf{x} }^{2}p(\\mathbf{x}, t)\\] 其对应的SDE为： \\[\\text{d}\\mathbf{x}= [\\mathbf{f}(\\mathbf{x}, t) - \\frac{1}{2}(g^{2}(t) - \\sigma^{2}(t))\\nabla_{\\mathbf{x} }\\log p_{t}(\\mathbf{x})]\\text{d}\\mathbf{t} + \\sigma(t)\\text{d}\\mathbf{w}\\] 因为前后两个SDE是等价的，他们对应的\\(p_{t}(\\mathbf{x})\\)是一样的，意味着我们可以改变第二个SDE的方差\\(\\sigma(t)\\)。当我们取\\(\\sigma(t)=0\\)，可以得到一个常微分方程(Ordinary Differential Equation, ODE), \\[\\text{d}\\mathbf{x}= [\\mathbf{f}(\\mathbf{x}, t) - \\frac{1}{2}g^{2}(t)\\nabla_ {\\mathbf{x} }\\log p_{t}(\\mathbf{x})]\\text{d}\\mathbf{t}\\] 这个结论有什么作用呢？首先，我们其实更在乎的是边缘概率分布\\(q_t(x)\\)，因为我们需要保证它在足够长的时刻\\(T\\)，\\(q_T(x)\\)可以变成一个纯噪声，同时我们还需要\\(q_0(x)\\)​符合原始数据分布。上述结论可以保证这一点。同时，扩散模型本质上是在学习一个扩散过程的逆过程，既然前向SDE存在一个对应的ODE，那么反向过程reverse SDE其实也有一个对应的ODE，这个反向过程对应的ODE形式也是上面的式子。 而 DDIM 恰是一种确定性情形，所以我们自然会想到——能不能用 ODE 来描述一个 DDIM 呢？答案是肯定的。DDIM的公式如下： \\[\\begin{align} x_{t-1}&amp;=\\sqrt{\\bar\\alpha_{t-1} }x_\\theta(x_t,t)+\\sqrt{1-\\bar\\alpha_{t-1} }\\epsilon_\\theta(x_t,t)\\\\ &amp;=\\frac{\\sqrt{\\bar\\alpha_{t-1} }}{\\sqrt{\\bar\\alpha_t} }\\left(x_t-\\sqrt{1-\\bar\\alpha_t}\\epsilon_\\theta(x_t,t)\\right)+\\sqrt{1-\\bar\\alpha_{t-1} }\\epsilon_\\theta(x_t,t) \\end{align}\\] 两边均减去\\(x_t\\)，得： \\[\\begin{align} x_{t-1}-x_t&amp;=\\frac{1}{\\sqrt{\\bar\\alpha_t} }\\left[\\left(\\sqrt{\\bar\\alpha_{t-1} }-\\sqrt{\\bar\\alpha_t}\\right)x_t-\\left(\\sqrt{\\bar\\alpha_{t-1}(1-\\bar\\alpha_t)}-\\sqrt{\\bar\\alpha_t(1-\\bar\\alpha_{t-1})}\\right)\\epsilon_\\theta(\\mathbf x_t,t)\\right]\\\\ &amp;=\\frac{1}{\\sqrt{\\bar\\alpha_t} }\\left(\\frac{\\bar\\alpha_{t-1}-\\bar\\alpha_t}{\\sqrt{\\bar\\alpha_{t-1} }+\\sqrt{\\bar\\alpha_t} }x_t-\\frac{\\bar\\alpha_{t-1}-\\bar\\alpha_t}{\\sqrt{\\bar\\alpha_{t-1}(1-\\bar\\alpha_t)}+\\sqrt{\\bar\\alpha_t(1-\\bar\\alpha_{t-1})} }\\epsilon_\\theta(x_t,t)\\right)\\\\ &amp;=\\frac{\\bar\\alpha_{t-1}-\\bar\\alpha_t}{\\sqrt{\\bar\\alpha_t} }\\left(\\frac{x_t}{\\sqrt{\\bar\\alpha_{t-1} }+\\sqrt{\\bar\\alpha_t} }-\\frac{\\epsilon_\\theta(\\mathbf x_t,t)}{\\sqrt{\\bar\\alpha_{t-1}(1-\\bar\\alpha_t)}+\\sqrt{\\bar\\alpha_t(1-\\bar\\alpha_{t-1})} }\\right) \\end{align}\\] 记\\(x(t)=x_t,\\barα(t)=\\barα_t\\)，将\\(t-1\\)换成\\(t−Δt\\)并令\\(Δt→0\\)，得： \\[\\mathrm dx=\\frac{\\mathrm d\\bar\\alpha(t)}{\\sqrt{\\bar\\alpha(t)} }\\left(\\frac{x(t)}{2\\sqrt{\\bar\\alpha(t)} }-\\frac{\\epsilon_\\theta(x(t),t)}{2\\sqrt{\\bar\\alpha(t)(1-\\bar\\alpha(t))} }\\right)=\\frac{\\bar\\alpha&#39;(t)}{2\\bar\\alpha(t)}\\left(x(t)-\\frac{\\epsilon_\\theta(x(t),t)}{\\sqrt{1-\\bar\\alpha(t)} }\\right)\\mathrm dt\\] 这就是 DDIM 的 ODE 描述。 在 DDPM 的设置下，有\\(f(x,t)=−\\frac{1}{2}β(t)x,g(t)=\\sqrt{β(t)}\\)，代入 \\[\\text{d}\\mathbf{x}= [\\mathbf{f}(\\mathbf{x}, t) - \\frac{1}{2}g^{2}(t)\\nabla_ {\\mathbf{x} }\\log p_{t}(\\mathbf{x})]\\text{d}\\mathbf{t}\\] 得： \\[\\mathrm dx=\\left[-\\frac{1}{2}\\beta(t)x-\\frac{1}{2}\\beta(t)\\nabla_{\\mathbf{x} }\\log p_{t}(\\mathbf{x})\\right]\\mathrm dt=-\\frac{1}{2}\\beta(t)\\left[x+\\nabla_{\\mathbf{x} }\\log p_{t}(\\mathbf{x})\\right]\\mathrm dt\\] 与我们上面的式子对应。 既然引入了ODE，那么我们的模型就可以去学习如何解这个ODE，同时也可以引入各种传统的ODE solver例如：Euler method, Runge–Kutta method等一些方法。这就是为什么我们可以看到像Stable Diffusion之类的模型会有那么多sampler的原因，本质上都是一些ODE solver和SDE solver。但是后面的研究者发现，传统的ODE solver在采样效果上比不过DDIM，这就非常奇怪了。DPM-Solver的作者在他们的论文中给出了原因：DDIM充分利用了diffusion ODE的半线性结构（semi-linear structure），并且它是一个semi-linear ODE的一阶Solver，而传统的ODE solver并没有利用好这个半线性结构，因此DDIM的准确度会更高一些，因此采样效果也更好。 这里还需要注意的点是，diffusion ODE这类模型相比diffusion SDE存在着诸多好处，比如： 没有随机性，ODE是一个确定性过程，可以以更快的速度收敛，因此可以达到更快的采样速度 由于是确定性过程，可以计算数据似然（likelihood）等。","categories":[{"name":"Stanford CS236深度生成模型","slug":"Stanford-CS236深度生成模型","permalink":"https://jia040223.github.io/categories/Stanford-CS236%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}],"tags":[{"name":"生成模型","slug":"生成模型","permalink":"https://jia040223.github.io/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}]},{"title":"Score Based Models","slug":"Score Based Models","date":"2024-09-24T12:40:58.000Z","updated":"2024-10-27T10:52:49.456Z","comments":true,"path":"2024/09/24/Score Based Models/","permalink":"https://jia040223.github.io/2024/09/24/Score%20Based%20Models/","excerpt":"","text":"本学习笔记用于记录我学习Stanford CS236课程的学习笔记，分享记录，也便于自己实时查看。 引入 Score function 上一次我们学习了Energy Based Model。其核心做法是对一个数据集\\({x_{1}, x_{2}, ..., x_{N}}\\)，我们把数据的概率分布\\(p(x)\\)建模为： \\[p_{\\theta}(\\mathbf{x}) = \\frac{e^{-f_{\\theta}(\\mathbf{x})}}{Z_{\\theta}}\\] 这里\\(f_{\\theta}(\\mathbf{x})\\in \\mathbb{R}\\)。\\(Z_{\\theta}\\)是归一化项保证\\(p_{\\theta}(\\mathbf{x})\\)是概率。\\(\\theta\\)是他们的参数。 我们一般可以通过最大似然估计的方式来训练参数\\(\\theta\\)， \\[\\max_{\\theta}\\sum\\limits_{i=1}^{N}\\log_{\\theta}(\\mathbf{x}_{i})\\] 但是因为 \\[\\log p_{\\theta}(\\mathbf{x}) = -f_{\\theta}(\\mathbf{x}) - \\log Z_{\\theta}\\] \\(Z_{\\theta}\\)是intractable的，我们无法求出\\(\\log p_{\\theta}(\\mathbf{x})\\)，自然也就无法优化参数\\(\\theta\\)。 为了解决归一化项无法计算的问题，我们引入score function。 score function的定义为\\(\\nabla _{\\mathbf{x}}\\log p(\\mathbf{x})\\) 所以我们可以发现，score function是与\\(Z _{\\theta}\\)无关的： \\[\\mathbf{s}_{\\theta}(\\mathbf{x}) = \\nabla_{\\mathbf{x}}\\log(\\mathbf{x}_{\\theta}) = -\\nabla_{\\mathbf{x}}f_{\\theta}(\\mathbf{x}) - \\nabla_{\\mathbf{x}}\\log Z_{\\theta} = -\\nabla_{\\mathbf{x}}f _{\\theta}(\\mathbf{x})\\] Score Based Model Score matching 现在我们想要训练一个网络来估计出真实的score function。自然地，我们可以最小化真实的score function和网络输出的MSE： \\[\\mathcal{L} =\\frac{1}{2} \\mathbb{E}_{p(\\mathbf{x})}[||\\nabla_{\\mathbf{x}}\\log p(\\mathbf{x}) - \\mathbf{s} _{\\theta}(\\mathbf{x})||^{2}]\\] 但是这样的一个loss我们是算不出来的，因为我们并不知道真实的\\(p(\\mathbf{x})\\)是什么。而score matching方法就可以让我们在不知道真实的\\(p(\\mathbf{x})\\)的情况下最小化这个loss。Score matching的推导如下： 我们把上面loss的期望写开，二次项打开，可以得到 \\[\\begin{align*}\\mathcal{L} =&amp; \\frac{1}{2}\\mathbb{E}_{p(\\mathbf{x})}[||\\nabla _{\\mathbf{x}}\\log p(\\mathbf{x}) - \\mathbf{s} _{\\theta}(\\mathbf{x})||^{2}]\\\\=&amp; \\frac{1}{2}\\int p(\\mathbf{x}) [||\\nabla _{\\mathbf{x}}\\log p(\\mathbf{x})||^{2} + ||\\mathbf{s} _{\\theta}(\\mathbf{x})||^{2} - 2(\\nabla _{\\mathbf{x}}\\log p(\\mathbf{x}))^{T}\\mathbf{s} _{\\theta}(\\mathbf{x})] d \\mathbf{x}\\end{align*}\\] 第一项对于\\(\\theta\\)来说是常数可以忽略。 第二项为 \\[\\int p(\\mathbf{x}) ||\\mathbf{s} _{\\theta}(\\mathbf{x})||^{2} d \\mathbf{x}\\] 对于第三项，若\\(\\mathbf{x}\\)的维度为\\(N\\)： \\[ \\begin{align*}&amp; -2\\int p(\\mathbf{x}) (\\nabla _{\\mathbf{x}}\\log p(\\mathbf{x}))^{T}\\mathbf{s} _{\\theta}(\\mathbf{x}) d \\mathbf{x}\\\\ =&amp; -2 \\int p(\\mathbf{x}) \\sum\\limits_{i=1}^{N}\\frac{\\partial \\log p(\\mathbf{x})}{\\partial \\mathbf{x}_{i}}\\mathbf{s}_{\\theta i}(\\mathbf{x}) d \\mathbf{x}\\\\ =&amp; -2 \\sum\\limits_{i=1}^{N} \\int p(\\mathbf{x}) \\frac{1}{p(\\mathbf{x})} \\frac{\\partial p(\\mathbf{x})}{\\partial \\mathbf{x}_{i}}\\mathbf{s}_{\\theta i}(\\mathbf{x}) d \\mathbf{x}\\\\ =&amp; -2 \\sum\\limits_{i=1}^{N} \\int \\frac{\\partial p(\\mathbf{x})}{\\partial \\mathbf{x}_{i}}\\mathbf{s}_{\\theta i}(\\mathbf{x}) d \\mathbf{x}\\\\ =&amp; 2 \\sum\\limits_{i=1}^{N} - \\int \\frac{\\partial p(\\mathbf{x})\\mathbf{s}_{\\theta i}(\\mathbf{x})}{\\partial \\mathbf{x}_{i}} d \\mathbf{x} + \\int p(\\mathbf{x}) \\frac{\\partial \\mathbf{s}_{\\theta i}(\\mathbf{x})}{\\partial \\mathbf{x}_{i}} d \\mathbf{x}\\\\ =&amp; 2 \\sum\\limits_{i=1}^{N} - \\int p(\\mathbf{x})\\mathbf{s}_{\\theta i}(\\mathbf{x})\\bigg\\rvert^{\\infty}_{-\\infty} d \\mathbf{x_{/i}} + \\int p(\\mathbf{x}) \\frac{\\partial \\mathbf{s}_{\\theta i}(\\mathbf{x})}{\\partial \\mathbf{x}_{i}} d \\mathbf{x}\\\\ =&amp; 2 \\sum\\limits_{i=1}^{N} \\int p(\\mathbf{x}) \\frac{\\partial \\mathbf{s}_{\\theta i}(\\mathbf{x})}{\\partial \\mathbf{x}_{i}} d \\mathbf{x}\\\\ =&amp; 2\\int p(\\mathbf{x}) \\sum\\limits_{i=1}^{N} \\frac{\\partial \\mathbf{s}_{\\theta i}(\\mathbf{x})}{\\partial \\mathbf{x}_{i}} d \\mathbf{x}\\\\ =&amp; 2\\int p(\\mathbf{x}) \\text{tr}(\\nabla _{\\mathbf{x}}\\mathbf{s}_{\\theta}(\\mathbf{x})) d \\mathbf{x}\\end{align*} \\] 所以最后的loss是第二和第三项的和： \\[ \\begin{align*} \\mathcal{L} &amp;=\\frac{1}{2} \\int p(\\mathbf{x}) ||\\mathbf{s} _{\\theta}(\\mathbf{x})||^{2} d \\mathbf{x} + \\int p(\\mathbf{x}) \\text{tr}(\\nabla _{\\mathbf{x}}\\mathbf{s}_{\\theta}(\\mathbf{x})) d \\mathbf{x}\\\\\\\\ &amp;= \\mathbb{E}_{p(\\mathbf{x})}[\\frac{1}{2}||\\mathbf{s} _{\\theta}(\\mathbf{x})||^{2} + \\text{tr}(\\nabla _{\\mathbf{x}}\\mathbf{s}_{\\theta}(\\mathbf{x}))]\\end{align*} \\] 当然，这个推导虽然是从能量模型引入的，但并不局限于能量模型，事实上，他是一个更大的模型家族。 Score Matching Langevin Dynamics (SMLD) 现在我们已经通过神经网络学习到了数据分布的score function，那么如何用score function从这个数据分布中得到样本呢？答案就是朗之万动力学采样(Langevin Dynamics): \\[ \\mathbf{x}_{i+1} = \\mathbf{x}_{i} + \\epsilon \\nabla_{\\mathbf{x}}\\log p(\\mathbf{x}) + \\sqrt{2 \\epsilon}\\mathbf{z}_{i}, \\quad \\mathbf{z} _{i} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}), \\quad i=0,1,\\cdots K\\ \\] 这里的采样是一个迭代的过程。\\(\\epsilon\\)是一个很小的量。\\(\\mathbf{x}_{0}\\)随机初始，通过上面的迭代式更新。当迭代次数\\(K\\)足够大的时候，\\(\\mathbf{x}\\)就收敛于该分布的一个样本。 上图的具体解释我就不再赘述了。 这样我们其实就得到了一个生成模型。我们可以先训练一个网络用来估计score function，然后用Langevin Dynamics和网络估计的score function采样，就可以得到原分布的样本。因为整个方法由score matching和Langevin Dynamics两部分组成，所以叫SMLD。 训练 说完了损失函数和采样过程，那么对这个模型我们怎么训练呢？相信敏锐的读者已经注意到了，我们损失函数： \\[ \\begin{align*} \\mathcal{L} &amp;= \\mathbb{E}_{p(\\mathbf{x})}[\\frac{1}{2}||\\mathbf{s} _{\\theta}(\\mathbf{x})||^{2} + \\text{tr}(\\nabla _{\\mathbf{x}}\\mathbf{s}_{\\theta}(\\mathbf{x}))]\\end{align*} \\] 这个第二项并不是很好计算。对于维度为\\(N\\)的数据，我们计算雅可比矩阵的迹需要进行\\(N\\)次反向传播，这对于高维度的数据的训练是不能接受的。 对于这个问题，主要有两种解决方法。 Denoising score matching Denoising score matching的做法就是在 score matching 的基础上，对输入数据加噪。需要注意的是，此时的 score 是对加噪后的数据进行求导，而非原输入数据。score 的方向是(对数)概率密度增长最快的方向，也就是最接近真实数据的方向。 Denoising score matching 的玩法是：在给定输入\\(x\\)的情况下，将条件分布\\(q(\\tilde{x}|x)\\)建模为高斯分布，其中\\(\\tilde{x}\\)代表加噪后的数据，并且边缘化这个条件分布，以\\(p(\\tilde{x}) \\equiv \\int q(\\tilde{x}|x)p(x) dx\\)来近似原数据分布，因此噪声强度不太大时，我们可以认为加噪后数据的概率分布与原数据的概率分布大致相同。 此时，score\\(\\frac{\\partial log(p(\\tilde{x}))}{\\partial \\tilde{x}}\\)中由于\\(p(x)\\)项在求导时与\\(\\tilde{x}\\)无关，可以略去了，具体推导如下： \\[ \\begin{align*} \\frac{1}{2} \\mathbb{E}_{\\tilde{x} \\sim q_{\\sigma}} \\left[ \\| \\nabla_{\\tilde{x}} \\log q_{\\sigma}(\\tilde{x}) - s_{\\theta}(\\tilde{x}) \\|_2^2 \\right] &amp;= \\frac{1}{2} \\int q_{\\sigma}(\\tilde{x}) \\| \\nabla_{\\tilde{x}} \\log q_{\\sigma}(\\tilde{x}) - s_{\\theta}(\\tilde{x}) \\|_2^2 d\\tilde{x} \\\\ &amp;= \\frac{1}{2} \\int q_{\\sigma}(\\tilde{x}) \\| \\nabla_{\\tilde{x}} \\log q_{\\sigma}(\\tilde{x}) \\|_2^2 d\\tilde{x} + \\frac{1}{2} \\int q_{\\sigma}(\\tilde{x}) \\| s_{\\theta}(\\tilde{x}) \\|_2^2 d\\tilde{x}- \\int q_{\\sigma}(\\tilde{x}) \\nabla_{\\tilde{x}} \\log q_{\\sigma}(\\tilde{x})^T s_{\\theta}(\\tilde{x}) d\\tilde{x} \\end{align*} \\] 这里一样的，第一项是常数，第二项只涉及\\(s_{\\theta}(\\tilde{x})\\)，我们可以处理，第三项比较棘手。但我们可以类似地用分布积分法进行处理： \\[ \\begin{align*} &amp;- \\int q_{\\sigma}(\\tilde{x}) \\nabla_{\\tilde{x}} \\log q_{\\sigma}(\\tilde{x})^T s_{\\theta}(\\tilde{x}) d\\tilde{x} \\\\ &amp;= - \\int q_{\\sigma}(\\tilde{x}) \\frac{1}{q_{\\sigma}(\\tilde{x})} \\nabla_{\\tilde{x}} q_{\\sigma}(\\tilde{x})^T s_{\\theta}(\\tilde{x}) d\\tilde{x} \\\\ &amp;= - \\int \\nabla_{\\tilde{x}} q_{\\sigma}(\\tilde{x})^T s_{\\theta}(\\tilde{x}) d\\tilde{x} \\\\ &amp;= - \\int \\nabla_{\\tilde{x}} \\left( \\int p_{\\text{data}}(x) q_{\\sigma}(\\tilde{x} | x) dx \\right)^T s_{\\theta}(\\tilde{x}) d\\tilde{x} \\\\ &amp;= - \\int \\left( \\int p_{\\text{data}}(x) \\nabla_{\\tilde{x}} q_{\\sigma}(\\tilde{x} | x) dx \\right)^T s_{\\theta}(\\tilde{x}) d\\tilde{x} \\\\ &amp;= - \\int \\left( \\int p_{\\text{data}}(x) q_{\\sigma}(\\tilde{x} | x) \\nabla_{\\tilde{x}} \\log q_{\\sigma}(\\tilde{x} | x) dx \\right)^T s_{\\theta}(\\tilde{x}) d\\tilde{x} \\\\ &amp;= - \\int \\int p_{\\text{data}}(x) q_{\\sigma}(\\tilde{x} | x) \\nabla_{\\tilde{x}} \\log q_{\\sigma}(\\tilde{x} | x)^Ts_{\\theta}(\\tilde{x}) dx \\ d\\tilde{x} \\end{align*} \\] 这里我们\\(q(\\tilde{x}|x)\\)是已知的，也就可以计算了。 OK，让我们代入原式之中： \\[ \\begin{align*} &amp;\\frac{1}{2} \\mathbb{E}_{\\tilde{\\mathbf{x}} \\sim q_{\\sigma}} \\left[ \\|\\nabla_{\\tilde{\\mathbf{x}}} \\log q_{\\sigma} (\\tilde{\\mathbf{x}}) - s_{\\theta} (\\tilde{\\mathbf{x}}) \\|_2^2 \\right] \\\\ &amp;= \\text{const.} + \\frac{1}{2} \\mathbb{E}_{\\mathbf{x} \\sim q_{\\sigma}} \\left[ \\| s_{\\theta} (\\mathbf{x}) \\|_2^2 \\right] - \\int q_{\\sigma} (\\tilde{\\mathbf{x}}) \\nabla_{\\tilde{\\mathbf{x}}} \\log q_{\\sigma} (\\tilde{\\mathbf{x}})^{\\top} s_{\\theta} (\\tilde{\\mathbf{x}}) d\\tilde{\\mathbf{x}} \\\\ &amp;= \\text{const.} + \\frac{1}{2} \\mathbb{E}_{\\mathbf{x} \\sim q_{\\sigma}} \\left[ \\| s_{\\theta} (\\tilde{\\mathbf{x}}) \\|_2^2 \\right] - \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}(\\mathbf{x}), \\tilde{\\mathbf{x}} \\sim q_{\\sigma}(\\tilde{\\mathbf{x}}|\\mathbf{x})} \\left[ \\nabla_{\\tilde{\\mathbf{x}}} \\log q_{\\sigma} (\\tilde{\\mathbf{x}}|\\mathbf{x})^{\\top} s_{\\theta} (\\tilde{\\mathbf{x}}) \\right] \\\\ &amp;= \\text{const.} + \\frac{1}{2} \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}(\\mathbf{x}), \\tilde{\\mathbf{x}} \\sim q_{\\sigma}(\\tilde{\\mathbf{x}}|\\mathbf{x})} \\left[ \\| s_{\\theta} (\\tilde{\\mathbf{x}}) - \\nabla_{\\tilde{\\mathbf{x}}} \\log q_{\\sigma} (\\tilde{\\mathbf{x}}|\\mathbf{x}) \\|_2^2 \\right] - \\frac{1}{2} \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}(\\mathbf{x}), \\tilde{\\mathbf{x}} \\sim q_{\\sigma}(\\tilde{\\mathbf{x}})} \\left[ \\| \\nabla_{\\tilde{\\mathbf{x}}} \\log q_{\\sigma} (\\tilde{\\mathbf{x}}) \\|_2^2 \\right] \\\\ &amp;= \\text{const.} + \\frac{1}{2} \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}(\\mathbf{x}), \\tilde{\\mathbf{x}} \\sim q_{\\sigma}(\\tilde{\\mathbf{x}}|\\mathbf{x})} \\left[ \\| s_{\\theta} (\\tilde{\\mathbf{x}}) - \\nabla_{\\tilde{\\mathbf{x}}} \\log q_{\\sigma} (\\tilde{\\mathbf{x}}|\\mathbf{x}) \\|_2^2 \\right] + \\text{const.} \\end{align*} \\] 看到没有！这也就是说，score 的方向与所加噪声的方向是相反的。 于是，在 denoising score matching 的体制下，朝着 score 的方向走，其实就是在去噪，在做 denoising。 在实践中，我们可以选择将\\(q(\\tilde{x}|x)\\)建模为\\(N(\\tilde{x};x;\\sigma^2)\\)，即均值为原数据\\(x\\)，方差为预设的\\(\\sigma^2\\)的高斯分布。于是，根据高斯分布的性质，有： \\[\\tilde{x}=x + \\sigma \\epsilon, \\epsilon\\sim N(0,I)\\] 其中，\\(\\epsilon\\)是从标准高斯分布中采样出来的噪声。 接着，在以上化简出的 score 中代入高斯分布的概率密度函数，可以得到 score 为： \\[\\frac{\\partial log (q(\\tilde{x}|x))}{\\partial \\tilde{x}} = -(\\frac{\\tilde{x}-x}{\\sigma^2})=-\\frac{\\epsilon}{\\sigma}\\] 虽然我们对计算进行了大幅度简化，但这也导致了我们估计的是加噪数据的梯度。具体训练流程如下： Sliced score matching Sliced score matching的思想是，如果模型预测的梯度与真实梯度相同等价于他们在不同方向下的投影均相同，所以我们引入一个投影向量用于训练。这样我们的目标和最终化简（用分部积分即可）的格式如下： goal： \\[ \\frac{1}{2} \\mathbb{E}_{\\mathbf{v} \\sim p_v} \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}} \\left[ \\left( \\mathbf{v}^{\\top} \\nabla_{\\mathbf{x}} \\log p_{\\text{data}} (\\mathbf{x}) - \\mathbf{v}^{\\top} s_{\\theta} (\\mathbf{x}) \\right)^2 \\right] \\] loss： \\[\\mathbb{E}_{\\mathbf{v} \\sim p_v} \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}} \\left[ \\mathbf{v}^{\\top} \\nabla_{\\mathbf{x}} s_{\\theta} (\\mathbf{x}) \\mathbf{v} + \\frac{1}{2} (\\mathbf{v}^{\\top} s_{\\theta} (\\mathbf{x}))^2 \\right] \\] 这样我们便只需要进行一次反向传播了，大大减少了训练需要的计算量，计算图如下： 具体训练过程如下： 虽然这种方法的训练计算量会比Denoising score matching大，但它是对真实数据梯度进行的估计 问题 现在我们得到了SMLD生成模型，但实际上这个模型由很大的问题。首先看一下其在实践中的效果： 可以看到效果并不好。我们不妨从损失函数来分析一下原因： \\[ \\mathcal{L} = \\mathbb{E}_{p(\\mathbf{x})}[||\\nabla_{\\mathbf{x}}\\log p(\\mathbf{x}) - \\mathbf{s}_{\\theta}(\\mathbf{x})||^{2}] = \\int p(\\mathbf{x})||\\nabla_{\\mathbf{x}}\\log p(\\mathbf{x}) - \\mathbf{s} _{\\theta}(\\mathbf{x})||^{2} d \\mathbf{x}\\ \\] 观察我们用来训练神经网络的损失函数，我们可以发现这个L2项其实是被\\(p(\\mathbf{x})\\)加权了。所以对于低概率的区域，估计出来的score function就很不准确： 对于上面这张图来说，只有在高概率的红色区域，loss才高，score function可以被准确地估计出来。但如果我们采样的初始点在低概率区域的话，因为估计出的score function不准确，很有可能生成不出真实分布的样本。 此外，在现实中，比如对于图片来说，其往往是分布在一个低维度流型上，也就是大部分空间的概率密度几乎为0，此时我们的梯度定义已经失去了意义： 同时，我们通过Langevin Dynamics进行采样并不能很好还原聚点的样本比： SMLD的改进 那怎么样才能解决上面的问题呢？Denoising score matching给我们给了一定的启发。 其实可以通过给数据增加噪声扰动的方式扩大高概率区域的面积。给原始分布加上高斯噪声，原始分布的方差会变大。这样相当于高概率区域的面积就增大了，更多区域的score function可以被准确地估计出来。 但是噪声扰动的强度如何控制是个问题： 强度太小起不到效果，高概率区域的面积还是太小 强度太大会破坏数据的原始分布，估计出来的score function就和原分布关系不大了 所以噪声强度越高，高概率区域面积越大，训练得到的梯度越准，但与原始数据的梯度差距也就越大。所以我们不妨加不同程度的噪声，让网络可以学到加了不同噪声的原始分布的score function。这样既保证了原始低概率密度地区能学习到有效的梯度，同时原始高概率密度区的梯度估计是准确的。 说起来很拗口，其实很好理解。我们定义序列\\({\\sigma_{1 \\sim L}} , \\quad \\sigma {1} \\lt \\sigma {2} \\lt \\cdots \\lt \\sigma _{L}\\)，代表从小到大的噪声强度。这样我们可以定义经过噪声扰动之后的数据样本，服从一个经过噪声扰动之后的分布， \\[ \\mathbf{x} + \\sigma_{i}\\mathbf{z} = \\int p(\\mathbf{y}) \\mathcal{N}(\\mathbf{x}|\\mathbf{y}, \\sigma {i}^{2}\\mathbf{I})d \\mathbf{y}\\ \\] 我们用神经网络来估计经过噪声扰动过的分布的score function，并把噪声强度\\(\\sigma_i\\)作为一个输入： \\[ \\mathcal{L} = \\frac{1}{L}\\sum_\\limits {i=1}^{L} \\lambda (i) \\mathbb{E}_{p _{\\sigma {i}}(\\mathbf{x})}[||\\nabla_{\\mathbf{x}}\\log p_{\\sigma _ {i}}(\\mathbf{x}) - \\mathbf{s} _{\\theta}(\\mathbf{x, \\sigma_i})||^{2}] \\] 其中\\(\\lambda(i)\\)是权重，在实践中可以取\\(\\sigma_{i}^{2}\\) 采样方式也要做出相应的变化，我们对于不同的噪声强度\\(L, L-1, \\cdots, 1\\)做Langevin采样，上一个scale的结果作为这一次的初始化。这样我们每一次的初始化都能在梯度估计的有效区域。 这种采样方式也叫做Annealed Langevin dynamics，具体训练流程如下： 从离散到连续 当我们做Langevin dynamics迭代次数足够多时，我们可以用随机微分方程(Stochastic Differential Equation, SDE)来建模这个采样过程。 \\[\\mathbf{x}_{i+1} = \\mathbf{x}_{i} + \\epsilon \\nabla_{\\mathbf{x}}\\log p(\\mathbf{x}_i) + \\sqrt{2 \\epsilon}\\mathbf{z}_{i}, \\quad i=0,1,\\cdots K\\] 当\\(K\\to\\infty\\)时，我们定义\\(\\Delta t = \\epsilon,\\; \\Delta t \\to 0\\) \\[\\mathbf{x}_{t+\\Delta t} - \\mathbf{x}_{t}= \\nabla_{\\mathbf{x}}\\log p(\\mathbf{x}_i)\\Delta t + \\sqrt{2 \\Delta t}\\mathbf{z}_{i}\\] 我们将\\(\\nabla _{\\mathbf{x}}\\log p(\\mathbf{x}_i)\\)和\\(\\sqrt{2}\\)一般化为\\(\\mathbf{f}(\\mathbf{x}, t)\\)和\\(g(t)\\)，这样上面就变成了 \\[\\mathbf{x} _{t+\\Delta t} - \\mathbf{x}_{t}= \\mathbf{f}(\\mathbf{x}, t)\\Delta t + g(t) \\sqrt{\\Delta t}\\mathbf{z} _{i}\\] 其中 \\[\\sqrt{\\Delta t}\\mathbf{z} _{i} \\sim \\mathcal{N}(\\mathbf{0}, \\Delta t\\mathbf{I})\\] 这里可以引入布朗运动，如果我们定义\\(\\mathbf{w}\\)是一个布朗运动，那么 \\[ \\begin{gather*}\\mathbf{w}_{t+\\Delta t} = \\mathbf{w}_{t} + \\mathcal{N}(\\mathbf{0}, \\Delta t\\mathbf{I}),\\\\ \\sqrt{\\Delta t}\\mathbf{z} _{i} = \\mathbf{w}_{t+\\Delta t} - \\mathbf{w}_{t}.\\end{gather*} \\] 讲布朗运动带入到上面，得到 \\[\\mathbf{x}_{t+\\Delta t} - \\mathbf{x}_{t}= \\mathbf{f}(\\mathbf{x}, t)\\Delta t + g(t)(\\mathbf{w}_{t+\\Delta t} - \\mathbf{w}_{t})\\] 当\\(\\Delta t \\to 0\\), \\[\\text{d}\\mathbf{x}= \\mathbf{f}(\\mathbf{x}, t)\\text{d}\\mathbf{t} + g(t)\\text{d}\\mathbf{w}\\] 这里\\(\\mathbf{f}(\\mathbf{x}, t)\\)叫做drift coefficient,\\(g(t)\\)代表diffusion coefficient。SDE的解也就代表了数据不断加噪声的过程。 有了正向过程的SDE，我们可以得到 反向的SDE \\[\\text{d}\\mathbf{x}= [\\mathbf{f}(\\mathbf{x}, t) - g^2(t)\\nabla _{\\mathbf{x}}\\log p(\\mathbf{x})]\\text{d}\\mathbf{t} + g(t)\\text{d}\\mathbf{w}\\] 以及score matching的损失函数 \\[\\mathbb{E}_{t\\in \\mathcal{U}(0, T)} \\mathbb{E}_{p_{t}(\\mathbf{x})}[g^2(t)||\\nabla_{\\mathbf{x}}\\log p_t(\\mathbf{x}) - \\mathbf{s}_{\\theta}(\\mathbf{x})||^2]\\] 可以看到，当我们知道了score后，就能解这个反向的SDE了。 整个基于SDE框架就是：我们在正向过程在图像中加噪声训练神经网络做score matching，估计出score function。然后在反向过程中从高斯噪声通过逆向SDE过程生成出数据分布的样本。 从SDE到ODE 对于一个SDE， \\[\\text{d}\\mathbf{x}= \\mathbf{f}(\\mathbf{x}, t)\\text{d}\\mathbf{t} + g(t)\\text{d}\\mathbf{w}\\] 我们写出它的福克-普朗克方程（Fokker-Planck equation）： \\[ \\begin{align*} \\nabla _{t}p(\\mathbf{x}, t) &amp;= -\\nabla _{\\mathbf{x}}[\\mathbf{f}(\\mathbf{x}, t)p(\\mathbf{x}, t)] + \\frac{1}{2}g^{2}(t)\\nabla _{\\mathbf{x}}^{2}p(\\mathbf{x}, t)\\\\ &amp;= -\\nabla _{\\mathbf{x}}[\\mathbf{f}(\\mathbf{x}, t)p(\\mathbf{x}, t) - \\frac{1}{2}(g^{2}(t) - \\sigma^{2}(t))\\nabla_\\mathbf{x}p(\\mathbf{x}, t)] + \\frac{1}{2}\\sigma^{2}(t)\\nabla _{\\mathbf{x}}^{2}p(\\mathbf{x}, t)\\\\ &amp;= -\\nabla _{\\mathbf{x}}[(\\mathbf{f}(\\mathbf{x}, t) - \\frac{1}{2}(g^{2}(t) - \\sigma^{2}(t))\\nabla_\\mathbf{x}\\log p(\\mathbf{x}, t))p(\\mathbf{x})] + \\frac{1}{2}\\sigma^{2}(t)\\nabla _{\\mathbf{x}}^{2}p(\\mathbf{x}, t)\\\\\\end{align*} \\] 现在我们把福克-普朗克方程变成了这样： \\[ \\nabla_{t}p(\\mathbf{x}, t) = -\\nabla_{\\mathbf{x}}[(\\mathbf{f}(\\mathbf{x}, t) - \\frac{1}{2}(g^{2}(t) - \\sigma^{2}(t))\\nabla_\\mathbf{x}\\log p(\\mathbf{x}, t))p(\\mathbf{x})] + \\frac{1}{2}\\sigma^{2}(t)\\nabla _{\\mathbf{x}}^{2}p(\\mathbf{x}, t) \\] 其对应的SDE为： \\[ \\text{d}\\mathbf{x}= [\\mathbf{f}(\\mathbf{x}, t) - \\frac{1}{2}(g^{2}(t) - \\sigma^{2}(t))\\nabla_{\\mathbf{x}}\\log p_{t}(\\mathbf{x})]\\text{d}\\mathbf{t} + \\sigma(t)\\text{d}\\mathbf{w} \\] 因为前后两个SDE是等价的，他们对应的\\(p_{t}(\\mathbf{x})\\)是一样的，意味着我们可以改变第二个SDE的方差\\(\\sigma(t)\\)。当我们取\\(\\sigma(t)=0\\)，可以得到一个常微分方程(Ordinary Differential Equation, ODE), \\[\\text{d}\\mathbf{x}= [\\mathbf{f}(\\mathbf{x}, t) - \\frac{1}{2}g^{2}(t)\\nabla_{\\mathbf{x}}\\log p_{t}(\\mathbf{x})]\\text{d}\\mathbf{t}\\] 下图就展示了SDE和ODE解的过程，可以看到ODE的轨迹是确定光滑的，而SDE的轨迹是随机的。这两个过程中的任意边缘分布\\({p_{t}(\\mathbf{x})}_{t\\in[0, T]}\\)都是一样的。 ODE形式有它的优点在于： 因为ODE比SDE好解，所以ODE的采样速度更快。 因为ODE是不带随机噪声的，整个过程是确定的，是可逆的，所以这个ODE也可以看做Normalizing flows，可以用来估计概率密度和似然。 但同时由于没有了随机噪声，可能导致多样性更差，实践中生成效果也不如SDE。","categories":[{"name":"Stanford CS236深度生成模型","slug":"Stanford-CS236深度生成模型","permalink":"https://jia040223.github.io/categories/Stanford-CS236%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}],"tags":[{"name":"生成模型","slug":"生成模型","permalink":"https://jia040223.github.io/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}]},{"title":"Energy Based Models","slug":"Energy Based Models","date":"2024-09-20T11:46:30.000Z","updated":"2024-09-25T11:18:58.626Z","comments":true,"path":"2024/09/20/Energy Based Models/","permalink":"https://jia040223.github.io/2024/09/20/Energy%20Based%20Models/","excerpt":"","text":"本学习笔记用于记录我学习Stanford CS236课程的学习笔记，分享记录，也便于自己实时查看。 引入 生成模型的核心目标是对目标样本的概率分布进行预测。而对于一个概率密度函数 \\(P(x)\\) ，它只需要满足下面两个条件： 非负， \\(P(x)\\) 在任何一个点都不能小于0，这很显然。 积分为1， \\(P(x)\\) 从负无穷积分到正无穷得是1。 其中对于第二点，如果 \\(P(x)\\) 的不是1，是 \\(Z\\) ，我们进行一下归一化，除一下 \\(Z\\) 就是1啦。反正至少得是有限的。那么如果我们有一个函数 \\(f(x)\\) ，我们只需要对其进行变换，满足上面两个特点，便能将其转化为一个概率密度函数。 首先可以让 \\(f(x)\\) 变为非负的 \\(g(x)\\) ，比如 \\(g(x) = f(x)^2\\) \\(g(x) = e^{f(x)}\\) \\(g(x) = log(1 + f(x)^2)\\) ...... 可以看到这样的选择有很多，然后接下来便是归一化了，只需要 \\[P(x) = \\frac{g(x)}{\\int g(x)dx} = \\frac{g(x)}{Z}\\] 那么所谓的Energy Based Model 呢，其实很简单，我们就是假设这个函数 \\(g(x) = e^{f(x)}\\) 。 这个时候，下面那个体积volume呢，也叫做partition function。 为啥要 \\(exp()\\) 呢？因为希望在算概率的时候取log，和这个 $ e x p ( ) $ 很多时候能够抵消。而且也和统计物理（虽然笔者并没有研究过统计物理）也有一些联系，这也是energy名字的最初由来。 基本定义 对数据的概率分布进行描述时，这些概率分布都可以写成基于能量函数的形式(energy funciton)， \\(f(\\mathbf x )\\) 。对于连续变量，每个数据点对应一个概率密度函数值，对应一个能量值，如此概率分布即可写成如下玻尔兹曼分布的形式，也叫作吉布斯分布(Boltzmann/Gibbs distribution)： \\(p(\\mathbf x )=\\frac{e^{f(\\mathbf x)}}{Z}\\) \\(Z\\) 为概率归一化的分母，也称为配分函数(partition function)， \\(Z=\\int e^{f(\\mathbf x)} dx\\) 由以上公式可知，概率值较高的位置对应着能力较低的点。举一个简单的例子看一下，将高斯分布以能量函数的形式表示： \\[f(x;\\mu,\\sigma^2)=-\\frac{1}{2\\sigma^2}(x-\\mu)^2\\] \\[p(x)=\\frac{e^{f(\\mathbf x)}}{\\int e^{f(\\mathbf x)}dx}=\\frac{e^{-\\frac{1}{2\\sigma^2}(x-\\mu)^2}}{\\sqrt {2\\pi \\sigma^2}}\\] 部分应用 分类任务 一般来说，这个partition funtion是不能算的，除非是限制为一些可以积分的函数，得到闭式解，但那样表达力又太弱了。而在实际中，我们的 \\(f(x)\\) 一般是用神经网络进行模拟的，所以很难求出这个积分（你也可以遍历所有的情况，但这对于训练或者推理都是无法接受的）。 有时候呢，除非是要算出具体的概率，我们不需要管这个partition function，反正就是知道它是个常数。 比如我们想知道 \\(p(a)\\) 和 \\(p(b)\\) 哪个概率大，就不用去知道绝对的值，只需要知道相对大小就可以啦。这就可以用在分类任务里了。 比如对于图像识别的任务，我只需要知道一个物体是更有可能像猫还是更有可能像狗，而不一定要知道他们的具体概率。 课程中还列举了一个Ising model 的例子，也很直观，不赘述了： 组合专家系统 通过EBMs，可以把多个专家模型混合起来，用乘法。在对模型采样的时候，就会具有多个生成模型的所有性质，比如又是女人又是年轻又是美貌，就不会生成一个年迈的男人。 受限玻尔兹曼机也是基于能量模型，能量形式如下: \\[f(\\mathbf x;\\theta)=exp(\\mathbf x ^T\\mathbf{Wx}+\\mathbf{b}^T\\mathbf{x} + \\mathbf{c}^T\\mathbf{z})\\] 其它就不赘述了。 训练 损失函数（训练目标） 那么如何优化这个模型，直接想法肯定是极大似然估计 \\[L = \\ log (\\frac{exp({f_\\theta(x_{train})})}{Z(\\theta)}) = f_\\theta(x_{train}) - log(Z(\\theta))\\] 这里有个小问题，直接最大化分子并不能解决问题，因为分母是分子的积分，如果只顾着最大化分子的话，可能分母也跟着变大，那最后这整个分数就可能不变甚至变小！但是积分我们又计算不出来怎么办？蒙特卡洛估计便可以派上用场了，我们对 \\(L\\) 求一下梯度： \\[ \\begin{align*} \\nabla_{\\theta} f_{\\theta}(x_{\\text{train}}) - \\nabla_{\\theta} \\log Z(\\theta) &amp;= \\nabla_{\\theta} f_{\\theta}(x_{\\text{train}}) - \\frac{\\nabla_{\\theta} Z(\\theta)}{Z(\\theta)} \\\\ &amp;= \\nabla_{\\theta} f_{\\theta}(x_{\\text{train}}) - \\frac{1}{Z(\\theta)} \\int \\nabla_{\\theta} \\exp \\{ f_{\\theta}(x) \\} dx \\\\ &amp;= \\nabla_{\\theta} f_{\\theta}(x_{\\text{train}}) - \\frac{1}{Z(\\theta)} \\int \\exp \\{ f_{\\theta}(x) \\} \\nabla_{\\theta} f_{\\theta}(x) dx \\\\ &amp;= \\nabla_{\\theta} f_{\\theta}(x_{\\text{train}}) - \\int \\frac{\\exp \\{ f_{\\theta}(x) \\}}{Z(\\theta)} \\nabla_{\\theta} f_{\\theta}(x) dx \\\\ &amp;= \\nabla_{\\theta} f_{\\theta}(x_{\\text{train}}) - \\mathbb{E}_{x_{\\text{sample}}} [\\nabla_{\\theta} f_{\\theta}(x_{\\text{sample}})] \\\\ &amp;= \\nabla_{\\theta} f_{\\theta}(x_{\\text{train}}) - \\nabla_{\\theta} f_{\\theta}(x_{\\text{sample}}) \\end{align*} \\] 其中 \\(x_{sample} \\sim exp(f_\\theta(x_{sample}))/Z_\\theta\\) 最后一步代表在训练过程中，我们只取一个样本作为期望的估计值。 其实主观上也很好理解，其实就是对比了训练集和从模型中的采样，让训练集中数据的概率比随便采样出来的概率大。 我们对上面公式取个负，就是损失函数了。 如何采样 那么问题来了，我们怎么从这个能量模型中采样呢？你看看上面能量模型的式子，你只知道x比y概率大还是概率小，但你不知道x或者y的准确概率。 这时候，MCMC马尔科夫链蒙特卡罗就出场啦。 这是课程对于MCMC的叙述，没明白的可以复习一下，其实就是MH算法： 课程中没强调这个noise是对称的，就是 \\(x\\) 到 \\(x&#39;\\) 的概率等于 \\(x&#39;\\) 到 \\(x\\) 的概率。这时候上图中的关于 \\(q\\) 的分数就等于一了。那就是说，如果 \\(f(x’)\\) 的值大于当前值，那就无脑接受就好啦（2.1步）。如果没有大于，那就算一下比例咯（2.2步）。所以课程中的这个算法就是MH算法。 MH算法很美妙，但太慢啦。那怎么办呢？我们可以用郎之万Langevin 动力学来帮助MH算法，让随机游走朝着概率更高的地方走。 这就是 Metropolis-adjusted Langevin algorithm。 最后总结一下，先用MH算法抽样，用这些抽样放到contrasive divergence 算法里训练能量模型的参数，来极大似然 Score Matching 上面我们用MH算法给出了一个训练和推理的方法，但缺点很明显，就是收敛的太慢了，随着维度的增加，收敛速度指数级别下降。虽然用了郎之万Langevin 动力学来进行提速，但每次梯度一更新之后，分布就变了。所以对于contrasive divergence 的每一步来说，MCMC都要从头开始采样直到收敛。（MCMC采样不是一开始就能用的，要丢弃前n个样本，叫做burn in） 拿能否训练时候不用sampling呢？ score function 先看一下什么叫score function 就是指向高概率方向的梯度。一个观察是，这个梯度和分母，就是partition function无关。至于为什么叫做score fuction，那是因为我们一般把 \\(f_\\theta(x)\\) 对输入x的梯度称为score。 score matching 在之前的MCMC采样方法训练中，当我们有了一个准确的能量模型后，我们从数据分布里采样就转换成了根据训练好的能量模型的score, 来进行MCMC采样。那么为什么不能换个思路，直接将能量模型建模成score，即用一个神经网络来拟合score! 这个方法就叫score-matching! 如上所示，我们的目标依旧是用score matching 来减小这两个分布的区别。难点在于，对于真实分布Pdata怎么求导呢？先看看一维的情况： \\[ \\begin{align*} \\frac{1}{2} \\mathbb{E}_{x \\sim p_{\\text{data}}} \\left[ (\\nabla_x \\log p_{\\text{data}}(x) - \\nabla_x \\log p_{\\theta}(x))^2 \\right] &amp;= \\frac{1}{2} \\int p_{\\text{data}}(x) \\left[ (\\nabla_x \\log p_{\\text{data}}(x) - \\nabla_x \\log p_{\\theta}(x))^2 \\right] dx &amp;\\\\ &amp;= \\frac{1}{2} \\int p_{\\text{data}}(x) (\\nabla_x \\log p_{\\text{data}}(x))^2 dx + \\frac{1}{2} \\int p_{\\text{data}}(x) (\\nabla_x \\log p_{\\theta}(x))^2 dx - \\int p_{\\text{data}}(x) \\nabla_x \\log p_{\\text{data}}(x) \\nabla_x \\log p_{\\theta}(x) dx &amp; \\end{align*} \\] 其中第一项是常数，我们不用管，第二项也只涉及到 \\(p_\\theta\\) (积分的 \\(p_{data}\\) 直接通过在训练集抽样即可)，第三项比较棘手，涉及到 \\(\\nabla_x \\log p_{\\text{data}}(x)\\) ，这个我们没法直接求出。 但是我们可以通过分布积分来进行化简： \\[ \\begin{align*} -\\int p_{\\text{data}}(x) \\nabla_x \\log p_{\\text{data}}(x) \\nabla_x \\log p_{\\theta}(x) dx &amp;= - \\int p_{\\text{data}}(x) \\frac{1}{p_{\\text{data}}(x)} \\nabla_x p_{\\text{data}}(x) \\nabla_x \\log p_{\\theta}(x) dx \\\\ &amp;= - p_{\\text{data}}(x) \\nabla_x \\log p_{\\theta}(x) \\Big|_{x = -\\infty}^{x = \\infty} + \\int p_{\\text{data}}(x) \\nabla_x^2 \\log p_{\\theta}(x) dx \\\\ &amp;= \\int p_{\\text{data}}(x) \\nabla_x^2 \\log p_{\\theta}(x) dx \\end{align*} \\] 其中我们认为 \\(p_{\\text{data}}(x) \\nabla_x \\log p_{\\theta}(x) \\Big|_{x = -\\infty}^{x = \\infty} = 0\\) ，因为我们假定无穷远处的 \\(p_{data}\\) 为0。 对于多维与一维类似，区别就是我们分部积分得到的结果是 \\(log(p_\\theta(x))\\) 的Hessian的迹，最终我们得到的形式如下： 我们通过分部积分把对 \\(P_{data}\\) 的梯度项给搞没了，就不用像之前那样费劲的去MCMC了。不过缺点是这个Hessian矩阵算起来很麻烦。 Noise contrastive estimation 把NCE用在Energy Based Model其实思想也很简单，我们在GANs中提到，对于一个真实样本和模型样本进行分类的最佳判别器是，对给定 \\(x\\) 的判定为真实样本的概率为\\(\\frac{P_{data}(x)}{P_{data}(x) + P_n(x)}\\)。 所以NCE的想法就是我去用生成器组成一个判别器，这个生成器输出概率为 \\(P_{\\theta^*}(x)\\) ，而判别器的输出则是 \\(\\frac{P_{\\theta^*}(x)}{P_{\\theta^*}(x) + P_n(x)}\\) ，这样当判别器训练成为最佳判别器时， \\(P_{\\theta^*}(x)\\) 就等于 $P_{data}(x) $ 。 注意，这里的 \\(P_n\\) 的概率是我们给定一个特定噪声分布进行采样的概率，所以很好获得。也就是说在NCE中，非真实样本的概率分布是事先指定的，而不是模型学习得到的。 但是，依旧会到那个问题， \\(P_{\\theta^*}(x) = \\frac{exp(f_{\\theta^*}(x))}{Z_{\\theta*}}\\) ，我的分母怎么处理呢？此时我们可以把 \\(Z\\) 也作为一个参数进行训练。假如我们能够得到最佳判别器，由于 \\(\\frac{exp(f_{\\theta^*}(x))}{Z^{*}} = P_{data}\\) ，所以 \\(Z\\) 也就肯定是最佳的分区函数了。 把这个形式带入我们二分类的目标函数（与GANs相同，这里不赘述了）： 当然，对于这个 \\(p_n\\) ，它对于训练效果有很显著的影响，毕竟区分图片和一堆噪声可不用很强的判别能力。所以后面也有对这个的改进工作，具体也就是类似GANs一样，再加一个生成器： 当时，这样就会训练得到两个生成模型了，具体推理阶段都可以使用。 注意，能量模型作为生成模型的一种，建模的是 \\(P(x)\\) ,主要功能是从 \\(P(x)\\) 里面采样。上面说的score matching是在训练的时候不用从中采样，加快训练的脚步，但是真正使用的时候还是得有MCMC。NCE因为显式的训练了partition function \\(Z\\) ，也许可以不用MCMC（但笔者感觉也没有比较好的直接采样方法，个人觉得还是需要靠MCMC，如果读者有好的想法也可以指正我）。","categories":[{"name":"Stanford CS236深度生成模型","slug":"Stanford-CS236深度生成模型","permalink":"https://jia040223.github.io/categories/Stanford-CS236%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}],"tags":[{"name":"生成模型","slug":"生成模型","permalink":"https://jia040223.github.io/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}]},{"title":"蒙特卡洛采样方法","slug":"蒙特卡洛采样方法","date":"2024-09-19T12:04:35.000Z","updated":"2024-10-27T10:53:30.248Z","comments":true,"path":"2024/09/19/蒙特卡洛采样方法/","permalink":"https://jia040223.github.io/2024/09/19/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E9%87%87%E6%A0%B7%E6%96%B9%E6%B3%95/","excerpt":"","text":"最近在学习Stanford CS236课程，里面多次提到了蒙特卡洛采样，但本人之前并没有系统地对蒙特卡洛采样进行过整理学习，所以也就正好趁此机会学习一下蒙特卡洛采样，分享记录，也便于自己实时查看。 蒙特卡洛估计 蒙特卡洛估计的原理 蒙特卡洛估计(Monte Carlo Estimator)的原理很简单，假设现在我们要求解一个一维的积分 \\(\\int_{a}^{b} g(x) dx\\) 。已知一个概率密度为 \\(f(x)\\) 的随机变量\\(X\\) ，蒙特卡洛估计可以表示为： \\[G_N = \\frac{1}{N}\\sum_{i=1}^{N}{\\frac{g(X_i)}{f(X_i)}}\\\\\\] 概率密度 \\(f(x)\\) 需要满足 \\[ \\begin{cases} f(x) &gt; 0, x \\in (a, b),\\\\ f(x) = 0, x \\notin (a, b).\\end{cases}\\\\\\] 现在来验证下, 这种方式是正确的： \\[\\begin{align*} E[G_N] &amp; =E\\left [ \\frac{1}{N}\\sum_{i=1}^{N}{\\frac{g(X_i)}{f(X_i)}} \\right]\\\\ &amp; = \\frac{1}{N}\\sum_{i=1}^{N}\\int_{a}^{b}\\frac{g(x)}{f(x)}f(x)dx\\\\ &amp;= \\frac{1}{N}\\sum_{i=1}^{N}\\int_{a}^{b}g(x)dx\\\\ &amp;= \\int_{a}^{b}g(x)dx \\end{align*}\\\\\\] 也就是说， \\(G_N\\) 的期望与 \\(\\int_{a}^{b} g(x) dx\\) 是相同的，而 \\(G_N\\) 的方差如下： \\[\\begin{align*} D[G_N] &amp; =D\\left [ \\frac{1}{N}\\sum_{i=1}^{N}{\\frac{g(X_i)}{f(X_i)}} \\right]\\\\ &amp; = \\frac{1}{N^2}D\\left [ \\sum_{i=1}^{N}{\\frac{g(X_i)}{f(X_i)}} \\right]\\\\ &amp;= \\frac{1}{N^2}\\cdot N \\cdot D\\left [ {\\frac{g(X_i)}{f(X_i)}} \\right]\\\\ &amp;= \\frac{1}{N} D\\left [ {\\frac{g(X_i)}{f(X_i)}} \\right] \\end{align*}\\\\\\] 从上面的式子，可以看出，要减少方差，有两种途径： 增加采样次数 \\(N\\) 减少 \\(D(\\frac{g(X)}{f(X)})\\) 理论上，只要我们采样次数足够多，方差趋近于0，\\(G_N\\) 也就依概率收敛于\\(\\int_{a}^{b} g(x) dx\\) 蒙特卡洛估计的优点 我们在考虑一个积分算法/Estimator 时，通常从两个角度考虑。 一个是计算的准确性，即随着采样次数增大时，结果是否趋近于我们期望的真实值。如果一个 estimator 的期望值和真实值相等，我们说它是无偏的/unbiased。如果一个 estimator 的期望值和真实值不相等，则它是有偏的。大部分 estimator 都是无偏的，在少数情况下，我们会使用一个有偏的但是计算收敛速度很快的 estimator。 另外一个角度是计算结果的方差。随着采样次数增大时，计算结果的方差应该总是减少的。两个estimator 的方差可以比较可以从两个角度来体现。即采样次数相同时的方差大小，以及随着采样次数增大，方差收敛的速度。我们总是期望使用一个方差较小且收敛较快的 estimator，来减少计算的事件。 计算结果表明，蒙特卡洛估计误差收敛的速度为 \\(O(\\sqrt N)\\) (意味着4倍的采样会使误差减少一半)，蒙特卡洛估计不受维度影响，在高维情况下比其他估计方法收敛要快得多。 蒙特卡洛估计的实践使用 在实际使用中，直接使用蒙特卡洛方法要求我们能够从 \\(p(x)\\)中采样——对于简单分布（如均匀分布）这是容易做到的；对于稍微复杂一些但可写出 PDF 或 CDF 分布，可以利用变量替换定理来直接采样；而对于更复杂的分布，我们则更多选择拒绝采样和重要性采样来实现这一点。。 变量替换定理 $ X$ 服从一个我们能直接进行采样的连续值（例如均匀分布），我们希望找到一个函数 \\(f(x)\\) ，让 \\(Y=f(X)\\) 满足我们需要得到的分布 \\(Y \\sim P_y\\) ，则使用累积分布函数： \\[P_y(y)\\triangleq P(Y\\le y)=P(f(X)\\le y)=P(X\\in(f(x)\\le y))\\] 概率密度函数可以通过累积分布函数求导得到。当单调，因此可逆时，可得： \\[P_y(y)=P(f(X)\\le y)=P(X\\le f^{-1}(y))=P_x(f^{-1}(y))\\] 求导可得： \\[p_y(y)\\triangleq\\frac{d}{dy}P_y(y)=\\frac{d}{dy}P_x(f^{-1}(y))=\\frac{dx}{dy}\\frac{d}{dx}P_x(x)=\\frac{dx}{dy}p_x(x)\\] 其中 \\(x=f^{-1}(y)\\) 。由于符号并不重要，因此可得一般表达式： \\[p_y(y)=p_x(x)|\\frac{dx}{dy}|\\] 可将上述结果拓展为多变量分布。令 \\(f\\) 为 \\(R^n\\) 到 \\(R^n\\) 的映射， \\(\\mathrm y=f(\\mathrm x)\\) 。则雅可比矩阵 \\(J\\) 为： \\[J_{\\mathrm x\\rightarrow\\mathrm y}\\triangleq\\frac{\\partial(y_1,\\ldots,y_n)}{\\partial(x_1,\\ldots,x_n)}\\triangleq \\begin{pmatrix} \\frac{\\partial y_1}{\\partial x_1} &amp;\\dots&amp;\\frac{\\partial y_1}{\\partial x_n}\\\\ \\vdots&amp;\\ddots&amp;\\vdots\\\\ \\frac{\\partial y_n}{\\partial x_1}&amp;\\dots&amp;\\frac{\\partial y_n}{\\partial x_n} \\end{pmatrix}\\] \\(|det J|\\) 度量了单位立方体在应用 \\(f\\) 时的体积变化量。如果 \\(f\\) 是一个可逆映射，可以使用反映射 \\(\\mathrm y\\rightarrow\\mathrm x\\) 的雅可比矩阵定义变换变量的概率密度函数： \\[p_y(\\mathrm y)=p_x(\\mathrm x)|\\det\\left(\\frac{\\partial \\mathrm x}{\\partial\\mathrm y}\\right)|=p_x(\\mathrm x)| \\det J_{\\mathrm y\\rightarrow\\mathrm x}|\\] 这就是随机变量的变量替换定理，通过这个我们可以对一些相对简单的分布进行直接采样了。 例如设 \\(x\\) 服从累积分布函数为 \\(F(x)=1-e^{-x}\\) (可验证是单调不减，且积分为1的函数)的分布，则可以通过逆变换的方法对 \\(F(x)\\) 直接采样，产生服从F(X)分布的样本X。 令 \\(y=1-e^{-x}\\)，则$ e^{-x}=1-y$.两边求对数可得: \\(x=-ln(1-y)\\) ,则 \\(F^{-1}(x)=-ln(1-x)\\) ，令 \\(x_i\\) 为均匀分布样本，则 \\(X_i=-ln(1-x_i)\\) 为服从累积分布函数为\\(F(x)\\) 分布的样本. 拒绝接受采样 拒绝接受采样的目的仍然是得到服从某个概率分布的样本，不过这种方法是直接利用概率密度函数(PDF)得到样本。如下图所示， \\(p(x)\\) 是我们希望采样的分布， \\(q(x)\\) 是我们提议的分布(proposal distribution)， \\(q(x)\\) 分布比较简单，令 \\(kq(x)&gt;p(x)\\) ，我们首先在 \\(kq(x)\\) 中按照直接采样的方法采样粒子，接下来以 \\(\\frac{p(x_i)}{kq(x_i)}\\) 的概率接受这个点，最终得到符合 \\(p(x)\\) 的N个粒子。 可以证明，这样做得到的样本是服从\\(p(x)\\)的，我们可以计算 \\(x_0\\) 对应的样本被取到的概率为： \\[\\frac{q(x_0)\\dfrac{\\tilde p(x_0)}{kq(x_0)}}{\\displaystyle\\int_x q(x)\\frac{\\tilde p(x)}{kq(x)}\\mathrm dx}=\\frac{\\tilde p(x_0)}{\\displaystyle\\int_x \\tilde p(x)\\mathrm dx}=p(x_0)\\] 所以拒绝接受采样的基本步骤： 生成服从 \\(q(x)\\) 的样本 \\(x_i\\) . 生成服从均匀分布 \\(U(0,1)\\) 的样本 \\(u_i\\) . 当 \\(k\\cdot q(x_i)\\cdot u_i&lt;p(x_i)\\) ,也就是二维点落在蓝线以下，此时接受 \\(X_k=x_i\\) 这里乘以 \\(u_i\\) ，是因为我们需要以 \\(\\frac{p(x_i)}{kq(x_i)}\\) 的概率接受这个点，因为如果 \\(k\\cdot q(x_i)\\cdot u_i&lt;p(x_i)\\) ，则 \\(u_i&lt;\\frac{p(x_i)}{k\\cdot q(x_i)}\\) ，而 \\(u_i\\) 服从均匀分布 \\(U(0,1)\\) 最终得到的 \\(X_k\\) 为服从 \\(p(x)\\) 的样本. 我们可以计算一下样本采样的接受率： \\[p(\\text{accept})=\\int_x \\frac{\\tilde p(x)}{kq(x)}q(x)\\mathrm dx=\\frac{1}{k}\\int_x\\tilde p(x)\\mathrm dx\\] 因此 \\(k\\) 越小，总接受率越大，算法效率越高。然而， \\(k\\) 小也意味着 \\(q(x)\\) 本身就要与 \\(p(x)\\) 比较相似，对于复杂的 \\(p(x)\\) 而言寻找到一个合适的 \\(q(x)\\) 非常困难的。 重要性采样 重要性采样的目的：求一个函数 \\(f(x)\\) 在概率密度函数为\\(p(x)\\) 分布下的期望，即 \\[\\mathbb{E}[f(x)]=\\int f(x)p(x)dx\\] 当 \\(p(x)\\) 很复杂时，不解析，积分不好求时，可以通过重要性采样来计算。当 \\(f(x)=x\\) ，则可以算 \\(p(x)\\) 的期望。 原理 首先, 当我们想要求一个函数 \\(f(x)\\) 在区间 \\([a, b]\\) 上的积分 \\(\\int_{a}^{b} f(x) d x\\) 时有可能会面临一个问题, 那就是积分曲线难以解析, 无法直接求积分。这时候我们可以采用一种估计的方式, 即在区间 \\([a, b]\\) 上进行采样: \\(\\left\\{x_{1}, x_{2} \\ldots, x_{n}\\right\\}\\) , 值为 \\(\\left\\{f\\left(x_{1}\\right), f\\left(x_{2}\\right), \\ldots, f\\left(x_{n}\\right)\\right\\}\\) 如果采样是均匀的, 即如下图所示: 那么显然可以得到这样的估计: \\(\\int_{a}^{b} f(x) d x=\\frac{b-a}{N} \\sum_{i=1}^{N} f\\left(x_{i}\\right)\\) , 在这里 \\(\\frac{b-a}{N}\\) 可以看作是上面小长方形的底部的 “宽”, 而 \\(f\\left(x_{i}\\right)\\) 则是坚直的 “长”。 上述的估计方法随着取样数的增长而越发精确，那么有什么方法能够在一定的抽样数量基础上来增加准确度，减少方差呢？比如 \\(x\\) 样本数量取10000，那么显然在 \\(f(x)\\) 比较大的地方，有更多的 \\(x_i\\) ，近似的积分更精确。 并且原函数 \\(f(x)\\) 也许本身就是定义在一个分布之上的, 我们定义这个分布为 \\(p(x)\\) , 我们无法直接从\\(p(x)\\) 上进行采样, 所以另辟蹊径重新找到一个更加简明的分布 \\(q(x)\\) , 从它进行取样, 希望间接地求出 \\(f(x)\\) 在分布 \\(p(x)\\) 下的期望。 若p(x)归一化 搞清楚了这一点我们可以继续分析了。首先我们知道函数 \\(f(x)\\) 在概率分布 \\(p(x)\\) 下的期望为: \\[\\mathbb{E}[f(x)]=\\int_{x} p(x) f(x) d x \\] 但是这个期望的值我们无法直接得到, 因此我们需要借助 \\(q(x)\\) 来进行采样, \\(q(x)\\) 可以选取简单的分布，比如设q(x)为均匀分布，当我们在 \\(q(x)\\) 上采样得到 \\(\\left\\{x_{1}, x_{2}, \\ldots, x_{n}\\right\\}\\) （即 \\(x_i\\) 服从 \\(q(x)\\) 分布）后，那么我们可以估计 \\(f\\) 在 \\({q(x)}\\) 下的期望为： \\[\\mathbb{E}[f(x)]=\\int_{x} q(x) f(x) d x \\approx \\frac{1}{N} \\sum_{i=1}^{N} f\\left(x_{i}\\right) \\] 上面这个式子就简单很多了，只要我们得到 \\(x_i\\) 然后代入 \\(f(x)\\) 然后求和就行了，而且均匀分布的样本 \\(x_i\\) 很容易获得。接着我们来考虑原问题，对式(1)进行改写, 即： \\(p(x) f(x)=q(x) \\frac{p(x)}{q(x)} f(x)\\) , 所以我们可以得到: \\[\\mathbb{E}[f(x)]=\\int_{x} q(x) \\frac{p(x)}{q(x)} f(x) d x\\] 这个式子我们可以看作是函数 \\(\\frac{p(x)}{q(x)} f(x)\\) 定义在分布 \\(q(x)\\) 上的期望, 当我们在 \\(q(x)\\) 上采样 \\(\\left\\{x_{1}, x_{2}, \\ldots, x_{n}\\right\\}\\) (服从q(x)分布)，可以估计 \\(f\\) 的期望: \\[\\begin{aligned}\\mathbb{E}[f(x)]&amp;=\\frac{1}{N} \\sum_{i=1}^{N} \\frac{p\\left(x_{i}\\right)}{q\\left(x_{i}\\right)} f\\left(x_{i}\\right)\\\\&amp;=\\frac{1}{N} \\sum_{i=1}^{N} w_i f\\left(x_{i}\\right)\\end{aligned}\\] 在这里 \\(w_i=\\frac{p\\left(x_{i}\\right)}{q\\left(x_{i}\\right)}\\) 就是重要性权重。 若p(x)没有归一化 上面的讨论是假设 \\(p(x)\\) 已经完成归一化了，也就是 \\(\\int p(x)=1\\) ,假如 \\(p(x)\\) 没有归一化，那么我们可以在上面的推导中对 \\(p(x)\\) 进行归一化： \\[\\begin{aligned}\\mathbb{E}[f(x)]&amp;=\\int f(x) \\frac{p(x)}{\\int p(x) d x} d x\\\\&amp;=\\frac{\\int f(x) p(x) d x}{\\int p(x) d x}\\\\&amp;=\\frac{\\int f(x) \\frac{p(x)}{q(x)} q(x) d x}{\\int \\frac{p(x)}{q(x)} q(x) d x}.\\end{aligned}\\] 而分子分母可分别得到，下面两式约等于都利用 \\(q(x)\\) 是均匀分布的假设： \\[\\begin{aligned}\\int f(x) \\frac{p(x)}{q(x)} q(x) d x &amp;\\approx \\frac{1}{n} \\sum_{i=1}^{n} W_{i} f\\left(x_{i}\\right), \\\\\\int \\frac{p(x)}{q(x)} q(x) d x &amp;\\approx \\frac{1}{n} \\sum_{i=1}^{n} W_{i}.\\end{aligned}\\] 其中 \\(W_i=\\frac{p(x_i)}{q(x_i)}\\) ，则最终可得 \\(\\mathbb{E}[f(x)]\\) : \\[\\begin{aligned}\\mathbb{E}[f(x)] \\approx \\sum_{i=1}^{n} w_{i} f\\left(x_{i}\\right), w_{i}=\\frac{W_{i}}{\\sum_{i=1}^{n} W_{i}}\\end{aligned}\\] 多重重要性采样 有的时候, 需要积分的方程中可能包含多个需要积分的部分, 这时候就需要用到多重重要性采样(multiple importance sampling/MIS). 比如现在要求解 \\(\\int_{}^{} g_1(x)g_2(x)\\) 这样的积分时, 两个部分分别对应两个概率密度 \\(f_1(x), f_2(x)\\) , MIS给出的新的蒙特卡洛估计为: \\[\\frac{1}{n_1} \\sum_{i=1}^{n_1}{\\frac{g_1(X_1)g_2(X_1)\\omega_1(X_1)}{f(X_1)}} + \\frac{1}{n_2} \\sum_{i=1}^{n_2}{\\frac{g_1(X_2)g_2(X_2)\\omega_2(X_2)}{f(X_2)}}\\\\\\] \\(n_1,n_2\\) 分别是两边的采样次数, \\(\\omega_1, \\omega_2\\)分别是两个部分对应的权重. 一个常用的权重函数为: \\[\\omega_k = \\frac{(n_kf_k(x))^2} {\\sum_{i}^{}{(n_1f_i(x))^2}}\\\\\\] 在上面有两个部分的情况下得: \\[\\omega_1 = \\frac{(n_1f_1(x))^2} {(n_1f_1(x))^2 +(n_2f_2(x))^2 }\\\\ \\omega_2 = \\frac{(n_2f_2(x))^2} {(n_1f_1(x))^2 +(n_2f_2(x))^2 }\\\\\\] 与拒绝采样一样，重要性采样的效果与提议分布 \\(q(x)\\)同 \\(p(x)\\) 的接近程度紧密相关。当 \\(p(x)\\) 比较复杂时，选择合适的 \\(q(x)\\) 是非常困难的。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://jia040223.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jia040223.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"概率论与数理统计","slug":"概率论与数理统计","permalink":"https://jia040223.github.io/tags/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/"}]},{"title":"GANs","slug":"GANs","date":"2024-09-18T14:10:33.000Z","updated":"2024-10-27T10:50:51.277Z","comments":true,"path":"2024/09/18/GANs/","permalink":"https://jia040223.github.io/2024/09/18/GANs/","excerpt":"","text":"本学习笔记用于记录我学习Stanford CS236课程的学习笔记，分享记录，也便于自己实时查看。 引入 前面我们学习了VAEs和Normalizing Flows，这两种模型都是基于最小化KL散度（对似然进行评估）来进行优化的。我们也可以看到，为了进行生成，我们往往会定义一个潜变量\\(z\\)，所以对似然进行评估并不容易。VAEs是通过优化似然的下限ELBO来绕过这个问题，而Normalizing Flows是通过限制映射的形式来计算似然。 直接计算似然来进行评估，要么只能计算其下界，要么需要限制映射的形式。那有没有一种方法能够用间接方法代替这种直接比较，使生成分布变得越来越接近真实分布呢？GANs便是基于一种间接的评估方式进行设计的。 基本思想 GANs的间接方法采用这两个分布的下游任务形式。然后，生成网络的训练是相对于该任务进行的，使生成分布变得越来越接近真实分布。GANs 的下游任务是区分真实样本和生成样本的任务。或者我们可以说是“非区分”任务，因为我们希望区分尽可能失败。 因此，在 GANs 架构中，我们有一个判别器，它接收真实和生成数据样本，并尽可能地对它们进行分类；还有一个生成器，它被训练成尽可能地欺骗判别器。即GANs由2个重要的部分构成： 生成器(Generator)：通过机器生成数据，目的是“骗过”判别器。 判别器(Discriminator)：判断数据是真实的还是生成的，目的是找出生成器做的“假数据”。 训练过程 我们知道GANs的思想后，便能很直观的想到用分类问题的交叉熵作为判别器的损失函数。同时生成器的目的则是最大化这个交叉熵损失函数（混淆判别器），所以我们的训练目标是： \\[\\mathop{\\text{min}}\\limits_{G}\\mathop{\\text{max}}\\limits_{D} \\ V(G,D) = \\mathbb{E}_{x \\sim p_{data}(x)} [\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)} [\\log(1 - D(G(z)))]\\] 其中\\(G\\)指的是生成器，\\(D\\)指的是判别器。 所以我们的训练目标是一个极大极小的优化问题，在实际中，我们只需要从数据集中进行采样，然后用生成器进行采样，然后对上面的目标函数进行近似计算，最后进行梯度上升或者梯度下降即可 与散度的关系 那么为什么这样的设计能够间接地去让生成器生成的样本与真实样本的分布相同呢？ 其实本质上，GANs通过引入判别器来间接地计算了\\(\\frac{P_\\theta(x)}{P_{data}(x)}\\)，可以证明，对于一个生成器下的最佳判别器对给定\\(x\\)的判定为真实样本的概率是\\(\\frac{P_{data}(x)}{P_{data}(x) + P_\\theta(x)}\\)， 证明如下： *Proof*: 二分类交叉熵损失函数为： \\[\\begin{align} \\mathrm{BCE}(\\mathcal P_1,\\mathcal P_2)&amp;=-\\mathbb E_{x\\sim \\mathcal P_1}[\\log D(x)]-\\mathbb E_{x\\sim \\mathcal P_2}[\\log(1-D(x))]\\\\ &amp;=-\\int \\log D(x)\\cdot p_1(x)\\mathrm d x-\\int\\log(1-D(x))\\cdot p_2(x)\\mathrm d x\\\\ &amp;=-\\int \\left[\\log D(x)\\cdot p_1(x)+\\log(1-D(x))\\cdot p_2(x)\\right]\\mathrm d x\\\\ \\end{align} \\\\\\] 易知\\(y=a\\log x+b\\log(1-x)\\)在\\(x=\\frac{a}{a+b}\\)处取到唯一极大值（其中\\(0\\leq a,b\\leq1\\)），所以欲使上式最小，只需： \\(\\forall x,\\,D(x)=\\frac{p_1(x)}{p_1(x)+p_2(x)} \\\\\\)这样就证明完成了。 那么，再看我们的训练目标： \\[\\min_G\\max_D V(G, D) \\\\ \\begin{align} V(G, D)&amp;=\\mathbb E_{x\\sim\\mathcal P_{data}}[\\log D(x)]+\\mathbb E_{z\\sim \\mathcal P_z}[\\log(1-D(G(z)))]\\\\ &amp;=\\mathbb E_{x\\sim\\mathcal P_{data}}[\\log D(x)]+\\mathbb E_{x\\sim \\mathcal P_{\\theta}}[\\log(1-D(x))] \\end{align} \\\\\\] 而最优判别器为： \\(D^\\ast(x)=\\frac{p_{data}(x)}{p_{data}(x)+p_\\theta(x)} \\\\\\) 将最优判别器代入\\(G\\)的优化目标： \\[\\begin{align} V(G, D^\\ast)&amp;=\\mathbb E_{x\\sim \\mathcal P_{data}}\\left[\\log\\frac{p_{data}(x)}{p_{data}(x)+p_\\theta(x)}\\right]+\\mathbb E_{x\\sim \\mathcal P_\\theta}\\left[\\log\\frac{p_\\theta(x)}{p_{data}(x)+p_\\theta(x)}\\right]\\\\ &amp;=2\\mathrm {JS}(\\mathcal P_{data}\\|\\mathcal P_\\theta)-2\\log2 \\end{align} \\\\\\] 因此，生成器实际上在最小化\\(\\mathcal P_{data}\\)和\\(\\mathcal P_\\theta\\)的\\(\\mathrm{JS}\\)散度，从而让生成数据的分布\\(\\mathcal P_\\theta\\)接近真实分布\\(\\mathcal P_{data}\\)。 注：\\(JS\\)散度的定义如下： *\\[\\begin{align} \\mathrm{JS}(\\mathcal P_1\\|\\mathcal P_2)&amp;=\\frac{1}{2}\\left[\\mathrm{KL}\\left(\\mathcal P_1\\|\\mathcal P_A\\right)+\\mathrm{KL}\\left(\\mathcal P_2\\|\\mathcal P_A\\right)\\right]\\\\ &amp;=\\log 2+\\frac{1}{2}\\mathbb E_{x\\sim\\mathcal P_1}\\left[\\log\\frac{p_1(x)}{p_1(x)+p_2(x)}\\right]+\\frac{1}{2}\\mathbb E_{x\\sim\\mathcal P_2}\\left[\\log\\frac{p_2(x)}{p_1(x)+p_2(x)}\\right] \\end{align} \\\\\\] 其相比\\(KL\\)散度最大的特点便是其是对称的*。 可以看出GANs是通过判别器来巧妙地规避了计算似然的问题，但正是因为在实践中我们很难得到真正的最佳判别器，所以实际上我们很多时候只是在优化\\(JS\\)散度的一个下界，笔者认为这是GANs不得不直面的一个问题。 fGAN F-散度(F-divergence) 在概率统计中，f散度是一个函数，这个函数用来衡量两个概率密度\\(p\\)和\\(q\\)的区别，也就是衡量这两个分布多么的相同或者不同。像\\(KL\\)散度和\\(JS\\)散度都是它的一种特例 f散度定义如下： \\[{D_f}(\\mathcal P_1\\|\\mathcal P_2)=\\int f (\\frac{p_2(x)}{p_1(x)})\\cdot p_1(x)\\mathrm d x=\\mathbb E_{x\\sim\\mathcal P_1}\\left[f(\\frac{p_2(x)}{p_1(x)})\\right] \\\\\\]\\(f(·)\\)就是不同的散度函数，\\(D_f\\)就是在f散度函数下，两个分布的差异。规定 \\(f\\)是凸函数(为了用琴生不等式) \\(f ( 1 ) = 0\\)(如果两个分布一样，刚好公式=0) 这两个规定保证了\\(D_f\\)是非负的，而且当两个分布相同时，其值为0，一些常见散度的\\(f\\)定义如下： 共轭函数(Fenchel Conjugate) 一个函数\\(f:\\;\\mathbb{R}^n\\mapsto\\mathbb{R}\\)的 Frenchel 共轭为： \\[\\begin{align} f^*( t)=\\sup_{ x}\\big(\\langle t, x\\rangle-f( x)) \\end{align}\\] Fenchel 共轭有几何上的解释。当\\(x\\)固定时，\\(\\langle t, x\\rangle-f( x)\\)是一个仿射函数，因此 Fenchel 共轭就是一组仿射函数的上确界。如果\\(f\\)可微，那么仿射函数取得上确界的位置正好是\\(f\\)的切线，此处有\\(\\nabla f( x)= t\\)。 我们拿\\(f ( x ) = x l o g x\\)来说，当\\(x=10,1, 0.1\\)时可以看到相应的函数直线，可以看到最大化y的点连起来是个凸函数，很类似\\(e^{t-1}\\) 公式图像： 用数学来推一下： 将\\(f ( x ) = x l o g x\\)代入\\(y ( t ) = x t − f ( x )\\)，得\\(y ( x ) = x t − x l o g x\\),对于每个给定的\\(t\\)都可以求出最大值，求导为0即可。 求导后得：\\(t − l o g x − 1 = 0\\),即\\(x=e^{t-1}\\)，代入\\(f^*(t)\\), 得\\(f^*(t)=te^{t-1}-e^{t-1}(t-1)=e^{t-1}\\) 读者可以对这个\\(f^*(t)\\)再求一次共轭，可以发现其又变回原函数了。 事实上，可以证明，对于凸函数来说\\(f^{**}(x) = f(x)\\) 应用于GAN 那这个跟GAN有啥关系呢？ 假如我们用一个\\(D_f\\)来评估生成模型，对于\\(p(x)\\)和\\(q(x)\\)之间的 f-divergence： \\[\\begin{aligned} D_f(P||Q) &amp;= \\int_{x} q(x) f\\left(\\frac{p(x)}{q(x)}\\right) dx \\\\ &amp;= \\int_{x} q(x) \\left( \\max_{t \\in \\operatorname{dom}(f^*)} \\left\\{\\frac{p(x)}{q(x)}t - f^*(t)\\right\\} \\right) dx \\end{aligned} \\] 记一个函数 D(x)，它输入是\\(x\\)，输出是\\(t\\)，用该函数代替上式中的\\(t\\)，得到 \\[\\begin{aligned} D_f(P||Q)&amp;\\geq\\int \\limits_{x}q(x)(\\frac{p(x)}{q(x)}D(x)-f^{*}(D(x)))dx\\\\ &amp;= \\int \\limits_{x}p(x)D(x)dx-\\int \\limits_{x}q(x)f^{*}(D(x))dx \\end{aligned}\\] D(x) 其实就是判别器，可以看出，它依然是在解一个求最大值问题，通过这种方法，去逼近 f-divergence。 \\[D_f(P||Q)\\approx\\max \\limits_{D}\\int \\limits_{x}p(x)D(x)dx-\\int \\limits_{x}q(x)f^{*}(D(x))dx\\] p(x) 和 q(x) 本质上是一个概率，于是有 \\[D_f(P||Q)\\approx\\max \\limits_{D}\\{E_{x\\sim P}[D(x)]-E_{x\\sim Q}[f^*(D(x))]\\}\\] 用\\(P_{data}\\)和\\(P_\\theta\\)来指代 P 和 Q，有 \\[D_f(P_{data}||P_\\theta)\\approx\\max \\limits_{D}\\{E_{x\\sim P_{data}}[D(x)]-E_{x\\sim P_\\theta}[f^*(D(x))]\\}\\] 有没有发现这一套下来很熟悉？其实这还是我们之前训练生成器判别器的那一套流程。也就是 \\[\\begin{aligned} G^*&amp;=\\mathop{argmin} \\limits_{G}D_f(P_{data}||P_\\theta)\\\\&amp;=\\mathop{argmin} \\limits_{G}\\max \\limits_{D}\\{E_{x\\sim P_{data}}[D(x)]-E_{x\\sim P_\\theta}[f^*(D(x))]\\}\\\\&amp;=\\mathop{argmin} \\limits_{G}\\max \\limits_{D}V(G, D) \\end{aligned}\\] 只不过这次的损失函数更加 general 了。换不同的\\(f(x)\\)，就可以量不同的散度（divergence）。 WGAN JS散度 to Wasserstein（Earth-Mover EM）距离 JS散度的问题 考虑两个分布\"完全不相交\"的时候，会发现\\(JS\\)散度为常量，梯度为\\(0\\)无法优化。 下面一个例子来说明: 假设两个二维空间上的概率分布，记为\\({P}_d(X_1, Z)\\)和\\({P}_g(X_2, Z)\\)。我们刻画\\(Z \\sim U(0, 1)\\)一个\\([0, 1]\\)上的均匀分布，而分别令\\(X_1 = 0\\)和\\(X_2 = \\theta\\)，因而，它们在二维空间上的概率分布空间就是两条平行线（垂直于\\(x\\)的轴，而平行于\\(z\\)的轴）。 当\\(\\theta = 0.5\\)时，我们考量等价于JS散度的损失函数\\(V(G, D^*)\\)，由于两个分布概率大于0的空间范围是完全没有重叠的，因此，对于任意\\(p_d(x,y) \\ne 0\\)必然有\\(p_g(x, y) =0\\)成立，反之亦然。 因而我们就有，对于任意\\(x \\in \\mathbb{R}^2\\)， \\[V(G, D^*)= \\int_x p_d(x) log \\frac{p_{d}(x)}{p_{d}(x) + p_{g}(x)} + p_g(x)log \\frac{p_{g}(x)}{p_{d}(x) + p_{g}(x)} dx \\\\ = \\int_x p_d(x) log (1) + p_g(x)log (1) dx = 0 \\\\\\]此时，损失函数恒为常量，无法继续指导生成器\\(G(x)\\)的优化。即此时出现了梯度消失的问题。 Wasserstein距离 为了弥补JS散度的局限性，我们需要一种全新的”分布间距离“的度量来进行优化，即使用Wasserstein距离，也被称为“推土机距离”（Earth-Mover），它定义如下： \\(W({P}_d, {P}_g) = inf_{\\gamma \\in \\Pi({P}_d, {P}_g)} {E}[||x - y||] \\\\\\)这样数学形式的刻画可能会让人看得颇为一头雾水，我们逐步来分析解释它。 其中，\\(\\Pi({P}_d, {P}_g)\\)代表一个\\({P}_d, {P}_g\\)构成的联合分布的集合，且这个集合中的所有联合分布必须满足其边际分布分别为\\({P}_d, {P}_g\\)。\\(||x-y||\\)是两个分布所在空间\\(\\mathbb{R}^n\\)中两点的欧式距离。 我们可以将\\(\\Pi({P}_d, {P}_g)\\)中的元素理解为一种“概率的搬运方案”。 而\\(\\gamma\\)是上述集合中的一个联合分布，可以使得任意两点的欧式距离期望最小，即将一个分布搬运为另外一个分布的最小开销。 此时，我们再重新观察上面的场景，当概率分布式为两条平行线上的均匀分布时，显然，最佳方案就是直接与x轴平行地进行概率搬运，对应为：\\(W(P_0, P_\\theta) = |\\theta|\\)。此时，即使两个分布完全没有重叠部分，我们仍然能通过优化Wasserstein距离来实现两个概率分布之间的距离优化。 可以给出证明的是，就像JS散度一样，Wasserstein距离收敛于0时，两个分布也完全一致。 固然，通过Wasserstein距离优化GAN的想法颇为\"美好\"，不过，找到\"最优搬运方案\"的优化问题却是难事，在实现层面上，我们难以直接计算Wasserstein距离。不过，基于对偶理论可以将Wasserstein距离变换为积分概率度量IPM框架下的形式，来方便我们进行优化。 IPM也是用于衡量两个分布之间的距离，它的想法是寻找某种限制下的函数空间\\(\\mathbb{F}\\)中的一个函数\\(f(·)\\)，使得对任意位置两个分布的差异最大： \\[d_F(p, q) = sup_{f \\in F} \\mathbb{E}_{x \\sim P}[f(x)] - \\mathbb{E}_{x \\sim Q}[f(x)] \\\\\\]对于Wasserstein距离而言，则变为： \\[W(p, q) = sup_{||f||_L \\le 1} \\mathbb{E}_{x \\sim P}[f(x)] - \\mathbb{E}_{x \\sim Q}[f(x)] \\\\\\]因而，在函数\\(f(·)\\)满足Lipschitz约束的函数空间中，即\\(||f(x) - f(y)|| \\le K||x - y||\\)，找到最佳的函数\\(f(·)\\)，该情况下上式的结果则为Wasserstein距离。 这个函数\\(f(·)\\)难以求解，但我们可以用神经网络来拟合它。需要注意的是，从此开始，GAN的\\(D\\)就不再是先前我们认为的“真假判别器”了，它的意义变成了一个距离的度量。此时，GAN的生成器并不改变仍然生产图片，对生成器的训练则是减小与真实分布的Wasserstein距离，判别器\\(D\\)负责给出真实图像和生产图像样本之间的Wasserstein距离，相应的，在固定生成器优化判别器时，化则变为了寻找函数空间\\(\\mathbb{F}\\)中最佳的\\(f(·)\\)。 下面的图就可以体现传统GAN的判别器梯度和WGAN的判别器梯度的区别 WGAN便有效解决了某些情况下传统GAN的梯度消失的问题","categories":[{"name":"Stanford CS236深度生成模型","slug":"Stanford-CS236深度生成模型","permalink":"https://jia040223.github.io/categories/Stanford-CS236%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}],"tags":[{"name":"生成模型","slug":"生成模型","permalink":"https://jia040223.github.io/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}]},{"title":"Normalizing Flows","slug":"Normalizing Flows","date":"2024-09-17T10:18:14.000Z","updated":"2024-10-27T10:53:01.807Z","comments":true,"path":"2024/09/17/Normalizing Flows/","permalink":"https://jia040223.github.io/2024/09/17/Normalizing%20Flows/","excerpt":"","text":"本学习笔记用于记录我学习Stanford CS236课程的学习笔记，分享记录，也便于自己实时查看。 引入 生成模型模型的目的是让得到的数据分布\\(P_{\\theta}\\)与真实的数据分布\\(P_{data}\\)相同，也就是需要通过给定的样本来建模对应的分布，使得输入经过该模型后可以生成与给定样本类似的新样本。在这种意义下，评估的最佳方式便是使用极大似然估计，然而VAEs的做法导致计算似然十分复杂，所以我们只能选择计算似然的下界，也就是ELBO。 不妨思考一下，VAEs无法计算似然的原因是什么。不难发现，关键在于需要对所有的潜变量\\(z\\)进行积分。所以假如我们有一个可逆映射，使得潜变量\\(z\\)和数据\\(x\\)之间的是一一对应的，那我们便可以很轻松计算似然了。 Normalizing Flows正是这么做的。但可逆映射意味着潜变量\\(z\\)的维度需要和数据\\(x\\)的维度一致，所以我们无法利用\\(z\\)进行压缩。 简介 正则化流（Normalizing Flow）是一种可逆生成模型，用于将一个原始分布通过学习的变换映射到另一个已知的概率分布。它可以将数据从原始分布转换为目标分布，从而实现数据的生成和采样。 在正则化流中，我们定义一个变换函数，它将输入样本从原始分布映射到目标分布。这个映射是一个可逆函数，确保转换是可逆的，也就是说，在给定目标分布样本的情况下，可以逆向计算出原始分布的样本。这个变换函数通常由一系列的可逆操作组成，每个操作都是可逆的，并且通过组合这些操作可以得到整个变换。常用的可逆操作包括仿射变换、尺度变换、平移变换等。 原理 变量替换 变量替换的形式如下：\\(p_{X}(X)=p_{Z}(f(X))|det~J(f(X))|\\) \\(Z=f(X)\\)是一个可逆的变换 \\(J(f(X))\\)是\\(f(X)\\)的雅可比行列式 如何理解呢：即给出一个\\(X\\)，使用一个可逆变换\\(f(\\cdot)\\)将\\(X\\)变为\\(Z\\)，那么\\(p(X)、p(Z)\\)这两个分布之间相差的就是这样一个雅可比行列式。 1.png 流的组合 基本原理：可导的可逆的函数在进行组合后依然是一个可导且可逆的函数 标准化方向：\\(f=f_{1}\\circ f_{2}\\circ....f_{N}\\) 采样构造概率的方向：\\(g=g_{N} \\circ g_{N-1} \\circ .... \\circ g_{1}\\) 2.png 这种流动的感觉就是标准化流这个名字的由来。 而由\\(p_{X}(X)=p_{Z}(f(X))|det~J(f(X))|\\)可知，上面组合出来的\\(f\\)的雅可比行列式刚好可以表示为每一个\\(f_{i}\\)的雅可比行列式相乘再求行列式。 \\(det~J(f)=det\\prod_{i=1}^{N}J(f_{i})=\\prod_{i=1}^{N}det~J(f_{i})\\) 因为每一个样本都是独立同分布采样出来的，所以它的log likelihood就是把他们的每一个log likelihood加起来。由于做过变量代换，就可以把它变成我们知道的非常简单的分布加上剩下的log 雅可比行列式的和。 3.png 计算 通过最大似然估计，我们便可以训练模型了。但问题在于，如何构建这种可逆映射和如何让雅可比行列式方便计算。因为对于一般的雅可比行列式的计算复杂度是\\(O(n^3)\\)，但是我们可以构造半三角的雅可比行矩阵，这样行列式的计算复杂度只有\\(O(n)\\)了 NICE: Non-linear Independent Components Estimation NICE的目标是找到一个transformation\\(z=f(x)\\), 将数据映射到一个新的空间中; 这个空间中的\\(z\\)的各个分量\\(z_d\\)之间都是独立的, 即\\(p_\\theta(z)=\\prod_d p_{\\theta_d}(z_d)\\).在这种\"各分量独立\"的假设下, 模型会自发地学习\"most important factors of variation\"; 否则, 比如\\(h_1\\)和\\(h_2\\)之间不独立, 那么就浪费了一部分建模能力, 从而无法达到最好的建模效果. 通过\\(z\\)的先验分布和\\(x=f^{-1}(z)\\), 可以实现\\(x\\)的生成(采样)。一般可以假定\\(z\\)的分布满足标准高斯分布。 映射构造(Additive coupling layer) 如何构造构造半三角的雅可比行矩阵呢？NICE给出的方法是： \\(z_{1\\sim d} = x_{1\\sim d}\\) \\(z_{ {d\\sim D} } = x_{ {d\\sim D} } + u_{\\theta}(x_{ {1\\sim d} })\\) 这个变换的雅克比矩阵为 \\[ \\frac{\\partial z}{\\partial x}=\\left[ \\begin{array}{cc} I_d &amp; \\bar{0} \\\\ [\\frac{\\partial u_\\theta}{\\partial x_{1\\sim d} }] &amp; I_{n-d} \\\\ \\end{array} \\right] \\] 这个映射的逆变换也很简单，为 \\(x_{1\\sim d} = z_{1\\sim d}\\) \\(x_{ {d\\sim D} } = z_{ {d\\sim D} } - u_{\\theta}(z_{ {1\\sim d} })\\) Combining coupling layers 事实上, 这个\\(f\\)是要用很多层叠在一起得到的, 即\\(f=f_L \\circ ... \\circ f_2 \\circ f_1\\)。 在堆叠coupling layer的时候, 注意到每个变换有一部分输入是不变的。这样才能让所有部分都能得到变换. 即, 第一层\\(z_1=x_1\\), 变\\(x_2\\), 那么第二层就\\(z_2=x_2\\), 变\\(z_1\\). 另外, 堆叠后的雅克比行列式为 \\[ \\left|\\det \\frac{\\partial z}{\\partial x} \\right| = \\left|\\det \\frac{\\partial f_L(x)}{\\partial f_{L-1}(x)}\\right| \\cdot \\left|\\det \\frac{\\partial f_{L-1}(x)}{\\partial f_{L-2}(x)}\\right| \\cdot \\ldots \\cdot \\left|\\det \\frac{\\partial f_2(x)}{\\partial f_1(x)}\\right| \\] 这些行列式的绝对值为1。 Allowing scaling 因为每个行列式的绝对值都是1, 因此\\(f\\)是volume preserving（体积不变的）的. 为了消除这个限制, 在\\(f_L\\)后又乘了一个diagonal scaling matrix\\(S\\), 即\\(z=S \\cdot f_{1, ...,L}(x)\\). 这样既可以让一些重要特征又更大的变化范围, 又可以让一些不重要的特征减小变化范围(降维). 所以最后目标函数为 \\(\\log p_X(x)=\\sum_{i=1}^D [\\log p_{H_i}(f_i(x)) + \\log |S_{ii}|]\\) Density Estimation Using Real NVP Real NVP将NICE中的每一层的映射改为如下: \\(\\begin{aligned} z_{1:d}&amp;=x_{1:d}\\\\ z_{d+1:D} &amp;=x_{d+1:D} \\odot exp(s(x_{1:d})) +t(x_{1:d}) \\end{aligned}\\) 逆变换为 \\(\\begin{aligned} x_{1:d}&amp;=z_{1:d}\\\\ x_{d+1:D} &amp;=(z_{d+1:D}- t(x_{1:d})) \\odot exp(-s(x_{1:d})) \\end{aligned}\\) 这个变换的雅克比矩阵为 \\[ \\frac{\\partial z}{\\partial x}=\\left[ \\begin{array}{cc} I_d &amp; \\bar{0} \\\\ \\frac{\\partial z_{d+1:D} }{\\partial x_{1:d} } &amp; diag(exp(s(x_{1:d}))) \\\\ \\end{array} \\right] \\] 其中\\(diag(exp(s(x_{1:d})))\\)是将\\(exp(s(x_{1:d}))\\)这个向量展开为对角矩阵. 这个雅克比矩阵的log-determinant为 \\[\\prod_{i=1}^d \\log \\exp(s(x_{1:d}))=\\sum_{i=1}^d s(x_{1:d})\\] 其中没有任何\\(s\\)和\\(t\\)行列式的计算, 因此二者可以任意复杂且hidden layer采用不同于输入的维度. 这样我们便完成了一个更加复杂的构造，同时它的表现也自然比NICE更好。","categories":[{"name":"Stanford CS236深度生成模型","slug":"Stanford-CS236深度生成模型","permalink":"https://jia040223.github.io/categories/Stanford-CS236%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}],"tags":[{"name":"生成模型","slug":"生成模型","permalink":"https://jia040223.github.io/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}]},{"title":"VAEs","slug":"VAEs","date":"2024-09-13T11:31:24.000Z","updated":"2024-10-27T10:52:34.396Z","comments":true,"path":"2024/09/13/VAEs/","permalink":"https://jia040223.github.io/2024/09/13/VAEs/","excerpt":"","text":"本学习笔记用于记录我学习Stanford CS236课程的学习笔记，分享记录，也便于自己实时查看。 潜变量 对于生成模型，我们可以试图寻找一组潜变量z，这个潜变量可以有具体含义，例如对于人脸生成模型的眼睛，鼻子，嘴巴等。通过修改这些潜变量我们可以得到不同风格的生成对象。但是对于图片或者自然语言而言，人为指定这种潜变量极为困难。 所以我们可以并不人为指定潜变量的含义，例如无监督学习的GMM（高斯混合聚类）就并没有指定每个类别具体的含义。但高斯混合聚类人为指定了类别的数量，这对于生成模型也是很难实现定义的。 对于GMM来说，事实上是定义了一组离散的潜变量，在每个潜变量下的数据分布服从高斯分布，它可以给我们一些启示，虽然每个类别的概率只是定义为正态分布，但它组合之后可以形成非常复杂的概率分布。 所以不妨我们可以设定有无穷多个高斯聚类的组合，即设定潜变量 z 是一个连续的随机变量，而每个潜变量 z 的值对应于一个高斯分布，事实上这也正是VAEs做的 核心思想 VAE 的目标是学习一个生成器，将随机向量\\(z \\in R^d\\)映射到\\(x \\in R^D\\), 使得\\(x\\)的分布尽可能接近真实数据的分布。 这里的\\(z\\)其实就是上面提到的潜变量，他是一个连续的随机变量，实践中一般定义为服从高斯分布。而对于每个z的值，我们假设x的分布是满足均值为\\(\\mu(z)\\)，协方差矩阵为\\(\\Sigma(z)\\)（可以通过神经网络进行学习）的高斯分布。理论上这样的组合可以逼近任意的概率分布。 当然PPT中的\\(z\\sim N(0, I)\\)只是一个例子，也可以有更复杂的定义，但在实践中一般使用标准正态分布。 生成和训练 损失函数 对于一个生成模型来说，生成和评估的难易很大程度上决定了它的实用性和价值。对于上面VAE的假设来说，生成是很简单。即假设我们已经知道了\\(p(x|z)\\)，我们只需要先采样\\(z\\)，再采样\\(x\\) 就能得到数据。 但是评估并不容易，这意味着模型的训练可能是一个棘手的问题。对于评估，既然是衡量两个分布的相似度，我们能否直接用各种散度（如 KL 散度）作为损失函数呢？当然可以。在蒙特卡洛抽样（Monte Carlo Sampling）下，最小化KL散度就是最大似然估计。 那么我们的目标是\\(θ_∗=argmax \\sum_{i=1}^{n} logp_θ​(x_i)\\)，注意到 \\(\\sum logP_\\theta(x) =\\sum log(\\sum q(z)P_\\theta(x|z))\\) 对于等式右边的计算是非常复杂的，因为\\(z\\)的取值理论上具有无穷多个 所以我们需要对这个公式进行简化，注意到 \\[ \\begin{align} P_\\theta(x) &amp;= \\sum (q(z) \\frac{p_\\theta(z, x)}{q(z)}) \\nonumber \\\\ &amp;= E_{z \\sim q(z)}\\left(\\frac{p_\\theta(z,x)}{q(z)}\\right) \\end{align} \\] 通过蒙特卡洛抽样（Monte Carlo Sampling），我们可以从\\(q(z)\\)中采样若干数据点，然后进行平均即可估计\\(P_\\theta(x)\\)的值。但很可惜，我们无法通过蒙特卡洛抽样来估计\\(log(P_\\theta(x))\\), 因为 \\(log(E_{z \\sim q(z)}(\\frac{p_\\theta(z,x)}{q(z)})) \\ne E_{z \\sim q(z)}(log(\\frac{p_\\theta(z,x)}{q(z)}))\\) 但幸运的是，对于对数函数是一个严格的凹函数，所以对于凹函数来说有 \\(log(px + (1-p)x^{&#39;}) \\geq plogx +(1-p)logx^{&#39;}\\)，进一步扩展便就是著名的琴生不等式： 琴生不等式 因此\\(log(E_{z \\sim q(z)}(\\frac{p_\\theta(z,x)}{q(z)})) \\geq E_{z \\sim q(z)}(log(\\frac{p_\\theta(z,x)}{q(z)}))\\) 所以我们可以通过这种方法来估计似然的下限，即上面不等号的右边，叫做ELBO（Evidence Lower Bound） 至于这个界限有多紧，我们对\\(logP(x)\\)进行一下推导，就能得到它们之间相差的便是\\(D_{KL}(q(z)||p(z|x;\\theta)\\)，也就是说当\\(q(z)\\)与我们的后验分布越接近，这个界限越紧。 其实这里的推导就是EM算法里面的推导，最大化ELBO的过程就是对应于EM算法里面的M步（后续有机会可能也会写一写）。非常可惜的是，EM 算法无法直接应用于此，因为 E-step 要求我们能够表达出后验分布\\(p_\\theta(z|x)\\)，但没关系，如果我们能够最大化ELBO，也能保证似然的下限被最大化。 问题似乎解决了，但值得注意的是，ELBO 是关于函数\\(q\\)的泛函，也就是说\\(q\\)可以取任意函数，这并不好直接优化。为了解决这个问题，我们可以将\\(q(z)\\)限制为以\\(\\phi\\)为参数的某可解分布族\\(q_\\phi(z|x)\\) ，这样优化变量就从函数\\(q\\)变成了参数\\(\\phi\\)。不过，由于我们限制了\\(q\\)的形式，所以即便能求出最优的参数\\(\\phi\\)，也大概率不是\\(q\\)的最优解。显然，为了尽可能逼近最优解，我们应该让选取的分布族越复杂越好。 那么这里有一个小问题——为什么\\(q(z)\\)参数化后写作\\(q_\\phi(z|x)\\)而不是\\(q_\\phi(z)\\)? 首先，\\(q\\)本来就是我们人为引入的，它是否以\\(x\\)为条件完全是我们的设计，且并不与之前的推导冲突；其次，ELBO与似然当\\(q(z)=p_θ(z|x)\\)时是完全等价的，可见对于不同的\\(x\\)，其\\(q(z)\\)的最佳形式是不同的，所以这么设定有利于减少ELBO与似然的距离。 在VAE中 的\\(p_θ(x|z)\\)和\\(q_\\phi(z|x)\\)都由神经网络表示，因此我们用梯度下降来最大化 ELBO 即可。即对ELBO取负数就是最终的损失函数。 注意到这样的形式中并没有\\(p_θ(x|z)\\)一项，我们只需要稍微变化一下： \\[ \\begin{align} L(x;\\theta, \\phi) &amp;= \\sum q_{\\phi} (z|x)\\left[\\log(p_{\\theta}(z,x;\\theta)) - \\log(q_{\\phi}(z|x))\\right] \\\\ &amp;= \\sum q_{\\phi} (z|x)\\left[\\log(p_{\\theta}(z,x;\\theta)) - \\log(p(z)) + \\log(p(z)) - \\log(q_{\\phi}(z|x))\\right] \\\\ &amp;= \\sum q_{\\phi} (z|x)\\left[\\log(p_{\\theta}(x|z)) - \\log\\left(\\frac{q_{\\phi}(z|x)}{p(z)}\\right)\\right] \\\\ &amp;= E_{z \\sim q_{\\phi}(z|x)}\\left[\\log(p_{\\theta}(x|z))\\right] - D_{KL}(q_{\\phi}(z|x) || p(z)) \\end{align} \\] 这里就把我们的目标分成了两项： 第一项是重构项，要求我们尽可能重构数据本身 第二项是正则项，要求我们的后验与先验接近 所以可以看到，它与自动编码器最大的区别在于有第二项，这保证了隐藏变量\\(z\\)的分布，从而我们可以从先验中对\\(z\\)取样从而进行生成。换句话来说，VAEs是对潜变量进行了正则化的自动编码器，因为我们知道了潜变量\\(z\\)的分布形式，所以它能够用于生成。 按照蒙特卡洛抽样（Monte Carlo Sampling），理论上求这个期望需要对每个样本多次采样进行计算，最后平均。但在具体实践中，往往采样一次进行计算就行。 梯度计算细节：重参数化技巧 有一个细节是现在\\(z\\)是从 \\(q_\\phi(z|x)∼N(μ_ϕ(x)，diag(\\sigma^{2}_ϕ(x)))\\) 中采样的，但梯度无法经过采样传播到参数\\(\\phi\\)。但其实解决方法很简单，对于高斯函数，只需要先从\\(N(0,I)\\)中采样\\(\\epsilon\\)再计算\\(z=μ_ϕ(x)+\\epsilon⋅σ_ϕ(x)\\)即可。 这种技巧也叫做重参数化技巧，其最开始应该是在强化学习中出现的，后面有时间也可以写一写。","categories":[{"name":"Stanford CS236深度生成模型","slug":"Stanford-CS236深度生成模型","permalink":"https://jia040223.github.io/categories/Stanford-CS236%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}],"tags":[{"name":"生成模型","slug":"生成模型","permalink":"https://jia040223.github.io/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"}]}],"categories":[{"name":"Probabilistic Machine Learning","slug":"Probabilistic-Machine-Learning","permalink":"https://jia040223.github.io/categories/Probabilistic-Machine-Learning/"},{"name":"生活blog","slug":"生活blog","permalink":"https://jia040223.github.io/categories/%E7%94%9F%E6%B4%BBblog/"},{"name":"旅行日志","slug":"生活blog/旅行日志","permalink":"https://jia040223.github.io/categories/%E7%94%9F%E6%B4%BBblog/%E6%97%85%E8%A1%8C%E6%97%A5%E5%BF%97/"},{"name":"更新日志","slug":"更新日志","permalink":"https://jia040223.github.io/categories/%E6%9B%B4%E6%96%B0%E6%97%A5%E5%BF%97/"},{"name":"Stanford CS236深度生成模型","slug":"Stanford-CS236深度生成模型","permalink":"https://jia040223.github.io/categories/Stanford-CS236%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"},{"name":"机器学习","slug":"机器学习","permalink":"https://jia040223.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jia040223.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"数学","slug":"数学","permalink":"https://jia040223.github.io/tags/%E6%95%B0%E5%AD%A6/"},{"name":"统计","slug":"统计","permalink":"https://jia040223.github.io/tags/%E7%BB%9F%E8%AE%A1/"},{"name":"信息论","slug":"信息论","permalink":"https://jia040223.github.io/tags/%E4%BF%A1%E6%81%AF%E8%AE%BA/"},{"name":"旅行日志","slug":"旅行日志","permalink":"https://jia040223.github.io/tags/%E6%97%85%E8%A1%8C%E6%97%A5%E5%BF%97/"},{"name":"更新日志","slug":"更新日志","permalink":"https://jia040223.github.io/tags/%E6%9B%B4%E6%96%B0%E6%97%A5%E5%BF%97/"},{"name":"生成模型","slug":"生成模型","permalink":"https://jia040223.github.io/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"},{"name":"概率论与数理统计","slug":"概率论与数理统计","permalink":"https://jia040223.github.io/tags/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/"}]}